{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# BTC Spot Data Analysis\n",
    "This notebook fetches and analyzes BTC data from various exchanges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b9032e8-b01d-4445-aa9c-a1c45e529f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_fetchers",
   "metadata": {},
   "source": [
    "## Exchange Fetchers\n",
    "Classes to fetch OHLCV data from different exchanges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce7297e-9028-4b94-b193-b4879eb0c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# Fetchers por exchange (1D)\n",
    "# ======================================================================================\n",
    "\n",
    "class HyperliquidFetcher:\n",
    "    \"\"\"Hyperliquid candles via native Info API (candleSnapshot).\"\"\"\n",
    "    ENDPOINTS = [\n",
    "        \"https://api.hyperliquid.xyz/info\",\n",
    "        \"https://api-ui.hyperliquid.xyz/info\",\n",
    "    ]\n",
    "\n",
    "    @staticmethod\n",
    "    def _post(payload: dict) -> Optional[dict]:\n",
    "        last_err = None\n",
    "        for url in HyperliquidFetcher.ENDPOINTS:\n",
    "            try:\n",
    "                r = requests.post(url, json=payload, headers=USER_AGENT, timeout=30)\n",
    "                if r.status_code == 200:\n",
    "                    return r.json()\n",
    "                last_err = f\"HTTP {r.status_code}: {r.text[:120]}\"\n",
    "            except Exception as e:\n",
    "                last_err = str(e)\n",
    "        print(f\"[Hyperliquid] API error: {last_err}\")\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch(symbol: str = \"BTC/USDC\", interval: str = \"1d\", start_ms: Optional[int] = None) -> pd.DataFrame:\n",
    "        coin = symbol.split('/')[0].upper()\n",
    "        since = start_ms or to_ms(datetime(2019, 9, 1, tzinfo=timezone.utc))\n",
    "        end_ms = to_ms(datetime.now(timezone.utc))\n",
    "        print(f\"[Hyperliquid] Fetch {symbol} interval={interval} since={ms_to_utc(since).date()} → {ms_to_utc(end_ms).date()}\")\n",
    "        payload = {\n",
    "            \"type\": \"candleSnapshot\",\n",
    "            \"req\": {\"coin\": coin, \"interval\": interval, \"startTime\": since, \"endTime\": end_ms},\n",
    "        }\n",
    "        js = HyperliquidFetcher._post(payload)\n",
    "        if not js or not isinstance(js, list) or not js:\n",
    "            print(\"[Hyperliquid] No data returned\")\n",
    "            return pd.DataFrame()\n",
    "        rows = []\n",
    "        for k in js:\n",
    "            t = int(k.get(\"t\", 0))\n",
    "            o = float(k.get(\"o\", 0)); h = float(k.get(\"h\", 0)); l = float(k.get(\"l\", 0)); c = float(k.get(\"c\", 0))\n",
    "            v = float(k.get(\"v\", 0))\n",
    "            rows.append([t, o, h, l, c, v])\n",
    "        df = normalize_ohlcv_rows(rows, symbol, \"Hyperliquid\")\n",
    "        if start_ms is not None and ONLY_LAST_2_DAYS_IF_MISSING:\n",
    "            start_dt = pd.to_datetime(start_ms, unit='ms', utc=True)\n",
    "            df = df[df[\"timestamp\"] >= start_dt]\n",
    "        print(f\"[Hyperliquid] Fetched rows: {len(df)}; range: {df['timestamp'].min().date() if not df.empty else '-'} → {df['timestamp'].max().date() if not df.empty else '-'}\")\n",
    "        return df\n",
    "\n",
    "\n",
    "class BinanceFetcher:\n",
    "    BASE = \"https://api.binance.com/api/v3/klines\"\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch(symbol: str = \"BTC/USDT\", interval: str = \"1d\", start_ms: Optional[int] = None) -> pd.DataFrame:\n",
    "        sym = symbol.replace(\"/\", \"\")  # BTCUSDT\n",
    "        start = start_ms or to_ms(datetime(2019,9,1,tzinfo=timezone.utc))\n",
    "        all_rows: List[List] = []\n",
    "        while True:\n",
    "            params = {\"symbol\": sym, \"interval\": interval, \"limit\": 1000, \"startTime\": start}\n",
    "            r = requests.get(BinanceFetcher.BASE, params=params, headers=USER_AGENT, timeout=30)\n",
    "            if r.status_code != 200:\n",
    "                print(\"[Binance] HTTP\", r.status_code, r.text[:120]); break\n",
    "            batch = r.json()\n",
    "            if not batch:\n",
    "                break\n",
    "            for k in batch:\n",
    "                all_rows.append([k[0], k[1], k[2], k[3], k[4], k[5]])\n",
    "            if len(batch) < 1000:\n",
    "                break\n",
    "            start = batch[-1][0] + 1\n",
    "            time.sleep(0.12)\n",
    "        return normalize_ohlcv_rows(all_rows, symbol, \"Binance\")\n",
    "\n",
    "\n",
    "class BybitFetcher:\n",
    "    BASE = \"https://api.bybit.com/v5/market/kline\"\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch(symbol: str = \"BTC/USDT\", interval: str = \"D\", start_ms: Optional[int] = None) -> pd.DataFrame:\n",
    "        all_rows: List[List] = []\n",
    "        end = to_ms(datetime.now(timezone.utc))\n",
    "        start = start_ms\n",
    "        while True:\n",
    "            params = {\"category\":\"spot\",\"symbol\":symbol.replace(\"/\",\"\"),\"interval\":interval,\"limit\":1000,\"end\":end}\n",
    "            if start is not None:\n",
    "                params[\"start\"] = start\n",
    "            r = requests.get(BybitFetcher.BASE, params=params, headers=USER_AGENT, timeout=30)\n",
    "            if r.status_code != 200:\n",
    "                print(\"[Bybit] HTTP\", r.status_code, r.text[:120]); break\n",
    "            js = r.json()\n",
    "            if js.get(\"retCode\", -1) != 0:\n",
    "                print(\"[Bybit] API\", js.get(\"retMsg\")); break\n",
    "            lst = js.get(\"result\",{}).get(\"list\",[])\n",
    "            if not lst:\n",
    "                break\n",
    "            for k in lst:\n",
    "                all_rows.append([int(k[0]), k[1], k[2], k[3], k[4], k[5]])\n",
    "            if len(lst) < 1000:\n",
    "                break\n",
    "            end = int(lst[-1][0]) - 1\n",
    "            time.sleep(0.12)\n",
    "        return normalize_ohlcv_rows(all_rows, symbol, \"Bybit\")\n",
    "\n",
    "\n",
    "class OKXFetcher:\n",
    "    BASE = \"https://www.okx.com/api/v5/market/history-candles\"\n",
    "    BASE_REGULAR = \"https://www.okx.com/api/v5/market/candles\"  # Para datos recientes\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch(symbol: str = \"BTC/USDT\", interval: str = \"1D\", start_ms: Optional[int] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fetch OHLCV data from OKX with proper pagination and recent data handling.\n",
    "        Uses both history-candles (for historical) and regular candles (for recent) endpoints.\n",
    "        \"\"\"\n",
    "        okx_symbol = symbol.replace(\"/\", \"-\")\n",
    "        limit = 100\n",
    "        all_rows: List[List] = []\n",
    "        seen_ts = set()\n",
    "        \n",
    "        print(f\"[OKX] Fetching {symbol} interval={interval}\")\n",
    "        \n",
    "        # PASO 1: Obtener datos recientes usando el endpoint regular (últimos 100 días)\n",
    "        print(f\"[OKX] Step 1: Fetching recent data for {symbol}...\")\n",
    "        params_recent = {\"instId\": okx_symbol, \"bar\": interval, \"limit\": limit}\n",
    "        \n",
    "        try:\n",
    "            r = requests.get(OKXFetcher.BASE_REGULAR, params=params_recent, headers=USER_AGENT, timeout=30)\n",
    "            if r.status_code == 200:\n",
    "                js = r.json()\n",
    "                if js.get(\"code\") == \"0\":\n",
    "                    recent_data = js.get(\"data\", [])\n",
    "                    print(f\"[OKX] Got {len(recent_data)} recent candles\")\n",
    "                    for k in recent_data:\n",
    "                        t = int(k[0])\n",
    "                        if t not in seen_ts:\n",
    "                            seen_ts.add(t)\n",
    "                            all_rows.append([t, k[1], k[2], k[3], k[4], k[5]])\n",
    "                    \n",
    "                    if recent_data:\n",
    "                        oldest_recent = min(int(k[0]) for k in recent_data)\n",
    "                        newest_recent = max(int(k[0]) for k in recent_data)\n",
    "                        print(f\"[OKX] Recent data range: {ms_to_utc(oldest_recent).date()} → {ms_to_utc(newest_recent).date()}\")\n",
    "                else:\n",
    "                    print(f\"[OKX] Recent data error: {js.get('msg')}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[OKX] Exception fetching recent data: {e}\")\n",
    "        \n",
    "        # PASO 2: Obtener datos historicals usando history-candles\n",
    "        print(f\"[OKX] Step 2: Fetching historical data for {symbol}...\")\n",
    "        \n",
    "        # Verificar primero si hay datos historicals disponibles\n",
    "        test_params = {\"instId\": okx_symbol, \"bar\": interval, \"limit\": 1}\n",
    "        try:\n",
    "            test_r = requests.get(OKXFetcher.BASE, params=test_params, headers=USER_AGENT, timeout=30)\n",
    "            if test_r.status_code == 200:\n",
    "                test_js = test_r.json()\n",
    "                if test_js.get(\"code\") == \"0\":\n",
    "                    test_data = test_js.get(\"data\", [])\n",
    "                    if not test_data:\n",
    "                        print(f\"[OKX] WARNING: No historical data available for {symbol}\")\n",
    "                        print(f\"[OKX] This might be a new or low-liquidity pair on OKX\")\n",
    "                    else:\n",
    "                        # Si hay datos historicals, proceder con la paginación\n",
    "                        pages = 0\n",
    "                        max_pages = 50\n",
    "                        after_ts = None\n",
    "                        \n",
    "                        while pages < max_pages:\n",
    "                            params = {\"instId\": okx_symbol, \"bar\": interval, \"limit\": limit}\n",
    "                            if after_ts:\n",
    "                                params[\"after\"] = after_ts\n",
    "                            \n",
    "                            r = requests.get(OKXFetcher.BASE, params=params, headers=USER_AGENT, timeout=30)\n",
    "                            if r.status_code != 200:\n",
    "                                print(f\"[OKX] HTTP {r.status_code}\")\n",
    "                                break\n",
    "                            \n",
    "                            js = r.json()\n",
    "                            if js.get(\"code\") != \"0\":\n",
    "                                error_msg = js.get(\"msg\", \"Unknown error\")\n",
    "                                if \"Invalid\" in error_msg or \"not exist\" in error_msg:\n",
    "                                    print(f\"[OKX] Pair {okx_symbol} might not be available for historical data\")\n",
    "                                else:\n",
    "                                    print(f\"[OKX] API error: {error_msg}\")\n",
    "                                break\n",
    "                            \n",
    "                            data = js.get(\"data\", [])\n",
    "                            if not data:\n",
    "                                break\n",
    "                            \n",
    "                            added = 0\n",
    "                            oldest_in_batch = None\n",
    "                            \n",
    "                            for k in data:\n",
    "                                t = int(k[0])\n",
    "                                if t not in seen_ts:\n",
    "                                    seen_ts.add(t)\n",
    "                                    all_rows.append([t, k[1], k[2], k[3], k[4], k[5]])\n",
    "                                    added += 1\n",
    "                                    if oldest_in_batch is None or t < oldest_in_batch:\n",
    "                                        oldest_in_batch = t\n",
    "                            \n",
    "                            if added == 0:\n",
    "                                print(f\"[OKX] No new historical data, stopping at page {pages}\")\n",
    "                                break\n",
    "                            \n",
    "                            if oldest_in_batch:\n",
    "                                after_ts = oldest_in_batch\n",
    "                                if pages % 5 == 0 and pages > 0:\n",
    "                                    print(f\"[OKX] Historical: {pages} pages, {len(all_rows)} total candles, \"\n",
    "                                          f\"oldest so far: {ms_to_utc(oldest_in_batch).date()}\")\n",
    "                            \n",
    "                            # Check if we've reached the start threshold\n",
    "                            if start_ms and oldest_in_batch and oldest_in_batch <= start_ms:\n",
    "                                print(f\"[OKX] Reached start threshold\")\n",
    "                                break\n",
    "                            \n",
    "                            pages += 1\n",
    "                            time.sleep(0.15)\n",
    "                        \n",
    "                        if pages > 0:\n",
    "                            print(f\"[OKX] Historical fetch complete: {pages} pages\")\n",
    "                else:\n",
    "                    print(f\"[OKX] Test query failed: {test_js.get('msg')}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[OKX] Exception during historical fetch: {e}\")\n",
    "        \n",
    "        # PASO 3: Filtrar y procesar datos\n",
    "        if start_ms:\n",
    "            all_rows = [r for r in all_rows if r[0] >= start_ms]\n",
    "        \n",
    "        # Ordenar por timestamp\n",
    "        all_rows.sort(key=lambda x: x[0])\n",
    "        \n",
    "        df = normalize_ohlcv_rows(all_rows, symbol, \"OKX\")\n",
    "        \n",
    "        if not df.empty:\n",
    "            print(f\"[OKX] {symbol} final dataset: {len(df)} rows\")\n",
    "            print(f\"[OKX] Date range: {df['timestamp'].min().date()} → {df['timestamp'].max().date()}\")\n",
    "            \n",
    "            # Verificar gaps significativos\n",
    "            if len(df) > 1:\n",
    "                df_sorted = df.sort_values('timestamp')\n",
    "                date_diffs = df_sorted['timestamp'].diff()\n",
    "                max_gap = date_diffs.max()\n",
    "                if max_gap > pd.Timedelta(days=30):\n",
    "                    gap_idx = date_diffs.idxmax()\n",
    "                    print(f\"[OKX] WARNING: Large gap detected ({max_gap.days} days) \"\n",
    "                          f\"at {df_sorted.loc[gap_idx, 'timestamp'].date()}\")\n",
    "        else:\n",
    "            print(f\"[OKX] WARNING: No data retrieved for {symbol}\")\n",
    "        \n",
    "        # DIAGNÓSTICO ESPECIAL PARA PARES CON POCOS DATOS\n",
    "        if not df.empty and len(df) < 100:\n",
    "            print(f\"[OKX] ⚠️ LIMITED DATA for {symbol}: Only {len(df)} candles available\")\n",
    "            print(f\"[OKX] This suggests {symbol} is:\")\n",
    "            print(f\"[OKX]   - A newly listed pair on OKX\")\n",
    "            print(f\"[OKX]   - A low liquidity/volume pair\")\n",
    "            print(f\"[OKX]   - Or has limited historical data on this exchange\")\n",
    "            print(f\"[OKX] Consider using a different exchange for this pair if more history is needed\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "class CoinbaseFetcher:\n",
    "    BASE = \"https://api.exchange.coinbase.com/products/{}/candles\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _iso_z(dt: datetime) -> str:\n",
    "        return dt.astimezone(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "    @staticmethod\n",
    "    def _pick_product(symbol: str) -> str:\n",
    "        if symbol in (\"BTC/USD\", \"BTC-USD\"): return \"BTC-USD\"\n",
    "        if symbol in (\"BTC/USDT\", \"BTC-USDT\", \"BTC/USDC\", \"BTC-USDC\"): return \"BTC-USD\"\n",
    "        return symbol.replace(\"/\",\"-\")\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch(symbol: str = \"BTC/USD\", granularity_sec: int = 86400, start_ms: Optional[int] = None,\n",
    "              include_partial_today: bool = True) -> pd.DataFrame:\n",
    "        product = CoinbaseFetcher._pick_product(symbol)\n",
    "        end_dt = datetime.now(timezone.utc)\n",
    "        since_dt = datetime(2019,9,1,tzinfo=timezone.utc) if not start_ms else datetime.fromtimestamp(start_ms/1000, tz=timezone.utc)\n",
    "        step = granularity_sec * 300\n",
    "        cur_end = end_dt\n",
    "        all_rows: List[List] = []\n",
    "        while cur_end > since_dt:\n",
    "            cur_start = max(since_dt, cur_end - timedelta(seconds=step - granularity_sec))\n",
    "            params = {\"granularity\": granularity_sec, \"start\": CoinbaseFetcher._iso_z(cur_start), \"end\": CoinbaseFetcher._iso_z(cur_end)}\n",
    "            r = requests.get(CoinbaseFetcher.BASE.format(product), params=params, headers=USER_AGENT, timeout=30)\n",
    "            if r.status_code == 404:\n",
    "                break\n",
    "            if r.status_code != 200:\n",
    "                print(\"[Coinbase] HTTP\", r.status_code, r.text[:120]); break\n",
    "            batch = r.json()  # [time, low, high, open, close, volume] DESC\n",
    "            if not batch:\n",
    "                break\n",
    "            for k in batch:\n",
    "                all_rows.append([k[0]*1000, k[3], k[2], k[1], k[4], k[5]])\n",
    "            oldest = min(b[0] for b in batch)\n",
    "            cur_end = datetime.fromtimestamp(oldest - granularity_sec, tz=timezone.utc)\n",
    "            time.sleep(0.12)\n",
    "        all_rows.sort(key=lambda r: r[0])\n",
    "        df = normalize_ohlcv_rows(all_rows, symbol, \"Coinbase\")\n",
    "        if include_partial_today and granularity_sec == 86400:\n",
    "            if not df.empty and df[\"timestamp\"].dt.date.max() < datetime.now(timezone.utc).date():\n",
    "                start_h = datetime.now(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "                params = {\"granularity\": 3600, \"start\": CoinbaseFetcher._iso_z(start_h), \"end\": CoinbaseFetcher._iso_z(end_dt)}\n",
    "                r = requests.get(CoinbaseFetcher.BASE.format(product), params=params, headers=USER_AGENT, timeout=30)\n",
    "                if r.status_code == 200:\n",
    "                    h = r.json()\n",
    "                    if h:\n",
    "                        h.sort(key=lambda x: x[0])\n",
    "                        o = float(h[0][3]); hhi = float(max(x[2] for x in h)); hlo = float(min(x[1] for x in h)); c = float(h[-1][4]); v = float(sum(x[5] for x in h))\n",
    "                        row = [to_ms(start_h), o, hhi, hlo, c, v]\n",
    "                        df = pd.concat([df, normalize_ohlcv_rows([row], symbol, \"Coinbase\")], ignore_index=True)\n",
    "        return df\n",
    "\n",
    "\n",
    "class CryptoComFetcher:\n",
    "    BASE = \"https://api.crypto.com/exchange/v1/public/get-candlestick\"\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch(symbol: str = \"BTC/USDT\", interval: str = \"1D\", start_ms: Optional[int] = None) -> pd.DataFrame:\n",
    "        inst = symbol.replace(\"/\", \"_\")\n",
    "        all_rows: List[List] = []\n",
    "        end_ts = to_ms(datetime.now(timezone.utc))\n",
    "        since = start_ms or to_ms(datetime(2019,9,1,tzinfo=timezone.utc))\n",
    "        while end_ts > since:\n",
    "            params = {\"instrument_name\": inst, \"timeframe\": interval, \"count\": 300, \"end_ts\": end_ts}\n",
    "            r = requests.get(CryptoComFetcher.BASE, params=params, headers=USER_AGENT, timeout=30)\n",
    "            if r.status_code != 200:\n",
    "                print(\"[Crypto.com] HTTP\", r.status_code, r.text[:120]); break\n",
    "            js = r.json()\n",
    "            if js.get(\"code\") != 0:\n",
    "                print(\"[Crypto.com] API\", js.get(\"message\")); break\n",
    "            data = js.get(\"result\",{}).get(\"data\",[])\n",
    "            if not data:\n",
    "                break\n",
    "            for k in data:\n",
    "                all_rows.append([k['t'], k['o'], k['h'], k['l'], k['c'], k.get('v',0)])\n",
    "            oldest = min(k['t'] for k in data)\n",
    "            end_ts = oldest - 1\n",
    "            time.sleep(0.1)\n",
    "        return normalize_ohlcv_rows(all_rows, symbol, \"Crypto.com\")\n",
    "\n",
    "\n",
    "class UpbitFetcher:\n",
    "    BASE = \"https://api.upbit.com/v1\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def fetch_days(symbol: str = \"BTC/KRW\", start_ms: Optional[int] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fetch daily candles from Upbit with robust date handling.\n",
    "        Fixed timezone issues and deprecated datetime methods.\n",
    "        \"\"\"\n",
    "        # Determine market based on symbol\n",
    "        if symbol.upper().endswith(\"KRW\"):\n",
    "            market = \"KRW-BTC\"\n",
    "        elif symbol.upper().endswith(\"USDT\"):\n",
    "            market = \"USDT-BTC\"\n",
    "        else:\n",
    "            market = \"KRW-BTC\"  # Default to KRW\n",
    "        \n",
    "        url = UpbitFetcher.BASE + \"/candles/days\"\n",
    "        count = 200\n",
    "        \n",
    "        # Use timezone-aware datetime (Python 3.12+ compatible)\n",
    "        to_dt = datetime.now(timezone.utc)\n",
    "        threshold_dt = datetime(2017, 1, 1, tzinfo=timezone.utc)\n",
    "        since_dt = datetime.fromtimestamp(start_ms/1000, tz=timezone.utc) if start_ms else None\n",
    "        \n",
    "        all_rows: List[List] = []\n",
    "        pages = 0\n",
    "        max_pages = 30  # Safety limit\n",
    "        \n",
    "        print(f\"[Upbit] Fetching {symbol} (market: {market})...\")\n",
    "        \n",
    "        while pages < max_pages:\n",
    "            try:\n",
    "                # Format datetime for Upbit API (ISO 8601)\n",
    "                to_str = to_dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "                params = {\n",
    "                    \"market\": market,\n",
    "                    \"count\": count,\n",
    "                    \"to\": to_str\n",
    "                }\n",
    "                \n",
    "                # Make request\n",
    "                r = requests.get(url, params=params, headers=USER_AGENT, timeout=30)\n",
    "                \n",
    "                if r.status_code == 429:\n",
    "                    print(\"[Upbit] Rate limit hit, waiting 1 second...\")\n",
    "                    time.sleep(1)\n",
    "                    continue\n",
    "                \n",
    "                if r.status_code != 200:\n",
    "                    print(f\"[Upbit] HTTP {r.status_code}: {r.text[:200]}\")\n",
    "                    break\n",
    "                \n",
    "                batch = r.json()\n",
    "                \n",
    "                # Check if response is an error dict\n",
    "                if isinstance(batch, dict) and 'error' in batch:\n",
    "                    print(f\"[Upbit] API error: {batch.get('error', {}).get('message', 'Unknown error')}\")\n",
    "                    break\n",
    "                \n",
    "                if not batch or not isinstance(batch, list):\n",
    "                    print(f\"[Upbit] No more data or unexpected response type\")\n",
    "                    break\n",
    "                \n",
    "                # Process batch\n",
    "                batch_count = 0\n",
    "                oldest_dt_in_batch = None\n",
    "                \n",
    "                for k in batch:\n",
    "                    try:\n",
    "                        # Get UTC timestamp\n",
    "                        iso = k.get(\"candle_date_time_utc\")\n",
    "                        if not iso:\n",
    "                            continue\n",
    "                        \n",
    "                        # Parse ISO timestamp robustly\n",
    "                        if 'T' in iso:\n",
    "                            if iso.endswith('Z'):\n",
    "                                dt = datetime.fromisoformat(iso[:-1] + '+00:00')\n",
    "                            elif '+' in iso or '-' in iso[-6:]:\n",
    "                                dt = datetime.fromisoformat(iso)\n",
    "                            else:\n",
    "                                dt = datetime.fromisoformat(iso).replace(tzinfo=timezone.utc)\n",
    "                        else:\n",
    "                            dt = pd.to_datetime(iso, utc=True).to_pydatetime()\n",
    "                        \n",
    "                        if dt.tzinfo is None:\n",
    "                            dt = dt.replace(tzinfo=timezone.utc)\n",
    "                        \n",
    "                        if oldest_dt_in_batch is None or dt < oldest_dt_in_batch:\n",
    "                            oldest_dt_in_batch = dt\n",
    "                        \n",
    "                        ts_ms = int(dt.timestamp() * 1000)\n",
    "                        \n",
    "                        o = float(k.get(\"opening_price\", 0))\n",
    "                        h = float(k.get(\"high_price\", 0))\n",
    "                        l = float(k.get(\"low_price\", 0))\n",
    "                        c = float(k.get(\"trade_price\", 0))\n",
    "                        v = float(k.get(\"candle_acc_trade_volume\", 0))  # Volume in base currency (BTC)\n",
    "                        \n",
    "                        all_rows.append([ts_ms, o, h, l, c, v])\n",
    "                        batch_count += 1\n",
    "                        \n",
    "                    except (ValueError, KeyError, TypeError) as e:\n",
    "                        print(f\"[Upbit] Error parsing candle: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                if batch_count == 0:\n",
    "                    print(f\"[Upbit] No valid candles in batch, stopping\")\n",
    "                    break\n",
    "                \n",
    "                print(f\"[Upbit] Page {pages+1}: {batch_count} candles, oldest: {oldest_dt_in_batch.date() if oldest_dt_in_batch else 'N/A'}\")\n",
    "                \n",
    "                if oldest_dt_in_batch:\n",
    "                    if since_dt and oldest_dt_in_batch <= since_dt:\n",
    "                        print(f\"[Upbit] Reached start date threshold\")\n",
    "                        break\n",
    "                    \n",
    "                    if oldest_dt_in_batch <= threshold_dt:\n",
    "                        print(f\"[Upbit] Reached historical threshold (2017)\")\n",
    "                        break\n",
    "                    \n",
    "                    to_dt = oldest_dt_in_batch - timedelta(seconds=1)\n",
    "                else:\n",
    "                    print(f\"[Upbit] Could not determine oldest date in batch, stopping\")\n",
    "                    break\n",
    "                \n",
    "                if len(batch) < count:\n",
    "                    print(f\"[Upbit] Got {len(batch)} < {count} candles, reached end of data\")\n",
    "                    break\n",
    "                \n",
    "                pages += 1\n",
    "                time.sleep(0.15)  # Rate limiting\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"[Upbit] Request error: {e}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"[Upbit] Unexpected error: {e}\")\n",
    "                break\n",
    "        \n",
    "        df = normalize_ohlcv_rows(all_rows, symbol, \"Upbit\")\n",
    "        \n",
    "        if start_ms is not None and ONLY_LAST_2_DAYS_IF_MISSING:\n",
    "            start_dt = pd.to_datetime(start_ms, unit='ms', utc=True)\n",
    "            df = df[df[\"timestamp\"] >= start_dt]\n",
    "            print(f\"[Upbit] Filtered to dates >= {start_dt.date()}\")\n",
    "        \n",
    "        if not df.empty:\n",
    "            print(f\"[Upbit] Final dataset: {len(df)} rows, range: {df['timestamp'].min().date()} → {df['timestamp'].max().date()}\")\n",
    "        else:\n",
    "            print(f\"[Upbit] No data retrieved\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "def test_upbit_api():\n",
    "    \"\"\"\n",
    "    Test Upbit API connectivity and response format\n",
    "    \"\"\"\n",
    "    print(\"\\n[Upbit Test] Testing API connectivity...\")\n",
    "    \n",
    "    # Test markets endpoint first\n",
    "    try:\n",
    "        markets_url = \"https://api.upbit.com/v1/market/all\"\n",
    "        r = requests.get(markets_url, timeout=10)\n",
    "        if r.status_code == 200:\n",
    "            markets = r.json()\n",
    "            btc_markets = [m for m in markets if 'BTC' in m.get('market', '')]\n",
    "            print(f\"[Upbit Test] ✓ Markets endpoint OK, found {len(btc_markets)} BTC markets\")\n",
    "            \n",
    "            for m in btc_markets[:5]:\n",
    "                print(f\"  - {m['market']}: {m.get('korean_name', m.get('english_name', 'N/A'))}\")\n",
    "        else:\n",
    "            print(f\"[Upbit Test] ✗ Markets endpoint failed: {r.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Upbit Test] ✗ Markets test failed: {e}\")\n",
    "    \n",
    "    # Test candles endpoint\n",
    "    try:\n",
    "        candles_url = \"https://api.upbit.com/v1/candles/days\"\n",
    "        params = {\"market\": \"KRW-BTC\", \"count\": 1}\n",
    "        r = requests.get(candles_url, params=params, timeout=10)\n",
    "        \n",
    "        if r.status_code == 200:\n",
    "            data = r.json()\n",
    "            if data and isinstance(data, list) and len(data) > 0:\n",
    "                candle = data[0]\n",
    "                print(f\"[Upbit Test] ✓ Candles endpoint OK\")\n",
    "                print(f\"  Latest candle date: {candle.get('candle_date_time_utc')}\")\n",
    "                print(f\"  Price: {candle.get('trade_price'):,.0f} KRW\")\n",
    "                print(f\"  Volume: {candle.get('candle_acc_trade_volume'):.4f} BTC\")\n",
    "            else:\n",
    "                print(f\"[Upbit Test] ✗ Unexpected response format: {data}\")\n",
    "        else:\n",
    "            print(f\"[Upbit Test] ✗ Candles endpoint failed: {r.status_code}\")\n",
    "            print(f\"  Response: {r.text[:200]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Upbit Test] ✗ Candles test failed: {e}\")\n",
    "    \n",
    "    print(\"[Upbit Test] Complete\\n\")\n",
    "\n",
    "\n",
    "class BitgetFetcher:\n",
    "    \"\"\"Bitget spot history candles (v2). Paginación por endTime (limit<=200).\"\"\"\n",
    "    BASE = \"https://api.bitget.com/api/v2/spot/market/history-candles\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _map_granularity(interval: str) -> str:\n",
    "        s = (interval or \"1D\").strip().lower()\n",
    "        if s in (\"1d\", \"1day\", \"1dutc\"): return \"1Dutc\"\n",
    "        if s in (\"3d\", \"3day\", \"3dutc\"): return \"3Dutc\"\n",
    "        if s in (\"1w\", \"1week\", \"1wutc\"): return \"1Wutc\"\n",
    "        if s == \"1m\": return \"1min\"\n",
    "        if s == \"3m\": return \"3min\"\n",
    "        if s == \"5m\": return \"5min\"\n",
    "        if s == \"15m\": return \"15min\"\n",
    "        if s == \"30m\": return \"30min\"\n",
    "        if s == \"1h\": return \"1h\"\n",
    "        if s == \"4h\": return \"4h\"\n",
    "        if s == \"6h\": return \"6h\"\n",
    "        if s == \"12h\": return \"12h\"\n",
    "        if s in (\"1mth\", \"1month\"): return \"1M\"\n",
    "        return interval\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch(symbol: str = \"BTC/USDT\", interval: str = \"1D\", start_ms: Optional[int] = None) -> pd.DataFrame:\n",
    "        sym = symbol.replace(\"/\", \"\")\n",
    "        end = to_ms(datetime.now(timezone.utc))\n",
    "        all_rows: List[List] = []\n",
    "        limit = 200\n",
    "        gran = BitgetFetcher._map_granularity(interval)\n",
    "        prev_oldest: Optional[int] = None\n",
    "\n",
    "        while True:\n",
    "            params = {\"symbol\": sym, \"granularity\": gran, \"endTime\": end, \"limit\": limit}\n",
    "            r = requests.get(BitgetFetcher.BASE, params=params, headers=USER_AGENT, timeout=30)\n",
    "            if r.status_code != 200:\n",
    "                print(\"[Bitget] HTTP\", r.status_code, r.text[:160])\n",
    "                break\n",
    "            js = r.json()\n",
    "            if js.get(\"code\") != \"00000\":\n",
    "                print(\"[Bitget] API\", js.get(\"msg\"))\n",
    "                break\n",
    "            data = js.get(\"data\", [])\n",
    "            if not data:\n",
    "                break\n",
    "\n",
    "            for k in data:\n",
    "                try:\n",
    "                    t = int(k[0]); o = float(k[1]); h = float(k[2]); l = float(k[3]); c = float(k[4]); v = float(k[5])\n",
    "                    all_rows.append([t, o, h, l, c, v])\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "            oldest = min(int(k[0]) for k in data)\n",
    "            if start_ms and oldest <= start_ms:\n",
    "                break\n",
    "            if prev_oldest is not None and oldest >= prev_oldest:\n",
    "                break  # sin progreso\n",
    "            prev_oldest = oldest\n",
    "            end = oldest - 1\n",
    "            time.sleep(0.12)\n",
    "\n",
    "        df = normalize_ohlcv_rows(all_rows, symbol, \"Bitget\")\n",
    "        if start_ms is not None:\n",
    "            df = df[df[\"timestamp\"] >= pd.to_datetime(start_ms, unit=\"ms\", utc=True)]\n",
    "        print(f\"[Bitget] Fetched rows: {len(df)}; range: \"\n",
    "              f\"{df['timestamp'].min().date() if not df.empty else '-'} → \"\n",
    "              f\"{df['timestamp'].max().date() if not df.empty else '-'}\")\n",
    "        return df\n",
    "\n",
    "\n",
    "class MEXCFetcher:\n",
    "    BASE = \"https://api.mexc.com/api/v3/klines\"\n",
    "    @staticmethod\n",
    "    def fetch(symbol=\"BTC/USDT\", interval=\"1d\", start_ms=None) -> pd.DataFrame:\n",
    "        sym = symbol.replace(\"/\", \"\")\n",
    "        end = to_ms(datetime.now(timezone.utc))\n",
    "        all_rows = []\n",
    "        limit = 1000\n",
    "        prev_oldest = None\n",
    "        while True:\n",
    "            params = {\"symbol\": sym, \"interval\": interval, \"limit\": limit, \"endTime\": end}\n",
    "            r = requests.get(MEXCFetcher.BASE, params=params, headers=USER_AGENT, timeout=30)\n",
    "            if r.status_code != 200:\n",
    "                print(\"[MEXC] HTTP\", r.status_code, r.text[:120]); break\n",
    "            batch = r.json()\n",
    "            if not batch:\n",
    "                break\n",
    "            for k in batch:\n",
    "                all_rows.append([int(k[0]), float(k[1]), float(k[2]), float(k[3]), float(k[4]), float(k[5])])\n",
    "            oldest = min(int(k[0]) for k in batch)\n",
    "            if start_ms and oldest <= start_ms:\n",
    "                break\n",
    "            if prev_oldest is not None and oldest >= prev_oldest:\n",
    "                break\n",
    "            prev_oldest = oldest\n",
    "            end = oldest - 1\n",
    "            time.sleep(0.12)\n",
    "        df = normalize_ohlcv_rows(all_rows, symbol, \"MEXC\")\n",
    "        if start_ms is not None and ONLY_LAST_2_DAYS_IF_MISSING:\n",
    "            df = df[df[\"timestamp\"] >= pd.to_datetime(start_ms, unit=\"ms\", utc=True)]\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_fx",
   "metadata": {},
   "source": [
    "## FX and Premium Metrics\n",
    "Functions to handle currency conversion and premium calculations (Coinbase, Kimchi)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91474351-2d6d-45dc-a2c9-e0d46b006fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# FX USDKRW (fallbacks sin API key) y métricas\n",
    "# ======================================================================================\n",
    "\n",
    "def fetch_usdkrw_timeseries(start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    \"\"\"Obtains USDKRW (KRW por USD) para [start_date, end_date].\n",
    "    Fallbacks: Frankfurter (app/dev) → FRED CSV (EXKOUS). Normalizes orientation (<1 → invertido).\n",
    "    \"\"\"\n",
    "    used = None\n",
    "    out: Optional[pd.DataFrame] = None\n",
    "    for base_url in (\"https://api.frankfurter.app\", \"https://api.frankfurter.dev\"):\n",
    "        try:\n",
    "            url = f\"{base_url}/{start_date}..{end_date}\"\n",
    "            params = {\"from\": \"USD\", \"to\": \"KRW\"}\n",
    "            r = requests.get(url, params=params, headers=USER_AGENT, timeout=30)\n",
    "            if r.status_code == 200:\n",
    "                js = r.json(); rates = js.get(\"rates\", {})\n",
    "                if rates:\n",
    "                    rows = [{\"date\": pd.to_datetime(d, utc=True), \"USDKRW\": float(rec[\"KRW\"])} for d, rec in sorted(rates.items())]\n",
    "                    out = pd.DataFrame(rows).set_index(\"date\").sort_index()\n",
    "                    used = f\"Frankfurter {base_url}\"\n",
    "                    break\n",
    "        except Exception:\n",
    "            pass\n",
    "    if out is None:\n",
    "        fred_url = \"https://fred.stlouisfed.org/graph/fredgraph.csv\"\n",
    "        try:\n",
    "            r = requests.get(fred_url, params={\"id\": \"EXKOUS\"}, headers=USER_AGENT, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            csv_df = pd.read_csv(io.StringIO(r.text))\n",
    "            csv_df.rename(columns={\"DATE\": \"date\", \"EXKOUS\": \"USDKRW\"}, inplace=True)\n",
    "            csv_df[\"date\"] = pd.to_datetime(csv_df[\"date\"], utc=True)\n",
    "            csv_df[\"USDKRW\"] = pd.to_numeric(csv_df[\"USDKRW\"], errors=\"coerce\")\n",
    "            csv_df = csv_df.dropna().set_index(\"date\").sort_index()\n",
    "            mask = (csv_df.index.date >= pd.to_datetime(start_date).date()) & (csv_df.index.date <= pd.to_datetime(end_date).date())\n",
    "            out = csv_df.loc[mask]\n",
    "            used = \"FRED EXKOUS\"\n",
    "        except Exception:\n",
    "            pass\n",
    "    if out is None or out.empty:\n",
    "        raise RuntimeError(\"Could not obtain USDKRW desde las fuentes públicas (Frankfurter/FRED)\")\n",
    "    med = out[\"USDKRW\"].median()\n",
    "    if pd.notna(med) and med < 1:\n",
    "        out[\"USDKRW\"] = 1.0 / out[\"USDKRW\"]\n",
    "        used += \" (invertido)\"\n",
    "    print(f\"[FX] USDKRW fuente: {used}; rango: {out.index.min().date()} → {out.index.max().date()} (mediana={out['USDKRW'].median():.2f})\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_coinbase_premium(cb_df: pd.DataFrame, usd_dfs: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"Coinbase premium using weights en USD (close*volume). Aplica penalización a Hyperliquid.\"\"\"\n",
    "    cb_dates = ensure_daily(cb_df)[[\"date\",\"close\",\"volume\"]].rename(columns={\"close\":\"cb_close\",\"volume\":\"cb_vol\"})\n",
    "\n",
    "    ref_parts = []\n",
    "    for d in usd_dfs:\n",
    "        if d is None or d.empty:\n",
    "            continue\n",
    "        dd = ensure_daily(d)[[\"date\",\"close\",\"volume\",\"exchange\"]].copy()\n",
    "        dd[\"w_usd\"] = dd[\"close\"] * dd[\"volume\"]\n",
    "        dd.loc[dd[\"exchange\"].str.contains(\"Hyperliquid\", case=False, na=False), \"w_usd\"] *= HYPERLIQUID_WEIGHT_PENALTY\n",
    "        ref_parts.append(dd[[\"date\",\"close\",\"w_usd\"]])\n",
    "    if not ref_parts:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    ref = pd.concat(ref_parts, ignore_index=True)\n",
    "    ref = ref.groupby(\"date\").apply(lambda g: pd.Series({\n",
    "        \"ref_close\": (g[\"close\"] * g[\"w_usd\"]).sum() / max(g[\"w_usd\"].sum(), 1e-12)\n",
    "    })).reset_index()\n",
    "\n",
    "    m = cb_dates.merge(ref, on=\"date\", how=\"inner\")\n",
    "    m[\"coinbase_premium_pct\"] = (m[\"cb_close\"] - m[\"ref_close\"]) / m[\"ref_close\"] * 100\n",
    "    return m[[\"date\",\"cb_close\",\"ref_close\",\"coinbase_premium_pct\"]]\n",
    "\n",
    "\n",
    "def compute_kimchi_premium(upbit_krw_df: pd.DataFrame, ref_usd_df: pd.DataFrame, fx_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    U = ensure_daily(upbit_krw_df)[[\"date\",\"close\"]].rename(columns={\"close\":\"upbit_close_krw\"})\n",
    "    R = ensure_daily(ref_usd_df)[[\"date\",\"close\"]].rename(columns={\"close\":\"ref_close_usd\"})\n",
    "    F = fx_df.reset_index().rename(columns={\"date\":\"date\"})\n",
    "    m = U.merge(R, on=\"date\", how=\"inner\").merge(F, on=\"date\", how=\"left\").sort_values(\"date\")\n",
    "    m[\"USDKRW\"] = m[\"USDKRW\"].ffill().bfill()\n",
    "    m[\"upbit_close_usd\"] = m[\"upbit_close_krw\"] / m[\"USDKRW\"]\n",
    "    m[\"kimchi_pct\"] = (m[\"upbit_close_usd\"] - m[\"ref_close_usd\"]) / m[\"ref_close_usd\"] * 100\n",
    "    return m[[\"date\",\"upbit_close_krw\",\"USDKRW\",\"upbit_close_usd\",\"ref_close_usd\",\"kimchi_pct\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_agg",
   "metadata": {},
   "source": [
    "## 6. Aggregation and Visualization\n",
    "This section handles the robust aggregation of BTC data from multiple exchanges, applying volume-weighting and outlier filtering. It then generates interactive visualizations using **TradingView Lightweight Charts**.\n",
    "\n",
    "### Features:\n",
    "- **Volume-Weighted Average Price (VWAP)**: Aggregated candles based on USD volume.\n",
    "- **Outlier Filtering**: Removes anomalous price spikes or data errors.\n",
    "- **Interactive Charts**: Standalone HTML files with candlestick, volume, and premium series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0671519c-2b8e-4907-a47c-8850b9ffe644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# Lightweight Charts HTML Generator\n",
    "# ======================================================================================\n",
    "\n",
    "import json\n",
    "\n",
    "def generate_lightweight_chart_html(charts_config, title, filepath):\n",
    "    \"\"\"\n",
    "    Generates a standalone HTML file with one or more TradingView Lightweight Charts.\n",
    "    charts_config: List of dicts, each representing a chart:\n",
    "        {\n",
    "            'id': 'chart1',\n",
    "            'title': 'BTC/USD',\n",
    "            'series': [\n",
    "                {'type': 'candlestick', 'data': [...], 'options': {...}},\n",
    "                {'type': 'histogram', 'data': [...], 'options': {..., 'priceScaleId': 'volume'}},\n",
    "                {'type': 'line', 'data': [...], 'options': {...}}\n",
    "            ],\n",
    "            'height': 400\n",
    "        }\n",
    "    \"\"\"\n",
    "    html_template = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>{{TITLE}}</title>\n",
    "    <script src=\"https://unpkg.com/lightweight-charts/dist/lightweight-charts.standalone.production.js\"></script>\n",
    "    <style>\n",
    "        body { font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif; background-color: #f8f9fa; margin: 0; padding: 20px; }\n",
    "        .container { max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); }\n",
    "        h1 { font-size: 24px; color: #1e293b; margin-bottom: 20px; text-align: center; }\n",
    "        .chart-container { margin-bottom: 30px; position: relative; }\n",
    "        .chart-title { font-size: 18px; font-weight: 600; color: #334155; margin-bottom: 8px; }\n",
    "        .chart-div { width: 100%; height: {{HEIGHT}}px; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>{{TITLE}}</h1>\n",
    "        {{CHARTS_HTML}}\n",
    "    </div>\n",
    "    <script>\n",
    "        const configs = {{CONFIGS}};\n",
    "        \n",
    "        configs.forEach(config => {\n",
    "            const chartOptions = {\n",
    "                layout: { background: { color: '#ffffff' }, textColor: '#334155' },\n",
    "                grid: { vertLines: { color: '#f0f3fa' }, horzLines: { color: '#f0f3fa' } },\n",
    "                crosshair: { mode: 0 },\n",
    "                timeScale: { borderColor: '#d1d5db', timeVisible: true, secondsVisible: false },\n",
    "                handleScroll: { vertTouchDrag: false },\n",
    "            };\n",
    "\n",
    "            const container = document.getElementById(config.id);\n",
    "            const chart = LightweightCharts.createChart(container, { ...chartOptions, height: config.height || 400 });\n",
    "            \n",
    "            config.series.forEach(s => {\n",
    "                let series;\n",
    "                if (s.type === 'candlestick') {\n",
    "                    series = chart.addCandlestickSeries(s.options || {\n",
    "                        upColor: '#26a69a', downColor: '#ef5350', borderVisible: false,\n",
    "                        wickUpColor: '#26a69a', wickDownColor: '#ef5350'\n",
    "                    });\n",
    "                } else if (s.type === 'histogram') {\n",
    "                    series = chart.addHistogramSeries(s.options || {\n",
    "                        color: '#26a69a', priceFormat: { type: 'volume' },\n",
    "                        priceScaleId: 'volume', \n",
    "                    });\n",
    "                    if (s.options && s.options.priceScaleId === 'volume') {\n",
    "                        chart.priceScale('volume').applyOptions({\n",
    "                            scaleMargins: { top: 0.8, bottom: 0 },\n",
    "                        });\n",
    "                    }\n",
    "                } else if (s.type === 'line') {\n",
    "                    series = chart.addLineSeries(s.options || { color: '#2962FF', lineWidth: 2 });\n",
    "                }\n",
    "                if (series && s.data) series.setData(s.data);\n",
    "            });\n",
    "            \n",
    "            chart.timeScale().fitContent();\n",
    "            \n",
    "            window.addEventListener('resize', () => {\n",
    "                chart.resize(container.clientWidth, config.height || 400);\n",
    "            });\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "    charts_html = \"\"\n",
    "    for cfg in charts_config:\n",
    "        charts_html += f'<div class=\"chart-container\"><div class=\"chart-title\">{cfg.get(\"title\", \"\")}</div><div id=\"{cfg[\"id\"]}\" class=\"chart-div\"></div></div>'\n",
    "\n",
    "    html = html_template.replace('{{TITLE}}', title)\n",
    "    html = html.replace('{{CHARTS_HTML}}', charts_html)\n",
    "    html = html.replace('{{CONFIGS}}', json.dumps(charts_config))\n",
    "    html = html.replace('{{HEIGHT}}', str(400)) \n",
    "\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(html)\n",
    "\n",
    "\n",
    "def aggregate_usd_candles(dfs: List[pd.DataFrame], outlier_factor: float = 0.35) -> pd.DataFrame:\n",
    "    \"\"\"Robust daily aggregation (open/close VWAP; high=max, low=min) with outlier filtering.\n",
    "    Weights in USD = close * volume; applies Hyperliquid penalty.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for d in dfs:\n",
    "        if d is None or d.empty:\n",
    "            continue\n",
    "        dd = ensure_daily(d)\n",
    "        if \"exchange\" not in dd.columns:\n",
    "            dd[\"exchange\"] = d.get(\"exchange\", \"?\")\n",
    "        parts.append(dd[[\"date\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"exchange\"]])\n",
    "    if not parts:\n",
    "        return pd.DataFrame()\n",
    "    big = pd.concat(parts, ignore_index=True)\n",
    "    big = big.dropna(subset=[\"open\",\"high\",\"low\",\"close\",\"volume\"]).copy()\n",
    "    big = big[(big[\"close\"] > 0) & (big[\"open\"] > 0) & (big[\"high\"] > 0) & (big[\"low\"] > 0) & (big[\"volume\"] > 0)]\n",
    "\n",
    "    med = big.groupby(\"date\")[\"close\"].median().rename(\"med_close\")\n",
    "    big = big.merge(med, on=\"date\", how=\"left\")\n",
    "    lo, hi = (1.0 - outlier_factor), (1.0 + outlier_factor)\n",
    "    mask = (big[\"open\"]>=lo*big[\"med_close\"])&(big[\"open\"]<=hi*big[\"med_close\"])&\\\n",
    "           (big[\"high\"]<=hi*big[\"med_close\"])&(big[\"low\"]>=lo*big[\"med_close\"])&\\\n",
    "           (big[\"close\"]>=lo*big[\"med_close\"])&(big[\"close\"]<=hi*big[\"med_close\"]) \n",
    "    dropped = int((~mask).sum())\n",
    "    if dropped > 0:\n",
    "        per_ex = big.loc[~mask].groupby(\"exchange\").size().sort_values(ascending=False)\n",
    "        print(f\"[AGG] filtered outliers: {dropped} rows\")\n",
    "        for ex, n in per_ex.items():\n",
    "            print(f\"[AGG]   - {ex}: {n}\")\n",
    "    big = big[mask].drop(columns=[\"med_close\"]) if dropped>0 else big.drop(columns=[\"med_close\"]) \n",
    "\n",
    "    big[\"high\"] = big[[\"high\",\"open\",\"close\"]].max(axis=1)\n",
    "    big[\"low\"]  = big[[\"low\",\"open\",\"close\"]].min(axis=1)\n",
    "\n",
    "    big[\"w_usd\"] = big[\"close\"] * big[\"volume\"]\n",
    "    big.loc[big[\"exchange\"].str.contains(\"Hyperliquid\", case=False, na=False), \"w_usd\"] *= HYPERLIQUID_WEIGHT_PENALTY\n",
    "\n",
    "    grp = big.groupby(\"date\")\n",
    "    open_vw  = grp.apply(lambda g: np.average(g[\"open\"],  weights=g[\"w_usd\"]))\n",
    "    close_vw = grp.apply(lambda g: np.average(g[\"close\"], weights=g[\"w_usd\"]))\n",
    "    high_max = grp[\"high\"].max()\n",
    "    low_min  = grp[\"low\"].min()\n",
    "    vol_usd_sum = grp[\"w_usd\"].sum()\n",
    "\n",
    "    out = pd.DataFrame({\"open\": open_vw, \"close\": close_vw, \"high\": high_max, \"low\": low_min, \"volume_usd\": vol_usd_sum})\n",
    "    out.reset_index(inplace=True)\n",
    "    out[\"timestamp\"] = (out[\"date\"].dt.tz_convert(\"UTC\") if pd.api.types.is_datetime64tz_dtype(out[\"date\"]) else pd.to_datetime(out[\"date\"], utc=True))\n",
    "    out[\"symbol\"] = \"BTC/USD-AGG\"\n",
    "    out[\"exchange\"] = \"AGGREGATED\"\n",
    "    print(f\"[AGG] aggregated days: {len(out)}; range: {out['timestamp'].min().date()} \\u2192 {out['timestamp'].max().date()}\")\n",
    "    return out[[\"timestamp\",\"open\",\"high\",\"low\",\"close\",\"volume_usd\",\"symbol\",\"exchange\"]]\n",
    "\n",
    "\n",
    "def plot_aggregated_candles_with_volume(df_agg: pd.DataFrame, charts_dir: str, title: str = \"BTC Aggregated (Vol-Weighted)\") -> str:\n",
    "    if df_agg.empty:\n",
    "        print(\"[PLOT] No aggregated data to plot\")\n",
    "        return \"\"\n",
    "    \n",
    "    cdl_data = []\n",
    "    vol_data = []\n",
    "    for _, row in df_agg.iterrows():\n",
    "        t = int(row['timestamp'].timestamp())\n",
    "        cdl_data.append({\n",
    "            'time': t,\n",
    "            'open': float(row['open']),\n",
    "            'high': float(row['high']),\n",
    "            'low': float(row['low']),\n",
    "            'close': float(row['close'])\n",
    "        })\n",
    "        vol_data.append({\n",
    "            'time': t,\n",
    "            'value': float(row['volume_usd']),\n",
    "            'color': '#26a69a' if row['close'] >= row['open'] else '#ef5350'\n",
    "        })\n",
    "\n",
    "    config = [{\n",
    "        'id': 'agg_chart',\n",
    "        'title': title,\n",
    "        'height': 600,\n",
    "        'series': [\n",
    "            {'type': 'candlestick', 'data': cdl_data},\n",
    "            {'type': 'histogram', 'data': vol_data, 'options': {'priceScaleId': 'volume', 'color': '#26a69a'}}\n",
    "        ]\n",
    "    }]\n",
    "    \n",
    "    filepath = os.path.join(charts_dir, \"btc_aggregated_with_volume.html\")\n",
    "    generate_lightweight_chart_html(config, title, filepath)\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def plot_premiums(coinbase_prem: pd.DataFrame, kimchi_prem: pd.DataFrame, charts_dir: str) -> str:\n",
    "    configs = []\n",
    "    if coinbase_prem is not None and not coinbase_prem.empty:\n",
    "        cb_data = [{'time': int(row['date'].timestamp()), 'value': float(row['coinbase_premium_pct'])} for _, row in coinbase_prem.iterrows()]\n",
    "        configs.append({\n",
    "            'id': 'cb_premium',\n",
    "            'title': 'Coinbase Premium (%)',\n",
    "            'series': [{'type': 'line', 'data': cb_data, 'options': {'color': '#2962FF', 'lineWidth': 2}}]\n",
    "        })\n",
    "    if kimchi_prem is not None and not kimchi_prem.empty:\n",
    "        kimchi_data = [{'time': int(row['date'].timestamp()), 'value': float(row['kimchi_pct'])} for _, row in kimchi_prem.iterrows()]\n",
    "        configs.append({\n",
    "            'id': 'kimchi_premium',\n",
    "            'title': 'Kimchi Premium (%)',\n",
    "            'series': [{'type': 'line', 'data': kimchi_data, 'options': {'color': '#FF6D00', 'lineWidth': 2}}]\n",
    "        })\n",
    "    \n",
    "    filepath = os.path.join(charts_dir, \"premiums_cb_kimchi.html\")\n",
    "    generate_lightweight_chart_html(configs, \"BTC Premiums\", filepath)\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def create_individual_pair_charts(data: Dict[str, pd.DataFrame], charts_dir: str) -> Dict[str, str]:\n",
    "    os.makedirs(charts_dir, exist_ok=True)\n",
    "    charts = {}\n",
    "    print(\"\\n=== Creating Individual Pair Charts ===\")\n",
    "    for key, df in data.items():\n",
    "        if df is None or df.empty:\n",
    "            continue\n",
    "        exchange, symbol = key.split(\":\", 1)\n",
    "        cdl_data = []\n",
    "        vol_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            t = int(row['timestamp'].timestamp())\n",
    "            cdl_data.append({'time': t, 'open': float(row['open']), 'high': float(row['high']), 'low': float(row['low']), 'close': float(row['close'])})\n",
    "            vol_data.append({'time': t, 'value': float(row['volume']), 'color': '#26a69a' if row['close'] >= row['open'] else '#ef5350'})\n",
    "\n",
    "        currency = \"KRW\" if \"KRW\" in symbol else \"USD\"\n",
    "        title = f\"{exchange} - {symbol} (1D)\"\n",
    "        filename = f\"{exchange.lower()}_{symbol.replace('/', '_').lower()}_chart.html\"\n",
    "        filepath = os.path.join(charts_dir, filename)\n",
    "        \n",
    "        config = [{\n",
    "            'id': 'pair_chart',\n",
    "            'title': title,\n",
    "            'height': 600,\n",
    "            'series': [\n",
    "                {'type': 'candlestick', 'data': cdl_data},\n",
    "                {'type': 'histogram', 'data': vol_data, 'options': {'priceScaleId': 'volume'}}\n",
    "            ]\n",
    "        }]\n",
    "        generate_lightweight_chart_html(config, title, filepath)\n",
    "        charts[key] = filepath\n",
    "        print(f\"  \\u2713 Created chart for {exchange} {symbol} \\u2192 {filename}\")\n",
    "    return charts\n",
    "\n",
    "\n",
    "def create_comparison_grid(data: Dict[str, pd.DataFrame], charts_dir: str, pairs_to_compare: list = None) -> Dict[str, str]:\n",
    "    if pairs_to_compare is None:\n",
    "        pairs_to_compare = [\"BTC/USDT\", \"BTC/USDC\", \"BTC/USD\", \"BTC/KRW\"]\n",
    "    results = {}\n",
    "    for target_symbol in pairs_to_compare:\n",
    "        exchanges_with_pair = []\n",
    "        for key, df in data.items():\n",
    "            if df is None or df.empty:\n",
    "                continue\n",
    "            exchange, symbol = key.split(\":\", 1)\n",
    "            if symbol.replace(\"-\", \"/\").upper() == target_symbol.upper():\n",
    "                exchanges_with_pair.append((exchange, df))\n",
    "        \n",
    "        if len(exchanges_with_pair) > 1:\n",
    "            configs = []\n",
    "            for idx, (exchange, df) in enumerate(exchanges_with_pair):\n",
    "                cdl_data = [{'time': int(row['timestamp'].timestamp()), 'open': float(row['open']), 'high': float(row['high']), 'low': float(row['low']), 'close': float(row['close'])} for _, row in df.iterrows()]\n",
    "                configs.append({\n",
    "                    'id': f'comp_{idx}',\n",
    "                    'title': f\"{exchange} - {target_symbol}\",\n",
    "                    'height': 300,\n",
    "                    'series': [{'type': 'candlestick', 'data': cdl_data}]\n",
    "                })\n",
    "            \n",
    "            filename = f\"comparison_{target_symbol.replace('/', '_').lower()}.html\"\n",
    "            filepath = os.path.join(charts_dir, filename)\n",
    "            generate_lightweight_chart_html(configs, f\"Comparison: {target_symbol}\", filepath)\n",
    "            results[target_symbol] = filepath\n",
    "    return results\n",
    "\n",
    "\n",
    "def create_all_visualizations(data: Dict[str, pd.DataFrame], df_agg: pd.DataFrame, \n",
    "                              coinbase_prem: pd.DataFrame, kimchi_prem: pd.DataFrame,\n",
    "                              charts_dir: str = \"exchange_charts\") -> Dict:\n",
    "    os.makedirs(charts_dir, exist_ok=True)\n",
    "    results = {'individual_charts': {}, 'aggregated_chart': None, 'premium_charts': None, 'comparison_grids': {}}\n",
    "    \n",
    "    print(\"\\n=== Creating Aggregated Chart ===\")\n",
    "    results['aggregated_chart'] = plot_aggregated_candles_with_volume(df_agg, charts_dir)\n",
    "    \n",
    "    print(\"\\n=== Creating Individual Charts ===\")\n",
    "    results['individual_charts'] = create_individual_pair_charts(data, charts_dir)\n",
    "    \n",
    "    print(\"\\n=== Creating Comparison Grids ===\")\n",
    "    results['comparison_grids'] = create_comparison_grid(data, charts_dir)\n",
    "    \n",
    "    print(\"\\n=== Creating Premium Charts ===\")\n",
    "    results['premium_charts'] = plot_premiums(coinbase_prem, kimchi_prem, charts_dir)\n",
    "    \n",
    "    print(f\"\\n=== Visualization Summary ===\")\n",
    "    print(f\"Total charts created: {len(results['individual_charts']) + len(results['comparison_grids']) + 2}\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95c0f9de-dc49-4d67-89e5-939fcd8fbef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# Styled Tables (Rich + HTML)\n",
    "# ======================================================================================\n",
    "\n",
    "def _style_html_table(df: pd.DataFrame, title: str) -> str:\n",
    "    if df.empty:\n",
    "        return f\"<h3>{title}</h3><p>No data available</p>\"\n",
    "    def format_value(val, col):\n",
    "        if pd.isna(val):\n",
    "            return \"\"\n",
    "        if col in [\"share_change_pp\", \"slope_pp_per_30d\"]:\n",
    "            return f\"{val:+.2f}\"\n",
    "        elif col == \"avg_weight\":\n",
    "            return f\"{val*100:.2f}%\"\n",
    "        else:\n",
    "            return str(val)\n",
    "    def get_color_for_change(val):\n",
    "        if pd.isna(val):\n",
    "            return \"\"\n",
    "        if val > 0:\n",
    "            intensity = min(abs(val) / 10, 1) * 0.3\n",
    "            return f\"background-color: rgba(34, 197, 94, {intensity});\"\n",
    "        else:\n",
    "            intensity = min(abs(val) / 10, 1) * 0.3\n",
    "            return f\"background-color: rgba(239, 68, 68, {intensity});\"\n",
    "    def get_color_for_slope(val):\n",
    "        if pd.isna(val) or val <= 0:\n",
    "            return \"\"\n",
    "        intensity = min(val / 10, 1) * 0.2\n",
    "        return f\"background-color: rgba(59, 130, 246, {intensity});\"\n",
    "    def get_bar_width(val, max_val):\n",
    "        if pd.isna(val) or max_val == 0:\n",
    "            return 0\n",
    "        return min((val / max_val) * 100, 100)\n",
    "    html_rows = []\n",
    "    max_weight = df[\"avg_weight\"].max() if \"avg_weight\" in df.columns else 1\n",
    "    for _, row in df.iterrows():\n",
    "        cells = []\n",
    "        for col in df.columns:\n",
    "            val = row[col]\n",
    "            formatted = format_value(val, col)\n",
    "            style = \"\"\n",
    "            if col == \"share_change_pp\":\n",
    "                style = get_color_for_change(val)\n",
    "            elif col == \"slope_pp_per_30d\":\n",
    "                style = get_color_for_slope(val)\n",
    "            elif col == \"avg_weight\" and not pd.isna(val):\n",
    "                bar_width = get_bar_width(val, max_weight)\n",
    "                style = f\"background: linear-gradient(90deg, rgba(99, 102, 241, 0.2) {bar_width}%, transparent {bar_width}%);\"\n",
    "            cells.append(f'<td style=\"padding:6px 8px;{style}\">{formatted}</td>')\n",
    "        html_rows.append(f'<tr>{\"\".join(cells)}</tr>')\n",
    "    headers = \"\".join([f'<th style=\"text-align:center;padding:6px 8px;background:#f3f4f6;\">{col}</th>' for col in df.columns])\n",
    "    html = f\"\"\"\n",
    "    <table style=\"border-collapse:collapse;width:100%;margin:10px 0;border:1px solid #e5e7eb;\">\n",
    "        <caption style=\"caption-side:top;font-size:16px;font-weight:600;padding:8px 0;text-align:left;\">\n",
    "            {title}\n",
    "        </caption>\n",
    "        <thead><tr>{headers}</tr></thead>\n",
    "        <tbody>{\"\".join(html_rows)}</tbody>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    return html\n",
    "\n",
    "def _export_winners_losers_html(w7: pd.DataFrame, w30: pd.DataFrame, w90: pd.DataFrame, out_path: str):\n",
    "    sections = []\n",
    "    for label, df in [(\"7D\", w7), (\"30D\", w30), (\"90D\", w90)]:\n",
    "        if df is None or df.empty:\n",
    "            continue\n",
    "        winners = df.sort_values(\"share_change_pp\", ascending=False).head(3)[[\"name\",\"share_change_pp\",\"slope_pp_per_30d\",\"avg_weight\"]]\n",
    "        losers  = df.sort_values(\"share_change_pp\", ascending=True).head(3)[[\"name\",\"share_change_pp\",\"slope_pp_per_30d\",\"avg_weight\"]]\n",
    "        sections.append(f\"<h2 style='font-family:system-ui;margin:16px 0 8px'>{label}</h2>\")\n",
    "        sections.append(_style_html_table(winners.reset_index(drop=True), f\"Winners {label} (pp of share)\"))\n",
    "        sections.append(_style_html_table(losers.reset_index(drop=True),  f\"Losers {label} (pp of share)\"))\n",
    "    html = f\"\"\"\n",
    "    <html><head><meta charset=\"utf-8\"><title>Winners/Losers BTC</title></head>\n",
    "    <body style=\"font-family:system-ui; margin:24px\">\n",
    "      <h1 style=\"margin:0 0 8px\">Winners / Losers por cuota</h1>\n",
    "      <div style=\"color:#666; margin-bottom:16px\">Automatically generated</div>\n",
    "      {''.join(sections)}\n",
    "    </body></html>\n",
    "    \"\"\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "\n",
    "def _print_top(df: pd.DataFrame, label: str):\n",
    "    if df is None or df.empty:\n",
    "        print(f\"{label}: no data\"); return\n",
    "    top_g = df.sort_values(\"share_change_pp\", ascending=False).head(3)[[\"name\",\"share_change_pp\",\"slope_pp_per_30d\",\"avg_weight\"]]\n",
    "    top_l = df.sort_values(\"share_change_pp\", ascending=True).head(3)[[\"name\",\"share_change_pp\",\"slope_pp_per_30d\",\"avg_weight\"]]\n",
    "    try:\n",
    "        from rich.console import Console\n",
    "        from rich.table import Table\n",
    "        from rich import box\n",
    "        console = Console()\n",
    "        def _as_table(title: str, data: pd.DataFrame, color: str):\n",
    "            t = Table(title=title, box=box.SIMPLE_HEAVY)\n",
    "            for col in [\"name\",\"share_change_pp\",\"slope_pp_per_30d\",\"avg_weight\"]:\n",
    "                t.add_column(col, justify=\"right\" if col != \"name\" else \"left\", style=\"bold\" if col==\"name\" else \"\")\n",
    "            for _, r in data.iterrows():\n",
    "                t.add_row(str(r[\"name\"]), f\"{r['share_change_pp']:+.2f}\", f\"{r['slope_pp_per_30d']:+.2f}\", f\"{r['avg_weight']:.2%}\", style=color)\n",
    "            return t\n",
    "        console.print(_as_table(f\"Winners {label} (pp of share)\", top_g, \"green\"))\n",
    "        console.print(_as_table(f\"Losers {label} (pp of share)\", top_l, \"red\"))\n",
    "    except Exception:\n",
    "        print(f\"Winners {label} (pp of share):\")\n",
    "        print(top_g.to_string(index=False, float_format=lambda x: f\"{x:.2f}\"))\n",
    "        print(f\"Losers {label} (pp of share):\")\n",
    "        print(top_l.to_string(index=False, float_format=lambda x: f\"{x:.2f}\"))\n",
    "    return top_g, top_l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_imports",
   "metadata": {},
   "source": [
    "## 7. Performance and Resiliency Layer\n",
    "This section provides incremental data loading, weight computation, and summary utilities. It ensures that only necessary data is fetched and that the aggregation is mathematically sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cd8ed12-8de0-4d58-92f8-749011497f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# Incremental Layer, Weights, and Summary Utilities\n",
    "# ======================================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from typing import Optional, List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "def ensure_dataset(exchange: str, symbol: str, fetch_fn, *fetch_args, **fetch_kwargs) -> pd.DataFrame:\n",
    "    \"\"\"Load from CSV if it exists. If it already contains TODAY → skip. If today is missing → download yesterday + today and append. With logs.\"\"\"\n",
    "    fname = expected_csv_name(exchange, symbol)\n",
    "    fpath = os.path.join(DATA_DIR, fname)\n",
    "    today = today_utc_date()\n",
    "    existing = read_csv_if_exists(fpath)\n",
    "    if existing is not None and not existing.empty:\n",
    "        last_dates = existing[\"timestamp\"].dt.date\n",
    "        print(f\"[{exchange}] {symbol} → CSV: {len(existing)} rows; última={existing['timestamp'].max().date()}\")\n",
    "        if (last_dates == today).any():\n",
    "            print(f\"[{exchange}] {symbol} → ya incluye HOY → skip\")\n",
    "            return existing\n",
    "        print(f\"[{exchange}] {symbol} → falta HOY → descargar AYER+HOY\")\n",
    "        start_ms = two_day_start_ms_utc() if ONLY_LAST_2_DAYS_IF_MISSING else None\n",
    "        new_df = fetch_fn(*fetch_args, **{**fetch_kwargs, \"start_ms\": start_ms})\n",
    "        if new_df is None or new_df.empty:\n",
    "            print(f\"[{exchange}] {symbol} → sin datos nuevos; mantengo CSV\")\n",
    "            return existing\n",
    "        start_dt = pd.to_datetime(start_ms, unit='ms', utc=True) if start_ms else existing['timestamp'].max().floor('D')\n",
    "        new_df = new_df[new_df['timestamp'] >= start_dt]\n",
    "        merged = pd.concat([existing, new_df], ignore_index=True).drop_duplicates(subset=['timestamp']).sort_values('timestamp')\n",
    "        added = len(merged) - len(existing)\n",
    "        print(f\"[{exchange}] {symbol} → added {added} rows; new latest={merged['timestamp'].max().date()}\")\n",
    "        save_csv(merged, fname)\n",
    "        return merged\n",
    "    print(f\"[{exchange}] {symbol} → no CSV found → {'historical' if FULL_HIST_ON_FIRST_RUN else 'AYER+HOY'}\")\n",
    "    start_ms = None if FULL_HIST_ON_FIRST_RUN else two_day_start_ms_utc()\n",
    "    df = fetch_fn(*fetch_args, **{**fetch_kwargs, \"start_ms\": start_ms})\n",
    "    print(f\"[{exchange}] {symbol} → downloaded {0 if df is None else len(df)} rows\")\n",
    "    save_csv(df, fname)\n",
    "    return df\n",
    "\n",
    "def compute_daily_weights(named_dfs: List[tuple]) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for name, df in named_dfs:\n",
    "        if df is None or df.empty:\n",
    "            continue\n",
    "        tmp = ensure_daily(df)[[\"date\",\"close\",\"volume\"]].copy()\n",
    "        tmp[\"name\"] = name\n",
    "        tmp[\"notional_usd\"] = tmp[\"close\"] * tmp[\"volume\"]\n",
    "        if \"hyperliquid\" in name.lower():\n",
    "            tmp[\"notional_usd\"] *= HYPERLIQUID_WEIGHT_PENALTY\n",
    "        frames.append(tmp[[\"date\",\"name\",\"notional_usd\"]])\n",
    "    if not frames:\n",
    "        return pd.DataFrame()\n",
    "    allv = pd.concat(frames, ignore_index=True)\n",
    "    allv = allv.groupby([\"date\",\"name\"], as_index=False)[\"notional_usd\"].sum()\n",
    "    sums = allv.groupby(\"date\")[\"notional_usd\"].sum().rename(\"tot_notional_usd\")\n",
    "    m = allv.merge(sums, on=\"date\", how=\"left\")\n",
    "    m[\"weight\"] = np.where(m[\"tot_notional_usd\"] > 0, m[\"notional_usd\"] / m[\"tot_notional_usd\"], 0.0)\n",
    "    return m[[\"date\",\"name\",\"notional_usd\",\"tot_notional_usd\",\"weight\"]].sort_values([\"date\",\"weight\"], ascending=[True, False])\n",
    "\n",
    "def summarize_period_weights(weights: pd.DataFrame, days: int) -> pd.DataFrame:\n",
    "    \"\"\"Weight summary for the last N days, with change vs. previous period y trend (monthly slope).\"\"\"\n",
    "    if weights is None or weights.empty:\n",
    "        return pd.DataFrame()\n",
    "    last_day = weights[\"date\"].max()\n",
    "    if pd.isna(last_day):\n",
    "        return pd.DataFrame()\n",
    "    period_start = last_day - pd.Timedelta(days=days-1)\n",
    "    cur = weights[(weights[\"date\"] >= period_start) & (weights[\"date\"] <= last_day)].copy()\n",
    "    if cur.empty:\n",
    "        return pd.DataFrame()\n",
    "    prev_start = period_start - pd.Timedelta(days=days)\n",
    "    prev_end = period_start - pd.Timedelta(days=1)\n",
    "    prev = weights[(weights[\"date\"] >= prev_start) & (weights[\"date\"] <= prev_end)].copy()\n",
    "\n",
    "    cur_agg = cur.groupby(\"name\").agg(\n",
    "        notional_usd=(\"notional_usd\",\"sum\"),\n",
    "        avg_weight=(\"weight\",\"mean\"),\n",
    "        days_observed=(\"date\",\"count\")\n",
    "    ).reset_index()\n",
    "\n",
    "    prev_w = prev.groupby(\"name\")[\"weight\"].mean().rename(\"avg_weight_prev\").reset_index()\n",
    "    prev_n = prev.groupby(\"name\")[\"notional_usd\"].sum().rename(\"notional_usd_prev\").reset_index()\n",
    "\n",
    "    out = cur_agg.merge(prev_w, on=\"name\", how=\"left\").merge(prev_n, on=\"name\", how=\"left\")\n",
    "    out[\"avg_weight_prev\"] = out[\"avg_weight_prev\"].fillna(0.0)\n",
    "    out[\"notional_usd_prev\"] = out[\"notional_usd_prev\"].fillna(0.0)\n",
    "    out[\"share_change_pp\"] = (out[\"avg_weight\"] - out[\"avg_weight_prev\"]) * 100.0\n",
    "    out[\"notional_change_pct\"] = np.where(out[\"notional_usd_prev\"] > 0.0,\n",
    "                                           (out[\"notional_usd\"] - out[\"notional_usd_prev\"]) / out[\"notional_usd_prev\"] * 100.0,\n",
    "                                           np.nan)\n",
    "    slopes = []\n",
    "    for name, g in cur.groupby(\"name\"):\n",
    "        if len(g) >= 2:\n",
    "            x = (g[\"date\"] - g[\"date\"].min()).dt.days.astype(float)\n",
    "            y = g[\"weight\"].astype(float).values\n",
    "            slope = float(np.polyfit(x, y, 1)[0]) * 100.0 * 30.0\n",
    "        else:\n",
    "            slope = np.nan\n",
    "        slopes.append({\"name\": name, \"slope_pp_per_30d\": slope})\n",
    "    out = out.merge(pd.DataFrame(slopes), on=\"name\", how=\"left\")\n",
    "    return out.sort_values([\"avg_weight\"], ascending=[False]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f580b956-bc6b-449e-8d28-160297ca1296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# Check/Diagnostic Methods for Exchange Pairs (como OKX), in a standalone cell\n",
    "# ======================================================================================\n",
    "\n",
    "# ---------- OKX ----------\n",
    "def check_okx_pair_availability(symbol: str = \"BTC/USDC\") -> Dict:\n",
    "    instruments_url = \"https://www.okx.com/api/v5/public/instruments\"\n",
    "    okx_symbol = symbol.replace(\"/\", \"-\")\n",
    "    params = {\"instType\": \"SPOT\", \"instId\": okx_symbol}\n",
    "    try:\n",
    "        r = requests.get(instruments_url, params=params, headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            js = r.json()\n",
    "            if js.get(\"code\") == \"0\":\n",
    "                data = js.get(\"data\", [])\n",
    "                if data:\n",
    "                    inst = data[0]\n",
    "                    return {\n",
    "                        \"available\": True,\n",
    "                        \"symbol\": inst.get(\"instId\"),\n",
    "                        \"baseCcy\": inst.get(\"baseCcy\"),\n",
    "                        \"quoteCcy\": inst.get(\"quoteCcy\"),\n",
    "                        \"state\": inst.get(\"state\"),\n",
    "                        \"minSize\": inst.get(\"minSz\"),\n",
    "                        \"listTime\": inst.get(\"listTime\"),\n",
    "                        \"alias\": inst.get(\"alias\")\n",
    "                    }\n",
    "    except Exception as e:\n",
    "        print(f\"[OKX] Error checking pair availability: {e}\")\n",
    "    return {\"available\": False, \"symbol\": okx_symbol}\n",
    "\n",
    "def diagnose_okx_pair(symbol: str = \"BTC/USDC\"):\n",
    "    print(f\"\\n[OKX DIAGNOSTIC] Checking {symbol}...\")\n",
    "    info = check_okx_pair_availability(symbol)\n",
    "    if not info.get(\"available\"):\n",
    "        print(f\"[OKX DIAGNOSTIC] ❌ Pair {symbol} NOT FOUND on OKX Spot\")\n",
    "        print(f\"[OKX DIAGNOSTIC] Suggestion: Use BTC/USDT instead, which has better liquidity\")\n",
    "        return\n",
    "    print(f\"[OKX DIAGNOSTIC] ✅ Pair found: {info.get('symbol')}\")\n",
    "    print(f\"[OKX DIAGNOSTIC] State: {info.get('state')}\")\n",
    "    if info.get('listTime'):\n",
    "        list_date = pd.to_datetime(int(info['listTime']), unit='ms')\n",
    "        days_since_listing = (datetime.now() - list_date).days\n",
    "        print(f\"[OKX DIAGNOSTIC] Listed: {list_date.date()} ({days_since_listing} days ago)\")\n",
    "        if days_since_listing < 30:\n",
    "            print(f\"[OKX DIAGNOSTIC] ⚠️ This is a NEWLY LISTED pair (< 30 days)\")\n",
    "    ticker_url = \"https://www.okx.com/api/v5/market/ticker\"\n",
    "    params = {\"instId\": info.get('symbol')}\n",
    "    try:\n",
    "        r = requests.get(ticker_url, params=params, headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            js = r.json()\n",
    "            if js.get(\"code\") == \"0\" and js.get(\"data\"):\n",
    "                ticker = js[\"data\"][0]\n",
    "                vol24h = float(ticker.get(\"vol24h\", 0))\n",
    "                volCcy24h = float(ticker.get(\"volCcy24h\", 0))\n",
    "                print(f\"[OKX DIAGNOSTIC] 24h Volume: {vol24h:,.2f} BTC (${volCcy24h:,.0f})\")\n",
    "                if volCcy24h < 100000:\n",
    "                    print(f\"[OKX DIAGNOSTIC] ⚠️ LOW LIQUIDITY: 24h volume < $100k\")\n",
    "    except Exception as e:\n",
    "        print(f\"[OKX DIAGNOSTIC] Could not fetch volume data: {e}\")\n",
    "    print(\"[OKX DIAGNOSTIC] Complete\\n\")\n",
    "\n",
    "# ---------- Binance ----------\n",
    "def check_binance_pair_availability(symbol: str = \"BTC/USDT\") -> Dict:\n",
    "    url = \"https://api.binance.com/api/v3/exchangeInfo\"\n",
    "    sym = symbol.replace(\"/\", \"\")\n",
    "    try:\n",
    "        r = requests.get(url, params={\"symbol\": sym}, headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            js = r.json()\n",
    "            if js.get(\"symbols\"):\n",
    "                s = js[\"symbols\"][0]\n",
    "                return {\"available\": True, \"symbol\": s.get(\"symbol\"), \"base\": s.get(\"baseAsset\"),\n",
    "                        \"quote\": s.get(\"quoteAsset\"), \"status\": s.get(\"status\")}\n",
    "    except Exception as e:\n",
    "        print(f\"[Binance] Availability error: {e}\")\n",
    "    return {\"available\": False, \"symbol\": sym}\n",
    "\n",
    "def diagnose_binance_pair(symbol: str = \"BTC/USDT\"):\n",
    "    print(f\"\\n[Binance DIAGNOSTIC] Checking {symbol}...\")\n",
    "    info = check_binance_pair_availability(symbol)\n",
    "    if not info.get(\"available\"):\n",
    "        print(f\"[Binance DIAGNOSTIC] ❌ Pair not found\"); return\n",
    "    print(f\"[Binance DIAGNOSTIC] ✅ {info['symbol']} | status={info.get('status')} base={info.get('base')} quote={info.get('quote')}\")\n",
    "    # 24h ticker\n",
    "    try:\n",
    "        r = requests.get(\"https://api.binance.com/api/v3/ticker/24hr\",\n",
    "                         params={\"symbol\": info['symbol']}, headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            t = r.json()\n",
    "            print(f\"[Binance DIAGNOSTIC] 24h vol: {float(t.get('volume',0)):.4f} {info.get('base')} | quoteVol: {float(t.get('quoteVolume',0)):.0f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Binance DIAGNOSTIC] Ticker error: {e}\")\n",
    "\n",
    "# ---------- Bybit ----------\n",
    "def check_bybit_pair_availability(symbol: str = \"BTC/USDT\") -> Dict:\n",
    "    url = \"https://api.bybit.com/v5/market/instruments-info\"\n",
    "    sym = symbol.replace(\"/\", \"\")\n",
    "    try:\n",
    "        r = requests.get(url, params={\"category\":\"spot\",\"symbol\": sym}, headers=USER_AGENT, timeout=30)\n",
    "        js = r.json()\n",
    "        if js.get(\"retCode\") == 0 and js.get(\"result\", {}).get(\"list\"):\n",
    "            s = js[\"result\"][\"list\"][0]\n",
    "            return {\"available\": True, \"symbol\": s.get(\"symbol\"), \"status\": s.get(\"status\")}\n",
    "    except Exception as e:\n",
    "        print(f\"[Bybit] Availability error: {e}\")\n",
    "    return {\"available\": False, \"symbol\": sym}\n",
    "\n",
    "def diagnose_bybit_pair(symbol: str = \"BTC/USDT\"):\n",
    "    print(f\"\\n[Bybit DIAGNOSTIC] Checking {symbol}...\")\n",
    "    info = check_bybit_pair_availability(symbol)\n",
    "    if not info.get(\"available\"):\n",
    "        print(\"[Bybit DIAGNOSTIC] ❌ Pair not found\"); return\n",
    "    print(f\"[Bybit DIAGNOSTIC] ✅ {info['symbol']} | status={info.get('status')}\")\n",
    "    try:\n",
    "        r = requests.get(\"https://api.bybit.com/v5/market/tickers\",\n",
    "                         params={\"category\":\"spot\",\"symbol\": info['symbol']}, headers=USER_AGENT, timeout=30)\n",
    "        js = r.json()\n",
    "        if js.get(\"retCode\") == 0 and js.get(\"result\",{}).get(\"list\"):\n",
    "            t = js[\"result\"][\"list\"][0]\n",
    "            print(f\"[Bybit DIAGNOSTIC] 24h vol: {float(t.get('volume24h',0)):.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Bybit DIAGNOSTIC] Ticker error: {e}\")\n",
    "\n",
    "# ---------- Coinbase ----------\n",
    "def check_coinbase_product_availability(symbol: str = \"BTC/USD\") -> Dict:\n",
    "    prod = CoinbaseFetcher._pick_product(symbol)\n",
    "    url = f\"https://api.exchange.coinbase.com/products/{prod}\"\n",
    "    try:\n",
    "        r = requests.get(url, headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            js = r.json()\n",
    "            return {\"available\": True, \"id\": js.get(\"id\"), \"base\": js.get(\"base_currency\"), \"quote\": js.get(\"quote_currency\"),\n",
    "                    \"status\": js.get(\"status\")}\n",
    "    except Exception as e:\n",
    "        print(f\"[Coinbase] Availability error: {e}\")\n",
    "    return {\"available\": False, \"id\": prod}\n",
    "\n",
    "def diagnose_coinbase_product(symbol: str = \"BTC/USD\"):\n",
    "    print(f\"\\n[Coinbase DIAGNOSTIC] Checking {symbol}...\")\n",
    "    info = check_coinbase_product_availability(symbol)\n",
    "    if not info.get(\"available\"):\n",
    "        print(\"[Coinbase DIAGNOSTIC] ❌ Product not found\"); return\n",
    "    print(f\"[Coinbase DIAGNOSTIC] ✅ {info['id']} | status={info.get('status')} base={info.get('base')} quote={info.get('quote')}\")\n",
    "    try:\n",
    "        r = requests.get(f\"https://api.exchange.coinbase.com/products/{info['id']}/ticker\", headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            t = r.json()\n",
    "            print(f\"[Coinbase DIAGNOSTIC] 24h vol: {float(t.get('volume',0)):.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Coinbase DIAGNOSTIC] Ticker error: {e}\")\n",
    "\n",
    "# ---------- Crypto.com ----------\n",
    "def check_crypto_com_pair_availability(symbol: str = \"BTC/USDT\") -> Dict:\n",
    "    url = \"https://api.crypto.com/exchange/v1/public/get-instruments\"\n",
    "    inst = symbol.replace(\"/\", \"_\")\n",
    "    try:\n",
    "        r = requests.get(url, headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            js = r.json()\n",
    "            data = js.get(\"result\", {}).get(\"instruments\", [])\n",
    "            ok = any(x.get(\"instrument_name\") == inst for x in data)\n",
    "            return {\"available\": ok, \"instrument_name\": inst}\n",
    "    except Exception as e:\n",
    "        print(f\"[Crypto.com] Availability error: {e}\")\n",
    "    return {\"available\": False, \"instrument_name\": inst}\n",
    "\n",
    "def diagnose_crypto_com_pair(symbol: str = \"BTC/USDT\"):\n",
    "    print(f\"\\n[Crypto.com DIAGNOSTIC] Checking {symbol}...\")\n",
    "    info = check_crypto_com_pair_availability(symbol)\n",
    "    if not info.get(\"available\"):\n",
    "        print(\"[Crypto.com DIAGNOSTIC] ❌ Pair not found\"); return\n",
    "    print(f\"[Crypto.com DIAGNOSTIC] ✅ {info['instrument_name']}\")\n",
    "    try:\n",
    "        r = requests.get(\"https://api.crypto.com/exchange/v1/public/get-ticker\",\n",
    "                         params={\"instrument_name\": info['instrument_name']}, headers=USER_AGENT, timeout=30)\n",
    "        js = r.json()\n",
    "        if js.get(\"code\") == 0 and js.get(\"result\",{}).get(\"data\"):\n",
    "            t = js[\"result\"][\"data\"][0]\n",
    "            print(f\"[Crypto.com DIAGNOSTIC] 24h vol: {float(t.get('v',0)):.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Crypto.com DIAGNOSTIC] Ticker error: {e}\")\n",
    "\n",
    "# ---------- Hyperliquid ----------\n",
    "def check_hyperliquid_pair_availability(symbol: str = \"BTC/USDC\") -> Dict:\n",
    "    coin = symbol.split('/')[0].upper()\n",
    "    payload = {\"type\": \"candleSnapshot\", \"req\": {\"coin\": coin, \"interval\": \"1d\",\n",
    "                                                 \"startTime\": to_ms(datetime.now(timezone.utc)-timedelta(days=2)),\n",
    "                                                 \"endTime\": to_ms(datetime.now(timezone.utc))}}\n",
    "    try:\n",
    "        js = HyperliquidFetcher._post(payload)\n",
    "        ok = isinstance(js, list) and len(js) > 0\n",
    "        return {\"available\": ok, \"coin\": coin}\n",
    "    except Exception as e:\n",
    "        print(f\"[Hyperliquid] Availability error: {e}\")\n",
    "    return {\"available\": False, \"coin\": coin}\n",
    "\n",
    "def diagnose_hyperliquid_pair(symbol: str = \"BTC/USDC\"):\n",
    "    print(f\"\\n[Hyperliquid DIAGNOSTIC] Checking {symbol}...\")\n",
    "    info = check_hyperliquid_pair_availability(symbol)\n",
    "    if not info.get(\"available\"):\n",
    "        print(\"[Hyperliquid DIAGNOSTIC] ❌ Pair not available\"); return\n",
    "    print(f\"[Hyperliquid DIAGNOSTIC] ✅ {info['coin']}\")\n",
    "    # No simple public 24h ticker USD endpoint; candle snapshot acts as basic check.\n",
    "\n",
    "# ---------- Upbit ----------\n",
    "def check_upbit_pair_availability(symbol: str = \"BTC/KRW\") -> Dict:\n",
    "    market = \"KRW-BTC\" if symbol.upper().endswith(\"KRW\") else (\"USDT-BTC\" if symbol.upper().endswith(\"USDT\") else \"KRW-BTC\")\n",
    "    try:\n",
    "        r = requests.get(\"https://api.upbit.com/v1/market/all\", headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            arr = r.json()\n",
    "            ok = any(x.get(\"market\") == market for x in arr)\n",
    "            return {\"available\": ok, \"market\": market}\n",
    "    except Exception as e:\n",
    "        print(f\"[Upbit] Availability error: {e}\")\n",
    "    return {\"available\": False, \"market\": market}\n",
    "\n",
    "def diagnose_upbit_pair(symbol: str = \"BTC/KRW\"):\n",
    "    print(f\"\\n[Upbit DIAGNOSTIC] Checking {symbol}...\")\n",
    "    info = check_upbit_pair_availability(symbol)\n",
    "    if not info.get(\"available\"):\n",
    "        print(\"[Upbit DIAGNOSTIC] ❌ Market not found\"); return\n",
    "    print(f\"[Upbit DIAGNOSTIC] ✅ {info['market']}\")\n",
    "    try:\n",
    "        r = requests.get(\"https://api.upbit.com/v1/ticker\", params={\"markets\": info['market']}, headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            t = r.json()[0]\n",
    "            print(f\"[Upbit DIAGNOSTIC] 24h vol: {float(t.get('acc_trade_volume_24h',0)):.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Upbit DIAGNOSTIC] Ticker error: {e}\")\n",
    "\n",
    "# ---------- Bitget ----------\n",
    "def check_bitget_pair_availability(symbol: str = \"BTC/USDT\") -> Dict:\n",
    "    sym = symbol.replace(\"/\", \"\")\n",
    "    try:\n",
    "        r = requests.get(\"https://api.bitget.com/api/v2/spot/public/symbols\", headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            js = r.json()\n",
    "            data = js.get(\"data\", [])\n",
    "            ok = any(x.get(\"symbol\") == sym for x in data)\n",
    "            return {\"available\": ok, \"symbol\": sym}\n",
    "    except Exception as e:\n",
    "        print(f\"[Bitget] Availability error: {e}\")\n",
    "    return {\"available\": False, \"symbol\": sym}\n",
    "\n",
    "def diagnose_bitget_pair(symbol: str = \"BTC/USDT\"):\n",
    "    print(f\"\\n[Bitget DIAGNOSTIC] Checking {symbol}...\")\n",
    "    info = check_bitget_pair_availability(symbol)\n",
    "    if not info.get(\"available\"):\n",
    "        print(\"[Bitget DIAGNOSTIC] ❌ Pair not found\"); return\n",
    "    print(f\"[Bitget DIAGNOSTIC] ✅ {info['symbol']}\")\n",
    "    try:\n",
    "        r = requests.get(\"https://api.bitget.com/api/v2/spot/market/tickers\", params={\"symbol\": info['symbol']}, headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            js = r.json()\n",
    "            data = js.get(\"data\", [])\n",
    "            if data:\n",
    "                t = data[0]\n",
    "                print(f\"[Bitget DIAGNOSTIC] 24h vol: {float(t.get('baseVol',0)):.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Bitget DIAGNOSTIC] Ticker error: {e}\")\n",
    "\n",
    "# ---------- MEXC ----------\n",
    "def check_mexc_pair_availability(symbol: str = \"BTC/USDT\") -> Dict:\n",
    "    sym = symbol.replace(\"/\", \"\")\n",
    "    try:\n",
    "        r = requests.get(\"https://api.mexc.com/api/v3/exchangeInfo\", params={\"symbol\": sym}, headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            js = r.json()\n",
    "            s = js.get(\"symbols\", [])\n",
    "            if s:\n",
    "                rec = s[0]\n",
    "                return {\"available\": True, \"symbol\": rec.get(\"symbol\"), \"status\": rec.get(\"status\")}\n",
    "    except Exception as e:\n",
    "        print(f\"[MEXC] Availability error: {e}\")\n",
    "    return {\"available\": False, \"symbol\": sym}\n",
    "\n",
    "def diagnose_mexc_pair(symbol: str = \"BTC/USDT\"):\n",
    "    print(f\"\\n[MEXC DIAGNOSTIC] Checking {symbol}...\")\n",
    "    info = check_mexc_pair_availability(symbol)\n",
    "    if not info.get(\"available\"):\n",
    "        print(\"[MEXC DIAGNOSTIC] ❌ Pair not found\"); return\n",
    "    print(f\"[MEXC DIAGNOSTIC] ✅ {info['symbol']} | status={info.get('status')}\")\n",
    "    try:\n",
    "        r = requests.get(\"https://api.mexc.com/api/v3/ticker/24hr\", params={\"symbol\": info['symbol']}, headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            t = r.json()\n",
    "            print(f\"[MEXC DIAGNOSTIC] 24h vol: {float(t.get('volume',0)):.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[MEXC DIAGNOSTIC] Ticker error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_main",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "Entry point to run the data collection and analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc138163-4111-42dc-b671-6f87a16bd6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OKX DIAGNOSTIC] Checking BTC/USDC...\n",
      "[OKX DIAGNOSTIC] ✅ Pair found: BTC-USDC\n",
      "[OKX DIAGNOSTIC] State: live\n",
      "[OKX DIAGNOSTIC] Listed: 2025-08-21 (78 days ago)\n",
      "[OKX DIAGNOSTIC] 24h Volume: 6.68 BTC ($680,873)\n",
      "[OKX DIAGNOSTIC] Complete\n",
      "\n",
      "=== Descargando/Actualizando velas 1D ===\n",
      "[Binance] BTC/USDT → no hay CSV → histórico\n",
      "[Binance] BTC/USDT → descargadas 2260 filas\n",
      "[Binance] BTC/USDC → no hay CSV → histórico\n",
      "[Binance] BTC/USDC → descargadas 2097 filas\n",
      "[Bybit] BTC/USDT → no hay CSV → histórico\n",
      "[Bybit] BTC/USDT → descargadas 1587 filas\n",
      "[Bybit] BTC/USDC → no hay CSV → histórico\n",
      "[Bybit] BTC/USDC → descargadas 1430 filas\n",
      "[OKX] BTC/USDT → no hay CSV → histórico\n",
      "[OKX] Fetching BTC/USDT interval=1D\n",
      "[OKX] Step 1: Fetching recent data for BTC/USDT...\n",
      "[OKX] Got 100 recent candles\n",
      "[OKX] Recent data range: 2025-07-30 → 2025-11-06\n",
      "[OKX] Step 2: Fetching historical data for BTC/USDT...\n",
      "[OKX] Historical: 5 pages, 601 total candles, oldest so far: 2024-03-16\n",
      "[OKX] Historical: 10 pages, 1101 total candles, oldest so far: 2022-11-02\n",
      "[OKX] Historical: 15 pages, 1601 total candles, oldest so far: 2021-06-20\n",
      "[OKX] Historical: 20 pages, 2101 total candles, oldest so far: 2020-02-06\n",
      "[OKX] Historical: 25 pages, 2601 total candles, oldest so far: 2018-09-24\n",
      "[OKX] Historical fetch complete: 30 pages\n",
      "[OKX] BTC/USDT final dataset: 2950 rows\n",
      "[OKX] Date range: 2017-10-10 → 2025-11-06\n",
      "[OKX] BTC/USDT → descargadas 2950 filas\n",
      "[OKX] BTC/USDC → no hay CSV → histórico\n",
      "[OKX] Fetching BTC/USDC interval=1D\n",
      "[OKX] Step 1: Fetching recent data for BTC/USDC...\n",
      "[OKX] Got 79 recent candles\n",
      "[OKX] Recent data range: 2025-08-20 → 2025-11-06\n",
      "[OKX] Step 2: Fetching historical data for BTC/USDC...\n",
      "[OKX] No new historical data, stopping at page 0\n",
      "[OKX] BTC/USDC final dataset: 79 rows\n",
      "[OKX] Date range: 2025-08-20 → 2025-11-06\n",
      "[OKX] ⚠️ LIMITED DATA for BTC/USDC: Only 79 candles available\n",
      "[OKX] This suggests BTC/USDC is:\n",
      "[OKX]   - A newly listed pair on OKX\n",
      "[OKX]   - A low liquidity/volume pair\n",
      "[OKX]   - Or has limited historical data on this exchange\n",
      "[OKX] Consider using a different exchange for this pair if more history is needed\n",
      "[OKX] BTC/USDC → descargadas 79 filas\n",
      "[Coinbase] BTC/USD → no hay CSV → histórico\n",
      "[Coinbase] BTC/USD → descargadas 2260 filas\n",
      "[Crypto.com] BTC/USDT → no hay CSV → histórico\n",
      "[Crypto.com] BTC/USDT → descargadas 1873 filas\n",
      "[Crypto.com] BTC/USDC → no hay CSV → histórico\n",
      "[Crypto.com] BTC/USDC → descargadas 1133 filas\n",
      "[Hyperliquid] BTC/USDC → no hay CSV → histórico\n",
      "[Hyperliquid] Fetch BTC/USDC interval=1d since=2019-09-01 → 2025-11-07\n",
      "[Hyperliquid] Fetched rows: 1907; range: 2020-08-19 → 2025-11-07\n",
      "[Hyperliquid] BTC/USDC → descargadas 1907 filas\n",
      "[Upbit] BTC/KRW → no hay CSV → histórico\n",
      "[Upbit] Fetching BTC/KRW (market: KRW-BTC)...\n",
      "[Upbit] Page 1: 200 candles, oldest: 2025-04-22\n",
      "[Upbit] Page 2: 200 candles, oldest: 2024-10-04\n",
      "[Upbit] Page 3: 200 candles, oldest: 2024-03-18\n",
      "[Upbit] Page 4: 200 candles, oldest: 2023-08-31\n",
      "[Upbit] Page 5: 200 candles, oldest: 2023-02-12\n",
      "[Upbit] Page 6: 200 candles, oldest: 2022-07-27\n",
      "[Upbit] Page 7: 200 candles, oldest: 2022-01-08\n",
      "[Upbit] Page 8: 200 candles, oldest: 2021-06-22\n",
      "[Upbit] Page 9: 200 candles, oldest: 2020-12-04\n",
      "[Upbit] Page 10: 200 candles, oldest: 2020-05-18\n",
      "[Upbit] Page 11: 200 candles, oldest: 2019-10-31\n",
      "[Upbit] Page 12: 200 candles, oldest: 2019-04-14\n",
      "[Upbit] Page 13: 200 candles, oldest: 2018-09-26\n",
      "[Upbit] Page 14: 200 candles, oldest: 2018-03-10\n",
      "[Upbit] Page 15: 166 candles, oldest: 2017-09-25\n",
      "[Upbit] Got 166 < 200 candles, reached end of data\n",
      "[Upbit] Final dataset: 2966 rows, range: 2017-09-25 → 2025-11-07\n",
      "[Upbit] BTC/KRW → descargadas 2966 filas\n",
      "[Bitget] BTC/USDT → no hay CSV → histórico\n",
      "[Bitget] Fetched rows: 2650; range: 2018-07-24 → 2025-11-06\n",
      "[Bitget] BTC/USDT → descargadas 2650 filas\n",
      "[Bitget] BTC/USDC → no hay CSV → histórico\n",
      "[Bitget] Fetched rows: 1256; range: 2022-05-25 → 2025-11-06\n",
      "[Bitget] BTC/USDC → descargadas 1256 filas\n",
      "[MEXC] BTC/USDT → no hay CSV → histórico\n",
      "[MEXC] BTC/USDT → descargadas 1000 filas\n",
      "[MEXC] BTC/USDC → no hay CSV → histórico\n",
      "[MEXC] BTC/USDC → descargadas 1000 filas\n",
      "=== Descargando USDKRW ===\n",
      "[FX] USDKRW fuente: Frankfurter https://api.frankfurter.app; rango: 2017-09-25 → 2025-11-03 (mediana=1204.91)\n",
      "=== Calculando Coinbase Premium ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pedro\\AppData\\Local\\Temp\\ipykernel_378520\\3912009310.py:66: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = ref.groupby(\"date\").apply(lambda g: pd.Series({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calculando Kimchi Premium ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pedro\\AppData\\Local\\Temp\\ipykernel_378520\\970366390.py:88: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref_usd = ref_concat.groupby(\"date\").apply(lambda g: pd.Series({\n",
      "C:\\Users\\Pedro\\AppData\\Local\\Temp\\ipykernel_378520\\970366390.py:92: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  ref_usd[\"timestamp\"] = (ref_usd[\"date\"].dt.tz_convert(\"UTC\") if pd.api.types.is_datetime64tz_dtype(ref_usd[\"date\"]) else pd.to_datetime(ref_usd[\"date\"], utc=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Construyendo vela agregada (USD) ===\n",
      "[AGG] outliers filtrados: 7 filas\n",
      "[AGG]   - Binance: 3\n",
      "[AGG]   - Bitget: 1\n",
      "[AGG]   - Bybit: 1\n",
      "[AGG]   - Coinbase: 1\n",
      "[AGG]   - OKX: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pedro\\AppData\\Local\\Temp\\ipykernel_378520\\2142712328.py:44: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  open_vw  = grp.apply(lambda g: np.average(g[\"open\"],  weights=g[\"w_usd\"]))\n",
      "C:\\Users\\Pedro\\AppData\\Local\\Temp\\ipykernel_378520\\2142712328.py:45: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  close_vw = grp.apply(lambda g: np.average(g[\"close\"], weights=g[\"w_usd\"]))\n",
      "C:\\Users\\Pedro\\AppData\\Local\\Temp\\ipykernel_378520\\2142712328.py:52: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  out[\"timestamp\"] = (out[\"date\"].dt.tz_convert(\"UTC\") if pd.api.types.is_datetime64tz_dtype(out[\"date\"]) else pd.to_datetime(out[\"date\"], utc=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AGG] días agregados: 2949; rango: 2017-10-11 → 2025-11-07 (vol_usd sum ok)\n",
      "=== Pesos por exchange (diario) ===\n",
      "Pesos última fecha:\n",
      "               name  weight\n",
      "      MEXC BTC/USDT  25.58%\n",
      "   Binance BTC/USDT  25.04%\n",
      "Hyperliquid BTC/USD  20.17%\n",
      "     Bybit BTC/USDT  11.19%\n",
      "   Coinbase BTC/USD   6.50%\n",
      "Crypto.com BTC/USDT   5.42%\n",
      "   Binance BTC/USDC   4.72%\n",
      "     Bybit BTC/USDC   1.06%\n",
      "      MEXC BTC/USDC   0.32%\n",
      "Crypto.com BTC/USDC   0.00%\n",
      "=== Pesos 7D/30D/90D y tendencias ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Ganadores 7D (pp de cuota)                      </span>\n",
       "                                                                      \n",
       " <span style=\"font-weight: bold\"> name             </span> <span style=\"font-weight: bold\"> share_change_pp </span> <span style=\"font-weight: bold\"> slope_pp_per_30d </span> <span style=\"font-weight: bold\"> avg_weight </span> \n",
       " ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Binance BTC/USDT </span> <span style=\"color: #008000; text-decoration-color: #008000\">           +2.93 </span> <span style=\"color: #008000; text-decoration-color: #008000\">           +20.55 </span> <span style=\"color: #008000; text-decoration-color: #008000\">     21.46% </span> \n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Bitget BTC/USDT  </span> <span style=\"color: #008000; text-decoration-color: #008000\">           +1.58 </span> <span style=\"color: #008000; text-decoration-color: #008000\">           -11.51 </span> <span style=\"color: #008000; text-decoration-color: #008000\">      4.41% </span> \n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Coinbase BTC/USD </span> <span style=\"color: #008000; text-decoration-color: #008000\">           +1.10 </span> <span style=\"color: #008000; text-decoration-color: #008000\">           +10.14 </span> <span style=\"color: #008000; text-decoration-color: #008000\">      6.51% </span> \n",
       "                                                                      \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Ganadores 7D (pp de cuota)                      \u001b[0m\n",
       "                                                                      \n",
       " \u001b[1m \u001b[0m\u001b[1mname            \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mshare_change_pp\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mslope_pp_per_30d\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mavg_weight\u001b[0m\u001b[1m \u001b[0m \n",
       " ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \n",
       " \u001b[1;32m \u001b[0m\u001b[1;32mBinance BTC/USDT\u001b[0m\u001b[1;32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          +2.93\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          +20.55\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m    21.46%\u001b[0m\u001b[32m \u001b[0m \n",
       " \u001b[1;32m \u001b[0m\u001b[1;32mBitget BTC/USDT \u001b[0m\u001b[1;32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          +1.58\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          -11.51\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m     4.41%\u001b[0m\u001b[32m \u001b[0m \n",
       " \u001b[1;32m \u001b[0m\u001b[1;32mCoinbase BTC/USD\u001b[0m\u001b[1;32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          +1.10\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          +10.14\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m     6.51%\u001b[0m\u001b[32m \u001b[0m \n",
       "                                                                      \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                       Perdedores 7D (pp de cuota)                       </span>\n",
       "                                                                         \n",
       " <span style=\"font-weight: bold\"> name                </span> <span style=\"font-weight: bold\"> share_change_pp </span> <span style=\"font-weight: bold\"> slope_pp_per_30d </span> <span style=\"font-weight: bold\"> avg_weight </span> \n",
       " ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Hyperliquid BTC/USD </span> <span style=\"color: #800000; text-decoration-color: #800000\">           -2.75 </span> <span style=\"color: #800000; text-decoration-color: #800000\">           +21.72 </span> <span style=\"color: #800000; text-decoration-color: #800000\">     20.94% </span> \n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> MEXC BTC/USDT       </span> <span style=\"color: #800000; text-decoration-color: #800000\">           -0.67 </span> <span style=\"color: #800000; text-decoration-color: #800000\">            +9.77 </span> <span style=\"color: #800000; text-decoration-color: #800000\">     20.66% </span> \n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Bybit BTC/USDT      </span> <span style=\"color: #800000; text-decoration-color: #800000\">           -0.48 </span> <span style=\"color: #800000; text-decoration-color: #800000\">            +7.75 </span> <span style=\"color: #800000; text-decoration-color: #800000\">      8.10% </span> \n",
       "                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                       Perdedores 7D (pp de cuota)                       \u001b[0m\n",
       "                                                                         \n",
       " \u001b[1m \u001b[0m\u001b[1mname               \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mshare_change_pp\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mslope_pp_per_30d\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mavg_weight\u001b[0m\u001b[1m \u001b[0m \n",
       " ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \n",
       " \u001b[1;31m \u001b[0m\u001b[1;31mHyperliquid BTC/USD\u001b[0m\u001b[1;31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m          -2.75\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m          +21.72\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m    20.94%\u001b[0m\u001b[31m \u001b[0m \n",
       " \u001b[1;31m \u001b[0m\u001b[1;31mMEXC BTC/USDT      \u001b[0m\u001b[1;31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m          -0.67\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m           +9.77\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m    20.66%\u001b[0m\u001b[31m \u001b[0m \n",
       " \u001b[1;31m \u001b[0m\u001b[1;31mBybit BTC/USDT     \u001b[0m\u001b[1;31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m          -0.48\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m           +7.75\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m     8.10%\u001b[0m\u001b[31m \u001b[0m \n",
       "                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                     Ganadores 30D (pp de cuota)                      </span>\n",
       "                                                                      \n",
       " <span style=\"font-weight: bold\"> name             </span> <span style=\"font-weight: bold\"> share_change_pp </span> <span style=\"font-weight: bold\"> slope_pp_per_30d </span> <span style=\"font-weight: bold\"> avg_weight </span> \n",
       " ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Binance BTC/USDT </span> <span style=\"color: #008000; text-decoration-color: #008000\">           +3.05 </span> <span style=\"color: #008000; text-decoration-color: #008000\">            +3.08 </span> <span style=\"color: #008000; text-decoration-color: #008000\">     19.27% </span> \n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> OKX BTC/USDT     </span> <span style=\"color: #008000; text-decoration-color: #008000\">           +1.41 </span> <span style=\"color: #008000; text-decoration-color: #008000\">            +1.61 </span> <span style=\"color: #008000; text-decoration-color: #008000\">      7.82% </span> \n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Binance BTC/USDC </span> <span style=\"color: #008000; text-decoration-color: #008000\">           +0.65 </span> <span style=\"color: #008000; text-decoration-color: #008000\">            +0.77 </span> <span style=\"color: #008000; text-decoration-color: #008000\">      4.71% </span> \n",
       "                                                                      \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                     Ganadores 30D (pp de cuota)                      \u001b[0m\n",
       "                                                                      \n",
       " \u001b[1m \u001b[0m\u001b[1mname            \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mshare_change_pp\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mslope_pp_per_30d\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mavg_weight\u001b[0m\u001b[1m \u001b[0m \n",
       " ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \n",
       " \u001b[1;32m \u001b[0m\u001b[1;32mBinance BTC/USDT\u001b[0m\u001b[1;32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          +3.05\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m           +3.08\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m    19.27%\u001b[0m\u001b[32m \u001b[0m \n",
       " \u001b[1;32m \u001b[0m\u001b[1;32mOKX BTC/USDT    \u001b[0m\u001b[1;32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          +1.41\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m           +1.61\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m     7.82%\u001b[0m\u001b[32m \u001b[0m \n",
       " \u001b[1;32m \u001b[0m\u001b[1;32mBinance BTC/USDC\u001b[0m\u001b[1;32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          +0.65\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m           +0.77\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m     4.71%\u001b[0m\u001b[32m \u001b[0m \n",
       "                                                                      \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                     Perdedores 30D (pp de cuota)                     </span>\n",
       "                                                                      \n",
       " <span style=\"font-weight: bold\"> name             </span> <span style=\"font-weight: bold\"> share_change_pp </span> <span style=\"font-weight: bold\"> slope_pp_per_30d </span> <span style=\"font-weight: bold\"> avg_weight </span> \n",
       " ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Bitget BTC/USDT  </span> <span style=\"color: #800000; text-decoration-color: #800000\">           -3.61 </span> <span style=\"color: #800000; text-decoration-color: #800000\">            -7.39 </span> <span style=\"color: #800000; text-decoration-color: #800000\">      5.37% </span> \n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Bybit BTC/USDT   </span> <span style=\"color: #800000; text-decoration-color: #800000\">           -1.69 </span> <span style=\"color: #800000; text-decoration-color: #800000\">            -0.65 </span> <span style=\"color: #800000; text-decoration-color: #800000\">      8.48% </span> \n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Coinbase BTC/USD </span> <span style=\"color: #800000; text-decoration-color: #800000\">           -0.55 </span> <span style=\"color: #800000; text-decoration-color: #800000\">            -0.04 </span> <span style=\"color: #800000; text-decoration-color: #800000\">      6.24% </span> \n",
       "                                                                      \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                     Perdedores 30D (pp de cuota)                     \u001b[0m\n",
       "                                                                      \n",
       " \u001b[1m \u001b[0m\u001b[1mname            \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mshare_change_pp\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mslope_pp_per_30d\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mavg_weight\u001b[0m\u001b[1m \u001b[0m \n",
       " ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \n",
       " \u001b[1;31m \u001b[0m\u001b[1;31mBitget BTC/USDT \u001b[0m\u001b[1;31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m          -3.61\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m           -7.39\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m     5.37%\u001b[0m\u001b[31m \u001b[0m \n",
       " \u001b[1;31m \u001b[0m\u001b[1;31mBybit BTC/USDT  \u001b[0m\u001b[1;31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m          -1.69\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m           -0.65\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m     8.48%\u001b[0m\u001b[31m \u001b[0m \n",
       " \u001b[1;31m \u001b[0m\u001b[1;31mCoinbase BTC/USD\u001b[0m\u001b[1;31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m          -0.55\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m           -0.04\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m     6.24%\u001b[0m\u001b[31m \u001b[0m \n",
       "                                                                      \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                       Ganadores 90D (pp de cuota)                       </span>\n",
       "                                                                         \n",
       " <span style=\"font-weight: bold\"> name                </span> <span style=\"font-weight: bold\"> share_change_pp </span> <span style=\"font-weight: bold\"> slope_pp_per_30d </span> <span style=\"font-weight: bold\"> avg_weight </span> \n",
       " ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> MEXC BTC/USDT       </span> <span style=\"color: #008000; text-decoration-color: #008000\">           +7.89 </span> <span style=\"color: #008000; text-decoration-color: #008000\">            +3.61 </span> <span style=\"color: #008000; text-decoration-color: #008000\">     18.00% </span> \n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Crypto.com BTC/USDT </span> <span style=\"color: #008000; text-decoration-color: #008000\">           +0.70 </span> <span style=\"color: #008000; text-decoration-color: #008000\">            -0.14 </span> <span style=\"color: #008000; text-decoration-color: #008000\">      5.35% </span> \n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> OKX BTC/USDT        </span> <span style=\"color: #008000; text-decoration-color: #008000\">           +0.12 </span> <span style=\"color: #008000; text-decoration-color: #008000\">            +0.45 </span> <span style=\"color: #008000; text-decoration-color: #008000\">      7.07% </span> \n",
       "                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                       Ganadores 90D (pp de cuota)                       \u001b[0m\n",
       "                                                                         \n",
       " \u001b[1m \u001b[0m\u001b[1mname               \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mshare_change_pp\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mslope_pp_per_30d\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mavg_weight\u001b[0m\u001b[1m \u001b[0m \n",
       " ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \n",
       " \u001b[1;32m \u001b[0m\u001b[1;32mMEXC BTC/USDT      \u001b[0m\u001b[1;32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          +7.89\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m           +3.61\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m    18.00%\u001b[0m\u001b[32m \u001b[0m \n",
       " \u001b[1;32m \u001b[0m\u001b[1;32mCrypto.com BTC/USDT\u001b[0m\u001b[1;32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          +0.70\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m           -0.14\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m     5.35%\u001b[0m\u001b[32m \u001b[0m \n",
       " \u001b[1;32m \u001b[0m\u001b[1;32mOKX BTC/USDT       \u001b[0m\u001b[1;32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          +0.12\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m           +0.45\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m     7.07%\u001b[0m\u001b[32m \u001b[0m \n",
       "                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Perdedores 90D (pp de cuota)                       </span>\n",
       "                                                                         \n",
       " <span style=\"font-weight: bold\"> name                </span> <span style=\"font-weight: bold\"> share_change_pp </span> <span style=\"font-weight: bold\"> slope_pp_per_30d </span> <span style=\"font-weight: bold\"> avg_weight </span> \n",
       " ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Hyperliquid BTC/USD </span> <span style=\"color: #800000; text-decoration-color: #800000\">           -3.76 </span> <span style=\"color: #800000; text-decoration-color: #800000\">            -1.78 </span> <span style=\"color: #800000; text-decoration-color: #800000\">     22.52% </span> \n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Bybit BTC/USDT      </span> <span style=\"color: #800000; text-decoration-color: #800000\">           -1.32 </span> <span style=\"color: #800000; text-decoration-color: #800000\">            +0.02 </span> <span style=\"color: #800000; text-decoration-color: #800000\">      9.04% </span> \n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Bitget BTC/USDT     </span> <span style=\"color: #800000; text-decoration-color: #800000\">           -0.75 </span> <span style=\"color: #800000; text-decoration-color: #800000\">            -2.07 </span> <span style=\"color: #800000; text-decoration-color: #800000\">      7.94% </span> \n",
       "                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Perdedores 90D (pp de cuota)                       \u001b[0m\n",
       "                                                                         \n",
       " \u001b[1m \u001b[0m\u001b[1mname               \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mshare_change_pp\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mslope_pp_per_30d\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mavg_weight\u001b[0m\u001b[1m \u001b[0m \n",
       " ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \n",
       " \u001b[1;31m \u001b[0m\u001b[1;31mHyperliquid BTC/USD\u001b[0m\u001b[1;31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m          -3.76\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m           -1.78\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m    22.52%\u001b[0m\u001b[31m \u001b[0m \n",
       " \u001b[1;31m \u001b[0m\u001b[1;31mBybit BTC/USDT     \u001b[0m\u001b[1;31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m          -1.32\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m           +0.02\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m     9.04%\u001b[0m\u001b[31m \u001b[0m \n",
       " \u001b[1;31m \u001b[0m\u001b[1;31mBitget BTC/USDT    \u001b[0m\u001b[1;31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m          -0.75\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m           -2.07\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m     7.94%\u001b[0m\u001b[31m \u001b[0m \n",
       "                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Tablas (HTML): exchange_charts\\winners_losers.html\n",
      "=== Creando todas las visualizaciones ===\n",
      "\n",
      "=== Creating Aggregated Chart with Volume ===\n",
      "  ✓ Saved aggregated chart → exchange_charts\\btc_aggregated_with_volume.html\n",
      "\n",
      "=== Creating Individual Pair Charts ===\n",
      "  ✓ Created chart for Binance BTC/USDT → binance_btc_usdt_chart.html\n",
      "  ✓ Created chart for Binance BTC/USDC → binance_btc_usdc_chart.html\n",
      "  ✓ Created chart for Bybit BTC/USDT → bybit_btc_usdt_chart.html\n",
      "  ✓ Created chart for Bybit BTC/USDC → bybit_btc_usdc_chart.html\n",
      "  ✓ Created chart for OKX BTC/USDT → okx_btc_usdt_chart.html\n",
      "  ✓ Created chart for OKX BTC/USDC → okx_btc_usdc_chart.html\n",
      "  ✓ Created chart for Coinbase BTC/USD → coinbase_btc_usd_chart.html\n",
      "  ✓ Created chart for Crypto.com BTC/USDT → crypto.com_btc_usdt_chart.html\n",
      "  ✓ Created chart for Crypto.com BTC/USDC → crypto.com_btc_usdc_chart.html\n",
      "  ✓ Created chart for Hyperliquid BTC/USDC → hyperliquid_btc_usdc_chart.html\n",
      "  ✓ Created chart for Upbit BTC/KRW → upbit_btc_krw_chart.html\n",
      "  ✓ Created chart for Bitget BTC/USDT → bitget_btc_usdt_chart.html\n",
      "  ✓ Created chart for Bitget BTC/USDC → bitget_btc_usdc_chart.html\n",
      "  ✓ Created chart for MEXC BTC/USDT → mexc_btc_usdt_chart.html\n",
      "  ✓ Created chart for MEXC BTC/USDC → mexc_btc_usdc_chart.html\n",
      "Total individual charts created: 15\n",
      "\n",
      "=== Creating Comparison Grids ===\n",
      "  ✓ Created comparison for BTC/USDT → comparison_btc_usdt.html\n",
      "  ✓ Created comparison for BTC/USDC → comparison_btc_usdc.html\n",
      "\n",
      "=== Visualization Summary ===\n",
      "Total charts created: 19\n",
      "Location: C:\\Users\\Pedro\\Documents\\GitHub\\BTC-TestGraph\\exchange_charts\n",
      "\n",
      "=== Resumen de Visualizaciones ===\n",
      "✓ Gráfico agregado con volumen: exchange_charts\\btc_aggregated_with_volume.html\n",
      "✓ Gráficos individuales creados: 15\n",
      "✓ Grids de comparación: 2\n",
      "✓ Ubicación: C:\\Users\\Pedro\\Documents\\GitHub\\BTC-TestGraph\\exchange_charts\n"
     ]
    }
   ],
   "source": [
    "# ================================ MAIN =====================================\n",
    "\n",
    "def main():\n",
    "    diagnose_okx_pair(\"BTC/USDC\")\n",
    "    start_hist_ms = to_ms(datetime(2019,9,1,tzinfo=timezone.utc))\n",
    "    print(\"=== Downloading/Updating 1D candles ===\")\n",
    "    tasks = [\n",
    "        (\"Binance\", \"BTC/USDT\", BinanceFetcher.fetch, {\"interval\":\"1d\", \"start_ms\": start_hist_ms}),\n",
    "        (\"Binance\", \"BTC/USDC\", BinanceFetcher.fetch, {\"interval\":\"1d\", \"start_ms\": start_hist_ms}),\n",
    "        (\"Bybit\",   \"BTC/USDT\", BybitFetcher.fetch,   {\"interval\":\"D\",   \"start_ms\": start_hist_ms}),\n",
    "        (\"Bybit\",   \"BTC/USDC\", BybitFetcher.fetch,   {\"interval\":\"D\",   \"start_ms\": start_hist_ms}),\n",
    "        (\"OKX\", \"BTC/USDT\", OKXFetcher.fetch, {\"interval\":\"1D\", \"start_ms\": start_hist_ms}),\n",
    "        (\"OKX\", \"BTC/USDC\", OKXFetcher.fetch, {\"interval\":\"1D\", \"start_ms\": start_hist_ms}),\n",
    "        (\"Coinbase\",\"BTC/USD\",  CoinbaseFetcher.fetch,{\"granularity_sec\":86400, \"start_ms\": start_hist_ms, \"include_partial_today\": True}),\n",
    "        (\"Crypto.com\",\"BTC/USDT\", CryptoComFetcher.fetch, {\"interval\":\"1D\", \"start_ms\": start_hist_ms}),\n",
    "        (\"Crypto.com\",\"BTC/USDC\", CryptoComFetcher.fetch, {\"interval\":\"1D\", \"start_ms\": start_hist_ms}),\n",
    "        (\"Hyperliquid\",\"BTC/USDC\", HyperliquidFetcher.fetch, {\"interval\":\"1d\", \"start_ms\": start_hist_ms}),\n",
    "        #(\"Hyperliquid\",\"BTC/USDT\", HyperliquidFetcher.fetch, {\"interval\":\"1d\", \"start_ms\": start_hist_ms}),\n",
    "        (\"Upbit\",   \"BTC/KRW\",  UpbitFetcher.fetch_days, {\"start_ms\": start_hist_ms}),\n",
    "        # Nuevos exchanges\n",
    "        (\"Bitget\",  \"BTC/USDT\", BitgetFetcher.fetch, {\"interval\": \"1D\", \"start_ms\": start_hist_ms}),\n",
    "        (\"Bitget\",  \"BTC/USDC\", BitgetFetcher.fetch, {\"interval\": \"1D\", \"start_ms\": start_hist_ms}),\n",
    "        (\"MEXC\",    \"BTC/USDT\", MEXCFetcher.fetch,   {\"interval\": \"1d\", \"start_ms\": start_hist_ms}),\n",
    "        (\"MEXC\",    \"BTC/USDC\", MEXCFetcher.fetch,   {\"interval\": \"1d\", \"start_ms\": start_hist_ms}),\n",
    "    ]\n",
    "\n",
    "    data: Dict[str, pd.DataFrame] = {}\n",
    "    named_usd_datasets: List[tuple] = []\n",
    "\n",
    "    for exch, sym, fn, kwargs in tasks:\n",
    "        df = ensure_dataset(exch, sym, fn, sym, **kwargs)\n",
    "        key = f\"{exch}:{sym}\"\n",
    "        data[key] = df\n",
    "\n",
    "    # Referencia USD (excluye Coinbase para premium CB). USD-like = USD/USDT/USDC, excluye Upbit.\n",
    "    def is_usd_like(exch: str, sym: str) -> bool:\n",
    "        return (sym.endswith(\"/USD\") or sym.endswith(\"/USDT\") or sym.endswith(\"/USDC\")) and exch != \"Upbit\"\n",
    "\n",
    "    # Canonicalización para evitar duplicados de la misma fuente\n",
    "    # - Hyperliquid publica un único mercado (USD-margined), así que unificamos USDT/USDC → \"BTC/USD\".\n",
    "    # - Para cualquier exchange: si por error llegaran dos datasets con el mismo nombre canónico,\n",
    "    #   se fusionan por timestamp para evitar duplicados en los pesos.\n",
    "    def canonicalize_pair(exchange: str, symbol: str) -> str:\n",
    "        s = symbol.upper().replace(\"-\", \"/\").strip()\n",
    "        if exchange == \"Hyperliquid\":\n",
    "            return \"BTC/USD\"\n",
    "        return s\n",
    "\n",
    "    named_usd_map: Dict[str, pd.DataFrame] = {}\n",
    "    for key, df in data.items():\n",
    "        if df is None or df.empty:\n",
    "            continue\n",
    "        exch, sym = key.split(\":\", 1)\n",
    "        if (sym.endswith(\"/USD\") or sym.endswith(\"/USDT\") or sym.endswith(\"/USDC\")) and exch != \"Upbit\":\n",
    "            cname = f\"{exch} {canonicalize_pair(exch, sym)}\"  # p.ej. \"MEXC BTC/USDT\"\n",
    "            if cname in named_usd_map:\n",
    "                merged = pd.concat([named_usd_map[cname], df], ignore_index=True)\n",
    "                merged = merged.drop_duplicates(subset=[\"timestamp\"]).sort_values(\"timestamp\").reset_index(drop=True)\n",
    "                named_usd_map[cname] = merged\n",
    "            else:\n",
    "                named_usd_map[cname] = df\n",
    "\n",
    "    named_usd_datasets = list(named_usd_map.items())\n",
    "\n",
    "\n",
    "    print(\"=== Descargando USDKRW ===\")\n",
    "    all_dfs_for_range = [df for df in data.values() if df is not None and not df.empty]\n",
    "    min_date = min(d[\"timestamp\"].min().date() for d in all_dfs_for_range)\n",
    "    max_date = max(d[\"timestamp\"].max().date() for d in all_dfs_for_range)\n",
    "    fx_df = fetch_usdkrw_timeseries(min_date.isoformat(), max_date.isoformat())\n",
    "\n",
    "    print(\"=== Calculando Coinbase Premium ===\")\n",
    "    coin_df = data.get(\"Coinbase:BTC/USD\")\n",
    "    refs_ex_coin = [df for name, df in named_usd_datasets if not name.startswith(\"Coinbase \")]\n",
    "    cb_prem = compute_coinbase_premium(coin_df, refs_ex_coin) if coin_df is not None and not coin_df.empty and refs_ex_coin else pd.DataFrame()\n",
    "    save_csv(cb_prem, \"coinbase_premium_daily.csv\")\n",
    "\n",
    "    print(\"=== Calculando Kimchi Premium ===\")\n",
    "    if refs_ex_coin:\n",
    "        # Construir referencia USD con pesos en USD y penalización a Hyperliquid\n",
    "        parts = []\n",
    "        for df_part in refs_ex_coin:\n",
    "            dd = ensure_daily(df_part)[[\"date\",\"close\",\"volume\",\"exchange\"]].copy()\n",
    "            dd[\"w_usd\"] = dd[\"close\"] * dd[\"volume\"]\n",
    "            dd.loc[dd[\"exchange\"].str.contains(\"Hyperliquid\", case=False, na=False), \"w_usd\"] *= HYPERLIQUID_WEIGHT_PENALTY\n",
    "            parts.append(dd[[\"date\",\"close\",\"w_usd\"]])\n",
    "        ref_concat = pd.concat(parts, ignore_index=True)\n",
    "        ref_usd = ref_concat.groupby(\"date\").apply(lambda g: pd.Series({\n",
    "            \"close\": (g[\"close\"]*g[\"w_usd\"]).sum()/max(g[\"w_usd\"].sum(),1e-12),\n",
    "            \"volume_usd\": g[\"w_usd\"].sum()\n",
    "        })).reset_index()\n",
    "        ref_usd[\"timestamp\"] = (ref_usd[\"date\"].dt.tz_convert(\"UTC\") if pd.api.types.is_datetime64tz_dtype(ref_usd[\"date\"]) else pd.to_datetime(ref_usd[\"date\"], utc=True))\n",
    "        ref_usd_df = pd.DataFrame({\n",
    "            \"timestamp\": ref_usd[\"timestamp\"],\n",
    "            \"open\": ref_usd[\"close\"],\n",
    "            \"high\": ref_usd[\"close\"],\n",
    "            \"low\": ref_usd[\"close\"],\n",
    "            \"close\": ref_usd[\"close\"],\n",
    "            \"volume\": ref_usd[\"volume_usd\"],\n",
    "            \"symbol\": \"REF/USD\", \"exchange\":\"REF\"\n",
    "        })\n",
    "    else:\n",
    "        ref_usd_df = pd.DataFrame()\n",
    "\n",
    "    upbit_df = data.get(\"Upbit:BTC/KRW\", pd.DataFrame())\n",
    "    kimchi = compute_kimchi_premium(upbit_df, ref_usd_df, fx_df) if not upbit_df.empty and not ref_usd_df.empty and not fx_df.empty else pd.DataFrame()\n",
    "    save_csv(kimchi, \"kimchi_premium_daily.csv\")\n",
    "\n",
    "    print(\"=== Construyendo vela agregada (USD) ===\")\n",
    "    usd_dfs = [df for _, df in named_usd_datasets]\n",
    "    agg = aggregate_usd_candles(usd_dfs)\n",
    "    save_csv(agg, \"aggregated_btc_usd_1d.csv\")\n",
    "\n",
    "    print(\"=== Pesos por exchange (diario) ===\")\n",
    "    weights_input = [(name, df) for name, df in named_usd_datasets]\n",
    "    weights = compute_daily_weights(weights_input)\n",
    "    save_csv(weights, \"aggregate_weights_daily.csv\")\n",
    "    if not weights.empty:\n",
    "        last_day = weights[\"date\"].max()\n",
    "        w_last = weights[weights[\"date\"] == last_day][[\"name\",\"weight\"]].sort_values(\"weight\", ascending=False)\n",
    "        print(\"Pesos última fecha:\")\n",
    "        print(w_last.to_string(index=False, float_format=lambda x: f\"{x*100:.2f}%\"))\n",
    "\n",
    "    # === Pesos 7D/30D/90D y análisis de tendencia ===\n",
    "    print(\"=== Pesos 7D/30D/90D y tendencias ===\")\n",
    "    w7  = summarize_period_weights(weights, 7)\n",
    "    w30 = summarize_period_weights(weights, 30)\n",
    "    w90 = summarize_period_weights(weights, 90)\n",
    "    save_csv(w7,  \"aggregate_weights_last7d.csv\")\n",
    "    save_csv(w30, \"aggregate_weights_last30d.csv\")\n",
    "    save_csv(w90, \"aggregate_weights_last90d.csv\")\n",
    "\n",
    "    _print_top(w7,  \"7D\")\n",
    "    _print_top(w30, \"30D\")\n",
    "    _print_top(w90, \"90D\")\n",
    "\n",
    "    # Exportar HTML con estilo de ganadores/perdedores\n",
    "    winners_html = os.path.join(CHARTS_DIR, \"winners_losers.html\")\n",
    "    _export_winners_losers_html(w7, w30, w90, winners_html)\n",
    "    print(\" - Tablas (HTML):\", winners_html)\n",
    "\n",
    "    # === Visualizaciones mejoradas ===\n",
    "    print(\"=== Creando todas las visualizaciones ===\")\n",
    "    \n",
    "    # Usa la nueva función integrada\n",
    "    viz_results = create_all_visualizations(\n",
    "        data=data,\n",
    "        df_agg=agg,\n",
    "        coinbase_prem=cb_prem,\n",
    "        kimchi_prem=kimchi,\n",
    "        charts_dir=CHARTS_DIR\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Resumen de Visualizaciones ===\")\n",
    "    print(f\"✓ Gráfico agregado con volumen: {viz_results['aggregated_chart']}\")\n",
    "    print(f\"✓ Gráficos individuales creados: {len(viz_results['individual_charts'])}\")\n",
    "    print(f\"✓ Grids de comparación: {len(viz_results['comparison_grids'])}\")\n",
    "    print(f\"✓ Ubicación: {os.path.abspath(CHARTS_DIR)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa667a9a-4db8-46d2-87dd-bb8319c1398d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618dbb53-6d4d-4afd-895a-b8aeb054fbfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf1b431-0e34-45ea-8625-6d35229c7f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c45e08-3bb1-4d7a-87c4-98c977419b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b444b-8aa2-4fb7-bef2-ef14f52e44db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bface8b2-6c0d-44eb-adec-c7d0c41392ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68851011-019f-41a0-927f-bd598a9426bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
