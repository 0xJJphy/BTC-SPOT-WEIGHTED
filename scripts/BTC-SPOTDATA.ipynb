{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BTC Spot Data Analysis\n",
    "This notebook fetches and analyzes BTC data from various exchanges."
   ],
   "id": "header"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b9032e8-b01d-4445-aa9c-a1c45e529f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exchange Fetchers\n",
    "Classes to fetch OHLCV data from different exchanges."
   ],
   "id": "md_fetchers"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce7297e-9028-4b94-b193-b4879eb0c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# Fetchers por exchange (1D)\n",
    "# ======================================================================================\n",
    "\n",
    "class HyperliquidFetcher:\n",
    "    \"\"\"Hyperliquid candles via native Info API (candleSnapshot).\"\"\"\n",
    "    ENDPOINTS = [\n",
    "        \"https://api.hyperliquid.xyz/info\",\n",
    "        \"https://api-ui.hyperliquid.xyz/info\",\n",
    "    ]\n",
    "\n",
    "    @staticmethod\n",
    "    def _post(payload: dict) -> Optional[dict]:\n",
    "        last_err = None\n",
    "        for url in HyperliquidFetcher.ENDPOINTS:\n",
    "            try:\n",
    "                r = requests.post(url, json=payload, headers=USER_AGENT, timeout=30)\n",
    "                if r.status_code == 200:\n",
    "                    return r.json()\n",
    "                last_err = f\"HTTP {r.status_code}: {r.text[:120]}\"\n",
    "            except Exception as e:\n",
    "                last_err = str(e)\n",
    "        print(f\"[Hyperliquid] API error: {last_err}\")\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch(symbol: str = \"BTC/USDC\", interval: str = \"1d\", start_ms: Optional[int] = None) -> pd.DataFrame:\n",
    "        coin = symbol.split('/')[0].upper()\n",
    "        since = start_ms or to_ms(datetime(2019, 9, 1, tzinfo=timezone.utc))\n",
    "        end_ms = to_ms(datetime.now(timezone.utc))\n",
    "        print(f\"[Hyperliquid] Fetch {symbol} interval={interval} since={ms_to_utc(since).date()} \u2192 {ms_to_utc(end_ms).date()}\")\n",
    "        payload = {\n",
    "            \"type\": \"candleSnapshot\",\n",
    "            \"req\": {\"coin\": coin, \"interval\": interval, \"startTime\": since, \"endTime\": end_ms},\n",
    "        }\n",
    "        js = HyperliquidFetcher._post(payload)\n",
    "        if not js or not isinstance(js, list) or not js:\n",
    "            print(\"[Hyperliquid] No data returned\")\n",
    "            return pd.DataFrame()\n",
    "        rows = []\n",
    "        for k in js:\n",
    "            t = int(k.get(\"t\", 0))\n",
    "            o = float(k.get(\"o\", 0)); h = float(k.get(\"h\", 0)); l = float(k.get(\"l\", 0)); c = float(k.get(\"c\", 0))\n",
    "            v = float(k.get(\"v\", 0))\n",
    "            rows.append([t, o, h, l, c, v])\n",
    "        df = normalize_ohlcv_rows(rows, symbol, \"Hyperliquid\")\n",
    "        if start_ms is not None and ONLY_LAST_2_DAYS_IF_MISSING:\n",
    "            start_dt = pd.to_datetime(start_ms, unit='ms', utc=True)\n",
    "            df = df[df[\"timestamp\"] >= start_dt]\n",
    "        print(f\"[Hyperliquid] Fetched rows: {len(df)}; range: {df['timestamp'].min().date() if not df.empty else '-'} \u2192 {df['timestamp'].max().date() if not df.empty else '-'}\")\n",
    "        return df\n",
    "\n",
    "\n",
    "class BinanceFetcher:\n",
    "    BASE = \"https://api.binance.com/api/v3/klines\"\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch(symbol: str = \"BTC/USDT\", interval: str = \"1d\", start_ms: Optional[int] = None) -> pd.DataFrame:\n",
    "        sym = symbol.replace(\"/\", \"\")  # BTCUSDT\n",
    "        start = start_ms or to_ms(datetime(2019,9,1,tzinfo=timezone.utc))\n",
    "        all_rows: List[List] = []\n",
    "        while True:\n",
    "            params = {\"symbol\": sym, \"interval\": interval, \"limit\": 1000, \"startTime\": start}\n",
    "            r = requests.get(BinanceFetcher.BASE, params=params, headers=USER_AGENT, timeout=30)\n",
    "            if r.status_code != 200:\n",
    "                print(\"[Binance] HTTP\", r.status_code, r.text[:120]); break\n",
    "            batch = r.json()\n",
    "            if not batch:\n",
    "                break\n",
    "            for k in batch:\n",
    "                all_rows.append([k[0], k[1], k[2], k[3], k[4], k[5]])\n",
    "            if len(batch) < 1000:\n",
    "                break\n",
    "            start = batch[-1][0] + 1\n",
    "            time.sleep(0.12)\n",
    "        return normalize_ohlcv_rows(all_rows, symbol, \"Binance\")\n",
    "\n",
    "\n",
    "class BybitFetcher:\n",
    "    BASE = \"https://api.bybit.com/v5/market/kline\"\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch(symbol: str = \"BTC/USDT\", interval: str = \"D\", start_ms: Optional[int] = None) -> pd.DataFrame:\n",
    "        all_rows: List[List] = []\n",
    "        end = to_ms(datetime.now(timezone.utc))\n",
    "        start = start_ms\n",
    "        while True:\n",
    "            params = {\"category\":\"spot\",\"symbol\":symbol.replace(\"/\",\"\"),\"interval\":interval,\"limit\":1000,\"end\":end}\n",
    "            if start is not None:\n",
    "                params[\"start\"] = start\n",
    "            r = requests.get(BybitFetcher.BASE, params=params, headers=USER_AGENT, timeout=30)\n",
    "            if r.status_code != 200:\n",
    "                print(\"[Bybit] HTTP\", r.status_code, r.text[:120]); break\n",
    "            js = r.json()\n",
    "            if js.get(\"retCode\", -1) != 0:\n",
    "                print(\"[Bybit] API\", js.get(\"retMsg\")); break\n",
    "            lst = js.get(\"result\",{}).get(\"list\",[])\n",
    "            if not lst:\n",
    "                break\n",
    "            for k in lst:\n",
    "                all_rows.append([int(k[0]), k[1], k[2], k[3], k[4], k[5]])\n",
    "            if len(lst) < 1000:\n",
    "                break\n",
    "            end = int(lst[-1][0]) - 1\n",
    "            time.sleep(0.12)\n",
    "        return normalize_ohlcv_rows(all_rows, symbol, \"Bybit\")\n",
    "\n",
    "\n",
    "class OKXFetcher:\n",
    "    BASE = \"https://www.okx.com/api/v5/market/history-candles\"\n",
    "    BASE_REGULAR = \"https://www.okx.com/api/v5/market/candles\"  # Para datos recientes\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch(symbol: str = \"BTC/USDT\", interval: str = \"1D\", start_ms: Optional[int] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fetch OHLCV data from OKX with proper pagination and recent data handling.\n",
    "        Uses both history-candles (for historical) and regular candles (for recent) endpoints.\n",
    "        \"\"\"\n",
    "        okx_symbol = symbol.replace(\"/\", \"-\")\n",
    "        limit = 100\n",
    "        all_rows: List[List] = []\n",
    "        seen_ts = set()\n",
    "        \n",
    "        print(f\"[OKX] Fetching {symbol} interval={interval}\")\n",
    "        \n",
    "        # PASO 1: Obtener datos recientes usando el endpoint regular (\u00faltimos 100 d\u00edas)\n",
    "        print(f\"[OKX] Step 1: Fetching recent data for {symbol}...\")\n",
    "        params_recent = {\"instId\": okx_symbol, \"bar\": interval, \"limit\": limit}\n",
    "        \n",
    "        try:\n",
    "            r = requests.get(OKXFetcher.BASE_REGULAR, params=params_recent, headers=USER_AGENT, timeout=30)\n",
    "            if r.status_code == 200:\n",
    "                js = r.json()\n",
    "                if js.get(\"code\") == \"0\":\n",
    "                    recent_data = js.get(\"data\", [])\n",
    "                    print(f\"[OKX] Got {len(recent_data)} recent candles\")\n",
    "                    for k in recent_data:\n",
    "                        t = int(k[0])\n",
    "                        if t not in seen_ts:\n",
    "                            seen_ts.add(t)\n",
    "                            all_rows.append([t, k[1], k[2], k[3], k[4], k[5]])\n",
    "                    \n",
    "                    if recent_data:\n",
    "                        oldest_recent = min(int(k[0]) for k in recent_data)\n",
    "                        newest_recent = max(int(k[0]) for k in recent_data)\n",
    "                        print(f\"[OKX] Recent data range: {ms_to_utc(oldest_recent).date()} \u2192 {ms_to_utc(newest_recent).date()}\")\n",
    "                else:\n",
    "                    print(f\"[OKX] Recent data error: {js.get('msg')}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[OKX] Exception fetching recent data: {e}\")\n",
    "        \n",
    "        # PASO 2: Obtener datos hist\u00f3ricos usando history-candles\n",
    "        print(f\"[OKX] Step 2: Fetching historical data for {symbol}...\")\n",
    "        \n",
    "        # Verificar primero si hay datos hist\u00f3ricos disponibles\n",
    "        test_params = {\"instId\": okx_symbol, \"bar\": interval, \"limit\": 1}\n",
    "        try:\n",
    "            test_r = requests.get(OKXFetcher.BASE, params=test_params, headers=USER_AGENT, timeout=30)\n",
    "            if test_r.status_code == 200:\n",
    "                test_js = test_r.json()\n",
    "                if test_js.get(\"code\") == \"0\":\n",
    "                    test_data = test_js.get(\"data\", [])\n",
    "                    if not test_data:\n",
    "                        print(f\"[OKX] WARNING: No historical data available for {symbol}\")\n",
    "                        print(f\"[OKX] This might be a new or low-liquidity pair on OKX\")\n",
    "                    else:\n",
    "                        # Si hay datos hist\u00f3ricos, proceder con la paginaci\u00f3n\n",
    "                        pages = 0\n",
    "                        max_pages = 50\n",
    "                        after_ts = None\n",
    "                        \n",
    "                        while pages < max_pages:\n",
    "                            params = {\"instId\": okx_symbol, \"bar\": interval, \"limit\": limit}\n",
    "                            if after_ts:\n",
    "                                params[\"after\"] = after_ts\n",
    "                            \n",
    "                            r = requests.get(OKXFetcher.BASE, params=params, headers=USER_AGENT, timeout=30)\n",
    "                            if r.status_code != 200:\n",
    "                                print(f\"[OKX] HTTP {r.status_code}\")\n",
    "                                break\n",
    "                            \n",
    "                            js = r.json()\n",
    "                            if js.get(\"code\") != \"0\":\n",
    "                                error_msg = js.get(\"msg\", \"Unknown error\")\n",
    "                                if \"Invalid\" in error_msg or \"not exist\" in error_msg:\n",
    "                                    print(f\"[OKX] Pair {okx_symbol} might not be available for historical data\")\n",
    "                                else:\n",
    "                                    print(f\"[OKX] API error: {error_msg}\")\n",
    "                                break\n",
    "                            \n",
    "                            data = js.get(\"data\", [])\n",
    "                            if not data:\n",
    "                                break\n",
    "                            \n",
    "                            added = 0\n",
    "                            oldest_in_batch = None\n",
    "                            \n",
    "                            for k in data:\n",
    "                                t = int(k[0])\n",
    "                                if t not in seen_ts:\n",
    "                                    seen_ts.add(t)\n",
    "                                    all_rows.append([t, k[1], k[2], k[3], k[4], k[5]])\n",
    "                                    added += 1\n",
    "                                    if oldest_in_batch is None or t < oldest_in_batch:\n",
    "                                        oldest_in_batch = t\n",
    "                            \n",
    "                            if added == 0:\n",
    "                                print(f\"[OKX] No new historical data, stopping at page {pages}\")\n",
    "                                break\n",
    "                            \n",
    "                            if oldest_in_batch:\n",
    "                                after_ts = oldest_in_batch\n",
    "                                if pages % 5 == 0 and pages > 0:\n",
    "                                    print(f\"[OKX] Historical: {pages} pages, {len(all_rows)} total candles, \"\n",
    "                                          f\"oldest so far: {ms_to_utc(oldest_in_batch).date()}\")\n",
    "                            \n",
    "                            # Check if we've reached the start threshold\n",
    "                            if start_ms and oldest_in_batch and oldest_in_batch <= start_ms:\n",
    "                                print(f\"[OKX] Reached start threshold\")\n",
    "                                break\n",
    "                            \n",
    "                            pages += 1\n",
    "                            time.sleep(0.15)\n",
    "                        \n",
    "                        if pages > 0:\n",
    "                            print(f\"[OKX] Historical fetch complete: {pages} pages\")\n",
    "                else:\n",
    "                    print(f\"[OKX] Test query failed: {test_js.get('msg')}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[OKX] Exception during historical fetch: {e}\")\n",
    "        \n",
    "        # PASO 3: Filtrar y procesar datos\n",
    "        if start_ms:\n",
    "            all_rows = [r for r in all_rows if r[0] >= start_ms]\n",
    "        \n",
    "        # Ordenar por timestamp\n",
    "        all_rows.sort(key=lambda x: x[0])\n",
    "        \n",
    "        df = normalize_ohlcv_rows(all_rows, symbol, \"OKX\")\n",
    "        \n",
    "        if not df.empty:\n",
    "            print(f\"[OKX] {symbol} final dataset: {len(df)} rows\")\n",
    "            print(f\"[OKX] Date range: {df['timestamp'].min().date()} \u2192 {df['timestamp'].max().date()}\")\n",
    "            \n",
    "            # Verificar gaps significativos\n",
    "            if len(df) > 1:\n",
    "                df_sorted = df.sort_values('timestamp')\n",
    "                date_diffs = df_sorted['timestamp'].diff()\n",
    "                max_gap = date_diffs.max()\n",
    "                if max_gap > pd.Timedelta(days=30):\n",
    "                    gap_idx = date_diffs.idxmax()\n",
    "                    print(f\"[OKX] WARNING: Large gap detected ({max_gap.days} days) \"\n",
    "                          f\"at {df_sorted.loc[gap_idx, 'timestamp'].date()}\")\n",
    "        else:\n",
    "            print(f\"[OKX] WARNING: No data retrieved for {symbol}\")\n",
    "        \n",
    "        # DIAGN\u00d3STICO ESPECIAL PARA PARES CON POCOS DATOS\n",
    "        if not df.empty and len(df) < 100:\n",
    "            print(f\"[OKX] \u26a0\ufe0f LIMITED DATA for {symbol}: Only {len(df)} candles available\")\n",
    "            print(f\"[OKX] This suggests {symbol} is:\")\n",
    "            print(f\"[OKX]   - A newly listed pair on OKX\")\n",
    "            print(f\"[OKX]   - A low liquidity/volume pair\")\n",
    "            print(f\"[OKX]   - Or has limited historical data on this exchange\")\n",
    "            print(f\"[OKX] Consider using a different exchange for this pair if more history is needed\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "class CoinbaseFetcher:\n",
    "    BASE = \"https://api.exchange.coinbase.com/products/{}/candles\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _iso_z(dt: datetime) -> str:\n",
    "        return dt.astimezone(timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "    @staticmethod\n",
    "    def _pick_product(symbol: str) -> str:\n",
    "        if symbol in (\"BTC/USD\", \"BTC-USD\"): return \"BTC-USD\"\n",
    "        if symbol in (\"BTC/USDT\", \"BTC-USDT\", \"BTC/USDC\", \"BTC-USDC\"): return \"BTC-USD\"\n",
    "        return symbol.replace(\"/\",\"-\")\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch(symbol: str = \"BTC/USD\", granularity_sec: int = 86400, start_ms: Optional[int] = None,\n",
    "              include_partial_today: bool = True) -> pd.DataFrame:\n",
    "        product = CoinbaseFetcher._pick_product(symbol)\n",
    "        end_dt = datetime.now(timezone.utc)\n",
    "        since_dt = datetime(2019,9,1,tzinfo=timezone.utc) if not start_ms else datetime.fromtimestamp(start_ms/1000, tz=timezone.utc)\n",
    "        step = granularity_sec * 300\n",
    "        cur_end = end_dt\n",
    "        all_rows: List[List] = []\n",
    "        while cur_end > since_dt:\n",
    "            cur_start = max(since_dt, cur_end - timedelta(seconds=step - granularity_sec))\n",
    "            params = {\"granularity\": granularity_sec, \"start\": CoinbaseFetcher._iso_z(cur_start), \"end\": CoinbaseFetcher._iso_z(cur_end)}\n",
    "            r = requests.get(CoinbaseFetcher.BASE.format(product), params=params, headers=USER_AGENT, timeout=30)\n",
    "            if r.status_code == 404:\n",
    "                break\n",
    "            if r.status_code != 200:\n",
    "                print(\"[Coinbase] HTTP\", r.status_code, r.text[:120]); break\n",
    "            batch = r.json()  # [time, low, high, open, close, volume] DESC\n",
    "            if not batch:\n",
    "                break\n",
    "            for k in batch:\n",
    "                all_rows.append([k[0]*1000, k[3], k[2], k[1], k[4], k[5]])\n",
    "            oldest = min(b[0] for b in batch)\n",
    "            cur_end = datetime.fromtimestamp(oldest - granularity_sec, tz=timezone.utc)\n",
    "            time.sleep(0.12)\n",
    "        all_rows.sort(key=lambda r: r[0])\n",
    "        df = normalize_ohlcv_rows(all_rows, symbol, \"Coinbase\")\n",
    "        if include_partial_today and granularity_sec == 86400:\n",
    "            if not df.empty and df[\"timestamp\"].dt.date.max() < datetime.now(timezone.utc).date():\n",
    "                start_h = datetime.now(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "                params = {\"granularity\": 3600, \"start\": CoinbaseFetcher._iso_z(start_h), \"end\": CoinbaseFetcher._iso_z(end_dt)}\n",
    "                r = requests.get(CoinbaseFetcher.BASE.format(product), params=params, headers=USER_AGENT, timeout=30)\n",
    "                if r.status_code == 200:\n",
    "                    h = r.json()\n",
    "                    if h:\n",
    "                        h.sort(key=lambda x: x[0])\n",
    "                        o = float(h[0][3]); hhi = float(max(x[2] for x in h)); hlo = float(min(x[1] for x in h)); c = float(h[-1][4]); v = float(sum(x[5] for x in h))\n",
    "                        row = [to_ms(start_h), o, hhi, hlo, c, v]\n",
    "                        df = pd.concat([df, normalize_ohlcv_rows([row], symbol, \"Coinbase\")], ignore_index=True)\n",
    "        return df\n",
    "\n",
    "\n",
    "class CryptoComFetcher:\n",
    "    BASE = \"https://api.crypto.com/exchange/v1/public/get-candlestick\"\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch(symbol: str = \"BTC/USDT\", interval: str = \"1D\", start_ms: Optional[int] = None) -> pd.DataFrame:\n",
    "        inst = symbol.replace(\"/\", \"_\")\n",
    "        all_rows: List[List] = []\n",
    "        end_ts = to_ms(datetime.now(timezone.utc))\n",
    "        since = start_ms or to_ms(datetime(2019,9,1,tzinfo=timezone.utc))\n",
    "        while end_ts > since:\n",
    "            params = {\"instrument_name\": inst, \"timeframe\": interval, \"count\": 300, \"end_ts\": end_ts}\n",
    "            r = requests.get(CryptoComFetcher.BASE, params=params, headers=USER_AGENT, timeout=30)\n",
    "            if r.status_code != 200:\n",
    "                print(\"[Crypto.com] HTTP\", r.status_code, r.text[:120]); break\n",
    "            js = r.json()\n",
    "            if js.get(\"code\") != 0:\n",
    "                print(\"[Crypto.com] API\", js.get(\"message\")); break\n",
    "            data = js.get(\"result\",{}).get(\"data\",[])\n",
    "            if not data:\n",
    "                break\n",
    "            for k in data:\n",
    "                all_rows.append([k['t'], k['o'], k['h'], k['l'], k['c'], k.get('v',0)])\n",
    "            oldest = min(k['t'] for k in data)\n",
    "            end_ts = oldest - 1\n",
    "            time.sleep(0.1)\n",
    "        return normalize_ohlcv_rows(all_rows, symbol, \"Crypto.com\")\n",
    "\n",
    "\n",
    "class UpbitFetcher:\n",
    "    BASE = \"https://api.upbit.com/v1\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def fetch_days(symbol: str = \"BTC/KRW\", start_ms: Optional[int] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fetch daily candles from Upbit with robust date handling.\n",
    "        Fixed timezone issues and deprecated datetime methods.\n",
    "        \"\"\"\n",
    "        # Determine market based on symbol\n",
    "        if symbol.upper().endswith(\"KRW\"):\n",
    "            market = \"KRW-BTC\"\n",
    "        elif symbol.upper().endswith(\"USDT\"):\n",
    "            market = \"USDT-BTC\"\n",
    "        else:\n",
    "            market = \"KRW-BTC\"  # Default to KRW\n",
    "        \n",
    "        url = UpbitFetcher.BASE + \"/candles/days\"\n",
    "        count = 200\n",
    "        \n",
    "        # Use timezone-aware datetime (Python 3.12+ compatible)\n",
    "        to_dt = datetime.now(timezone.utc)\n",
    "        threshold_dt = datetime(2017, 1, 1, tzinfo=timezone.utc)\n",
    "        since_dt = datetime.fromtimestamp(start_ms/1000, tz=timezone.utc) if start_ms else None\n",
    "        \n",
    "        all_rows: List[List] = []\n",
    "        pages = 0\n",
    "        max_pages = 30  # Safety limit\n",
    "        \n",
    "        print(f\"[Upbit] Fetching {symbol} (market: {market})...\")\n",
    "        \n",
    "        while pages < max_pages:\n",
    "            try:\n",
    "                # Format datetime for Upbit API (ISO 8601)\n",
    "                to_str = to_dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "                params = {\n",
    "                    \"market\": market,\n",
    "                    \"count\": count,\n",
    "                    \"to\": to_str\n",
    "                }\n",
    "                \n",
    "                # Make request\n",
    "                r = requests.get(url, params=params, headers=USER_AGENT, timeout=30)\n",
    "                \n",
    "                if r.status_code == 429:\n",
    "                    print(\"[Upbit] Rate limit hit, waiting 1 second...\")\n",
    "                    time.sleep(1)\n",
    "                    continue\n",
    "                \n",
    "                if r.status_code != 200:\n",
    "                    print(f\"[Upbit] HTTP {r.status_code}: {r.text[:200]}\")\n",
    "                    break\n",
    "                \n",
    "                batch = r.json()\n",
    "                \n",
    "                # Check if response is an error dict\n",
    "                if isinstance(batch, dict) and 'error' in batch:\n",
    "                    print(f\"[Upbit] API error: {batch.get('error', {}).get('message', 'Unknown error')}\")\n",
    "                    break\n",
    "                \n",
    "                if not batch or not isinstance(batch, list):\n",
    "                    print(f\"[Upbit] No more data or unexpected response type\")\n",
    "                    break\n",
    "                \n",
    "                # Process batch\n",
    "                batch_count = 0\n",
    "                oldest_dt_in_batch = None\n",
    "                \n",
    "                for k in batch:\n",
    "                    try:\n",
    "                        # Get UTC timestamp\n",
    "                        iso = k.get(\"candle_date_time_utc\")\n",
    "                        if not iso:\n",
    "                            continue\n",
    "                        \n",
    "                        # Parse ISO timestamp robustly\n",
    "                        if 'T' in iso:\n",
    "                            if iso.endswith('Z'):\n",
    "                                dt = datetime.fromisoformat(iso[:-1] + '+00:00')\n",
    "                            elif '+' in iso or '-' in iso[-6:]:\n",
    "                                dt = datetime.fromisoformat(iso)\n",
    "                            else:\n",
    "                                dt = datetime.fromisoformat(iso).replace(tzinfo=timezone.utc)\n",
    "                        else:\n",
    "                            dt = pd.to_datetime(iso, utc=True).to_pydatetime()\n",
    "                        \n",
    "                        if dt.tzinfo is None:\n",
    "                            dt = dt.replace(tzinfo=timezone.utc)\n",
    "                        \n",
    "                        if oldest_dt_in_batch is None or dt < oldest_dt_in_batch:\n",
    "                            oldest_dt_in_batch = dt\n",
    "                        \n",
    "                        ts_ms = int(dt.timestamp() * 1000)\n",
    "                        \n",
    "                        o = float(k.get(\"opening_price\", 0))\n",
    "                        h = float(k.get(\"high_price\", 0))\n",
    "                        l = float(k.get(\"low_price\", 0))\n",
    "                        c = float(k.get(\"trade_price\", 0))\n",
    "                        v = float(k.get(\"candle_acc_trade_volume\", 0))  # Volume in base currency (BTC)\n",
    "                        \n",
    "                        all_rows.append([ts_ms, o, h, l, c, v])\n",
    "                        batch_count += 1\n",
    "                        \n",
    "                    except (ValueError, KeyError, TypeError) as e:\n",
    "                        print(f\"[Upbit] Error parsing candle: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                if batch_count == 0:\n",
    "                    print(f\"[Upbit] No valid candles in batch, stopping\")\n",
    "                    break\n",
    "                \n",
    "                print(f\"[Upbit] Page {pages+1}: {batch_count} candles, oldest: {oldest_dt_in_batch.date() if oldest_dt_in_batch else 'N/A'}\")\n",
    "                \n",
    "                if oldest_dt_in_batch:\n",
    "                    if since_dt and oldest_dt_in_batch <= since_dt:\n",
    "                        print(f\"[Upbit] Reached start date threshold\")\n",
    "                        break\n",
    "                    \n",
    "                    if oldest_dt_in_batch <= threshold_dt:\n",
    "                        print(f\"[Upbit] Reached historical threshold (2017)\")\n",
    "                        break\n",
    "                    \n",
    "                    to_dt = oldest_dt_in_batch - timedelta(seconds=1)\n",
    "                else:\n",
    "                    print(f\"[Upbit] Could not determine oldest date in batch, stopping\")\n",
    "                    break\n",
    "                \n",
    "                if len(batch) < count:\n",
    "                    print(f\"[Upbit] Got {len(batch)} < {count} candles, reached end of data\")\n",
    "                    break\n",
    "                \n",
    "                pages += 1\n",
    "                time.sleep(0.15)  # Rate limiting\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"[Upbit] Request error: {e}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"[Upbit] Unexpected error: {e}\")\n",
    "                break\n",
    "        \n",
    "        df = normalize_ohlcv_rows(all_rows, symbol, \"Upbit\")\n",
    "        \n",
    "        if start_ms is not None and ONLY_LAST_2_DAYS_IF_MISSING:\n",
    "            start_dt = pd.to_datetime(start_ms, unit='ms', utc=True)\n",
    "            df = df[df[\"timestamp\"] >= start_dt]\n",
    "            print(f\"[Upbit] Filtered to dates >= {start_dt.date()}\")\n",
    "        \n",
    "        if not df.empty:\n",
    "            print(f\"[Upbit] Final dataset: {len(df)} rows, range: {df['timestamp'].min().date()} \u2192 {df['timestamp'].max().date()}\")\n",
    "        else:\n",
    "            print(f\"[Upbit] No data retrieved\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "def test_upbit_api():\n",
    "    \"\"\"\n",
    "    Test Upbit API connectivity and response format\n",
    "    \"\"\"\n",
    "    print(\"\\n[Upbit Test] Testing API connectivity...\")\n",
    "    \n",
    "    # Test markets endpoint first\n",
    "    try:\n",
    "        markets_url = \"https://api.upbit.com/v1/market/all\"\n",
    "        r = requests.get(markets_url, timeout=10)\n",
    "        if r.status_code == 200:\n",
    "            markets = r.json()\n",
    "            btc_markets = [m for m in markets if 'BTC' in m.get('market', '')]\n",
    "            print(f\"[Upbit Test] \u2713 Markets endpoint OK, found {len(btc_markets)} BTC markets\")\n",
    "            \n",
    "            for m in btc_markets[:5]:\n",
    "                print(f\"  - {m['market']}: {m.get('korean_name', m.get('english_name', 'N/A'))}\")\n",
    "        else:\n",
    "            print(f\"[Upbit Test] \u2717 Markets endpoint failed: {r.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Upbit Test] \u2717 Markets test failed: {e}\")\n",
    "    \n",
    "    # Test candles endpoint\n",
    "    try:\n",
    "        candles_url = \"https://api.upbit.com/v1/candles/days\"\n",
    "        params = {\"market\": \"KRW-BTC\", \"count\": 1}\n",
    "        r = requests.get(candles_url, params=params, timeout=10)\n",
    "        \n",
    "        if r.status_code == 200:\n",
    "            data = r.json()\n",
    "            if data and isinstance(data, list) and len(data) > 0:\n",
    "                candle = data[0]\n",
    "                print(f\"[Upbit Test] \u2713 Candles endpoint OK\")\n",
    "                print(f\"  Latest candle date: {candle.get('candle_date_time_utc')}\")\n",
    "                print(f\"  Price: {candle.get('trade_price'):,.0f} KRW\")\n",
    "                print(f\"  Volume: {candle.get('candle_acc_trade_volume'):.4f} BTC\")\n",
    "            else:\n",
    "                print(f\"[Upbit Test] \u2717 Unexpected response format: {data}\")\n",
    "        else:\n",
    "            print(f\"[Upbit Test] \u2717 Candles endpoint failed: {r.status_code}\")\n",
    "            print(f\"  Response: {r.text[:200]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Upbit Test] \u2717 Candles test failed: {e}\")\n",
    "    \n",
    "    print(\"[Upbit Test] Complete\\n\")\n",
    "\n",
    "\n",
    "class BitgetFetcher:\n",
    "    \"\"\"Bitget spot history candles (v2). Paginaci\u00f3n por endTime (limit<=200).\"\"\"\n",
    "    BASE = \"https://api.bitget.com/api/v2/spot/market/history-candles\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _map_granularity(interval: str) -> str:\n",
    "        s = (interval or \"1D\").strip().lower()\n",
    "        if s in (\"1d\", \"1day\", \"1dutc\"): return \"1Dutc\"\n",
    "        if s in (\"3d\", \"3day\", \"3dutc\"): return \"3Dutc\"\n",
    "        if s in (\"1w\", \"1week\", \"1wutc\"): return \"1Wutc\"\n",
    "        if s == \"1m\": return \"1min\"\n",
    "        if s == \"3m\": return \"3min\"\n",
    "        if s == \"5m\": return \"5min\"\n",
    "        if s == \"15m\": return \"15min\"\n",
    "        if s == \"30m\": return \"30min\"\n",
    "        if s == \"1h\": return \"1h\"\n",
    "        if s == \"4h\": return \"4h\"\n",
    "        if s == \"6h\": return \"6h\"\n",
    "        if s == \"12h\": return \"12h\"\n",
    "        if s in (\"1mth\", \"1month\"): return \"1M\"\n",
    "        return interval\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch(symbol: str = \"BTC/USDT\", interval: str = \"1D\", start_ms: Optional[int] = None) -> pd.DataFrame:\n",
    "        sym = symbol.replace(\"/\", \"\")\n",
    "        end = to_ms(datetime.now(timezone.utc))\n",
    "        all_rows: List[List] = []\n",
    "        limit = 200\n",
    "        gran = BitgetFetcher._map_granularity(interval)\n",
    "        prev_oldest: Optional[int] = None\n",
    "\n",
    "        while True:\n",
    "            params = {\"symbol\": sym, \"granularity\": gran, \"endTime\": end, \"limit\": limit}\n",
    "            r = requests.get(BitgetFetcher.BASE, params=params, headers=USER_AGENT, timeout=30)\n",
    "            if r.status_code != 200:\n",
    "                print(\"[Bitget] HTTP\", r.status_code, r.text[:160])\n",
    "                break\n",
    "            js = r.json()\n",
    "            if js.get(\"code\") != \"00000\":\n",
    "                print(\"[Bitget] API\", js.get(\"msg\"))\n",
    "                break\n",
    "            data = js.get(\"data\", [])\n",
    "            if not data:\n",
    "                break\n",
    "\n",
    "            for k in data:\n",
    "                try:\n",
    "                    t = int(k[0]); o = float(k[1]); h = float(k[2]); l = float(k[3]); c = float(k[4]); v = float(k[5])\n",
    "                    all_rows.append([t, o, h, l, c, v])\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "            oldest = min(int(k[0]) for k in data)\n",
    "            if start_ms and oldest <= start_ms:\n",
    "                break\n",
    "            if prev_oldest is not None and oldest >= prev_oldest:\n",
    "                break  # sin progreso\n",
    "            prev_oldest = oldest\n",
    "            end = oldest - 1\n",
    "            time.sleep(0.12)\n",
    "\n",
    "        df = normalize_ohlcv_rows(all_rows, symbol, \"Bitget\")\n",
    "        if start_ms is not None:\n",
    "            df = df[df[\"timestamp\"] >= pd.to_datetime(start_ms, unit=\"ms\", utc=True)]\n",
    "        print(f\"[Bitget] Fetched rows: {len(df)}; range: \"\n",
    "              f\"{df['timestamp'].min().date() if not df.empty else '-'} \u2192 \"\n",
    "              f\"{df['timestamp'].max().date() if not df.empty else '-'}\")\n",
    "        return df\n",
    "\n",
    "\n",
    "class MEXCFetcher:\n",
    "    BASE = \"https://api.mexc.com/api/v3/klines\"\n",
    "    @staticmethod\n",
    "    def fetch(symbol=\"BTC/USDT\", interval=\"1d\", start_ms=None) -> pd.DataFrame:\n",
    "        sym = symbol.replace(\"/\", \"\")\n",
    "        end = to_ms(datetime.now(timezone.utc))\n",
    "        all_rows = []\n",
    "        limit = 1000\n",
    "        prev_oldest = None\n",
    "        while True:\n",
    "            params = {\"symbol\": sym, \"interval\": interval, \"limit\": limit, \"endTime\": end}\n",
    "            r = requests.get(MEXCFetcher.BASE, params=params, headers=USER_AGENT, timeout=30)\n",
    "            if r.status_code != 200:\n",
    "                print(\"[MEXC] HTTP\", r.status_code, r.text[:120]); break\n",
    "            batch = r.json()\n",
    "            if not batch:\n",
    "                break\n",
    "            for k in batch:\n",
    "                all_rows.append([int(k[0]), float(k[1]), float(k[2]), float(k[3]), float(k[4]), float(k[5])])\n",
    "            oldest = min(int(k[0]) for k in batch)\n",
    "            if start_ms and oldest <= start_ms:\n",
    "                break\n",
    "            if prev_oldest is not None and oldest >= prev_oldest:\n",
    "                break\n",
    "            prev_oldest = oldest\n",
    "            end = oldest - 1\n",
    "            time.sleep(0.12)\n",
    "        df = normalize_ohlcv_rows(all_rows, symbol, \"MEXC\")\n",
    "        if start_ms is not None and ONLY_LAST_2_DAYS_IF_MISSING:\n",
    "            df = df[df[\"timestamp\"] >= pd.to_datetime(start_ms, unit=\"ms\", utc=True)]\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FX and Premium Metrics\n",
    "Functions to handle currency conversion and premium calculations (Coinbase, Kimchi)."
   ],
   "id": "md_fx"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91474351-2d6d-45dc-a2c9-e0d46b006fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# FX USDKRW (fallbacks sin API key) y m\u00e9tricas\n",
    "# ======================================================================================\n",
    "\n",
    "def fetch_usdkrw_timeseries(start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    \"\"\"Obtains USDKRW (KRW por USD) para [start_date, end_date].\n",
    "    Fallbacks: Frankfurter (app/dev) \u2192 FRED CSV (EXKOUS). Normalizes orientation (<1 \u2192 invertido).\n",
    "    \"\"\"\n",
    "    used = None\n",
    "    out: Optional[pd.DataFrame] = None\n",
    "    for base_url in (\"https://api.frankfurter.app\", \"https://api.frankfurter.dev\"):\n",
    "        try:\n",
    "            url = f\"{base_url}/{start_date}..{end_date}\"\n",
    "            params = {\"from\": \"USD\", \"to\": \"KRW\"}\n",
    "            r = requests.get(url, params=params, headers=USER_AGENT, timeout=30)\n",
    "            if r.status_code == 200:\n",
    "                js = r.json(); rates = js.get(\"rates\", {})\n",
    "                if rates:\n",
    "                    rows = [{\"date\": pd.to_datetime(d, utc=True), \"USDKRW\": float(rec[\"KRW\"])} for d, rec in sorted(rates.items())]\n",
    "                    out = pd.DataFrame(rows).set_index(\"date\").sort_index()\n",
    "                    used = f\"Frankfurter {base_url}\"\n",
    "                    break\n",
    "        except Exception:\n",
    "            pass\n",
    "    if out is None:\n",
    "        fred_url = \"https://fred.stlouisfed.org/graph/fredgraph.csv\"\n",
    "        try:\n",
    "            r = requests.get(fred_url, params={\"id\": \"EXKOUS\"}, headers=USER_AGENT, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            csv_df = pd.read_csv(io.StringIO(r.text))\n",
    "            csv_df.rename(columns={\"DATE\": \"date\", \"EXKOUS\": \"USDKRW\"}, inplace=True)\n",
    "            csv_df[\"date\"] = pd.to_datetime(csv_df[\"date\"], utc=True)\n",
    "            csv_df[\"USDKRW\"] = pd.to_numeric(csv_df[\"USDKRW\"], errors=\"coerce\")\n",
    "            csv_df = csv_df.dropna().set_index(\"date\").sort_index()\n",
    "            mask = (csv_df.index.date >= pd.to_datetime(start_date).date()) & (csv_df.index.date <= pd.to_datetime(end_date).date())\n",
    "            out = csv_df.loc[mask]\n",
    "            used = \"FRED EXKOUS\"\n",
    "        except Exception:\n",
    "            pass\n",
    "    if out is None or out.empty:\n",
    "        raise RuntimeError(\"Could not obtain USDKRW desde las fuentes p\u00fablicas (Frankfurter/FRED)\")\n",
    "    med = out[\"USDKRW\"].median()\n",
    "    if pd.notna(med) and med < 1:\n",
    "        out[\"USDKRW\"] = 1.0 / out[\"USDKRW\"]\n",
    "        used += \" (invertido)\"\n",
    "    print(f\"[FX] USDKRW fuente: {used}; rango: {out.index.min().date()} \u2192 {out.index.max().date()} (mediana={out['USDKRW'].median():.2f})\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_coinbase_premium(cb_df: pd.DataFrame, usd_dfs: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"Coinbase premium using weights en USD (close*volume). Aplica penalizaci\u00f3n a Hyperliquid.\"\"\"\n",
    "    cb_dates = ensure_daily(cb_df)[[\"date\",\"close\",\"volume\"]].rename(columns={\"close\":\"cb_close\",\"volume\":\"cb_vol\"})\n",
    "\n",
    "    ref_parts = []\n",
    "    for d in usd_dfs:\n",
    "        if d is None or d.empty:\n",
    "            continue\n",
    "        dd = ensure_daily(d)[[\"date\",\"close\",\"volume\",\"exchange\"]].copy()\n",
    "        dd[\"w_usd\"] = dd[\"close\"] * dd[\"volume\"]\n",
    "        dd.loc[dd[\"exchange\"].str.contains(\"Hyperliquid\", case=False, na=False), \"w_usd\"] *= HYPERLIQUID_WEIGHT_PENALTY\n",
    "        ref_parts.append(dd[[\"date\",\"close\",\"w_usd\"]])\n",
    "    if not ref_parts:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    ref = pd.concat(ref_parts, ignore_index=True)\n",
    "    ref = ref.groupby(\"date\").apply(lambda g: pd.Series({\n",
    "        \"ref_close\": (g[\"close\"] * g[\"w_usd\"]).sum() / max(g[\"w_usd\"].sum(), 1e-12)\n",
    "    })).reset_index()\n",
    "\n",
    "    m = cb_dates.merge(ref, on=\"date\", how=\"inner\")\n",
    "    m[\"coinbase_premium_pct\"] = (m[\"cb_close\"] - m[\"ref_close\"]) / m[\"ref_close\"] * 100\n",
    "    return m[[\"date\",\"cb_close\",\"ref_close\",\"coinbase_premium_pct\"]]\n",
    "\n",
    "\n",
    "def compute_kimchi_premium(upbit_krw_df: pd.DataFrame, ref_usd_df: pd.DataFrame, fx_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    U = ensure_daily(upbit_krw_df)[[\"date\",\"close\"]].rename(columns={\"close\":\"upbit_close_krw\"})\n",
    "    R = ensure_daily(ref_usd_df)[[\"date\",\"close\"]].rename(columns={\"close\":\"ref_close_usd\"})\n",
    "    F = fx_df.reset_index().rename(columns={\"date\":\"date\"})\n",
    "    m = U.merge(R, on=\"date\", how=\"inner\").merge(F, on=\"date\", how=\"left\").sort_values(\"date\")\n",
    "    m[\"USDKRW\"] = m[\"USDKRW\"].ffill().bfill()\n",
    "    m[\"upbit_close_usd\"] = m[\"upbit_close_krw\"] / m[\"USDKRW\"]\n",
    "    m[\"kimchi_pct\"] = (m[\"upbit_close_usd\"] - m[\"ref_close_usd\"]) / m[\"ref_close_usd\"] * 100\n",
    "    return m[[\"date\",\"upbit_close_krw\",\"USDKRW\",\"upbit_close_usd\",\"ref_close_usd\",\"kimchi_pct\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation and Visualization\n",
    "Aggregating data across exchanges and generating charts."
   ],
   "id": "md_agg"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0671519c-2b8e-4907-a47c-8850b9ffe644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# Agregado y gr\u00e1ficos\n",
    "# ======================================================================================\n",
    "\n",
    "def aggregate_usd_candles(dfs: List[pd.DataFrame], outlier_factor: float = 0.35) -> pd.DataFrame:\n",
    "    \"\"\"Robust daily aggregation (open/close VWAP; high=max, low=min) con filtro de outliers.\n",
    "    Pesos en USD = close*volume; aplica penalizaci\u00f3n a Hyperliquid.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for d in dfs:\n",
    "        if d is None or d.empty:\n",
    "            continue\n",
    "        dd = ensure_daily(d)\n",
    "        if \"exchange\" not in dd.columns:\n",
    "            dd[\"exchange\"] = d.get(\"exchange\", \"?\")\n",
    "        parts.append(dd[[\"date\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"exchange\"]])\n",
    "    if not parts:\n",
    "        return pd.DataFrame()\n",
    "    big = pd.concat(parts, ignore_index=True)\n",
    "    big = big.dropna(subset=[\"open\",\"high\",\"low\",\"close\",\"volume\"]).copy()\n",
    "    big = big[(big[\"close\"] > 0) & (big[\"open\"] > 0) & (big[\"high\"] > 0) & (big[\"low\"] > 0) & (big[\"volume\"] > 0)]\n",
    "\n",
    "    med = big.groupby(\"date\")[\"close\"].median().rename(\"med_close\")\n",
    "    big = big.merge(med, on=\"date\", how=\"left\")\n",
    "    lo, hi = (1.0 - outlier_factor), (1.0 + outlier_factor)\n",
    "    mask = (big[\"open\"]>=lo*big[\"med_close\"])&(big[\"open\"]<=hi*big[\"med_close\"])&\\\n",
    "           (big[\"high\"]<=hi*big[\"med_close\"])&(big[\"low\"]>=lo*big[\"med_close\"])&\\\n",
    "           (big[\"close\"]>=lo*big[\"med_close\"])&(big[\"close\"]<=hi*big[\"med_close\"]) \n",
    "    dropped = int((~mask).sum())\n",
    "    if dropped > 0:\n",
    "        per_ex = big.loc[~mask].groupby(\"exchange\").size().sort_values(ascending=False)\n",
    "        print(f\"[AGG] filtered outliers: {dropped} filas\")\n",
    "        for ex, n in per_ex.items():\n",
    "            print(f\"[AGG]   - {ex}: {n}\")\n",
    "    big = big[mask].drop(columns=[\"med_close\"]) if dropped>0 else big.drop(columns=[\"med_close\"]) \n",
    "\n",
    "    big[\"high\"] = big[[\"high\",\"open\",\"close\"]].max(axis=1)\n",
    "    big[\"low\"]  = big[[\"low\",\"open\",\"close\"]].min(axis=1)\n",
    "\n",
    "    big[\"w_usd\"] = big[\"close\"] * big[\"volume\"]\n",
    "    big.loc[big[\"exchange\"].str.contains(\"Hyperliquid\", case=False, na=False), \"w_usd\"] *= HYPERLIQUID_WEIGHT_PENALTY\n",
    "\n",
    "    grp = big.groupby(\"date\")\n",
    "    open_vw  = grp.apply(lambda g: np.average(g[\"open\"],  weights=g[\"w_usd\"]))\n",
    "    close_vw = grp.apply(lambda g: np.average(g[\"close\"], weights=g[\"w_usd\"]))\n",
    "    high_max = grp[\"high\"].max()\n",
    "    low_min  = grp[\"low\"].min()\n",
    "    vol_usd_sum = grp[\"w_usd\"].sum()\n",
    "\n",
    "    out = pd.DataFrame({\"open\": open_vw, \"close\": close_vw, \"high\": high_max, \"low\": low_min, \"volume_usd\": vol_usd_sum})\n",
    "    out.reset_index(inplace=True)\n",
    "    out[\"timestamp\"] = (out[\"date\"].dt.tz_convert(\"UTC\") if pd.api.types.is_datetime64tz_dtype(out[\"date\"]) else pd.to_datetime(out[\"date\"], utc=True))\n",
    "    out[\"symbol\"] = \"BTC/USD-AGG\"\n",
    "    out[\"exchange\"] = \"AGGREGATED\"\n",
    "    print(f\"[AGG] aggregated days: {len(out)}; rango: {out['timestamp'].min().date()} \u2192 {out['timestamp'].max().date()} (vol_usd sum ok)\")\n",
    "    return out[[\"timestamp\",\"open\",\"high\",\"low\",\"close\",\"volume_usd\",\"symbol\",\"exchange\"]]\n",
    "\n",
    "\n",
    "def plot_aggregated_candles(df_agg: pd.DataFrame, title: str = \"BTC Aggregated (Vol-Weighted)\") -> go.Figure:\n",
    "    fig = go.Figure(data=[go.Candlestick(x=df_agg[\"timestamp\"], open=df_agg[\"open\"], high=df_agg[\"high\"], low=df_agg[\"low\"], close=df_agg[\"close\"], name=\"AGG\")])\n",
    "    fig.update_layout(title=title, xaxis_rangeslider_visible=False, height=600, hovermode='x unified')\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_premiums(coinbase_prem: pd.DataFrame, kimchi_prem: pd.DataFrame) -> go.Figure:\n",
    "    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.07, subplot_titles=(\"Coinbase Premium (%)\", \"Kimchi Premium (%)\"))\n",
    "    if coinbase_prem is not None and not coinbase_prem.empty:\n",
    "        fig.add_trace(go.Scatter(x=coinbase_prem[\"date\"], y=coinbase_prem[\"coinbase_premium_pct\"], name=\"CB Prem %\"), row=1, col=1)\n",
    "    if kimchi_prem is not None and not kimchi_prem.empty:\n",
    "        fig.add_trace(go.Scatter(x=kimchi_prem[\"date\"], y=kimchi_prem[\"kimchi_pct\"], name=\"Kimchi %\"), row=2, col=1)\n",
    "    fig.update_layout(height=600, hovermode='x unified')\n",
    "    return fig\n",
    "\n",
    "\n",
    "# ============================= Visualizaciones mejoradas ==============================\n",
    "\n",
    "def plot_aggregated_candles_with_volume(df_agg: pd.DataFrame, title: str = \"BTC Aggregated (Vol-Weighted)\") -> go.Figure:\n",
    "    if df_agg.empty:\n",
    "        print(\"[PLOT] No aggregated data to plot\")\n",
    "        return go.Figure()\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.03,\n",
    "        row_heights=[0.7, 0.3],\n",
    "        subplot_titles=('BTC Aggregated Price (Volume-Weighted)', 'Aggregated Volume (USD)')\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Candlestick(\n",
    "            x=df_agg[\"timestamp\"],\n",
    "            open=df_agg[\"open\"],\n",
    "            high=df_agg[\"high\"],\n",
    "            low=df_agg[\"low\"],\n",
    "            close=df_agg[\"close\"],\n",
    "            name=\"Price\",\n",
    "            increasing_line_color='#26a69a',\n",
    "            decreasing_line_color='#ef5350',\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    colors = ['#ef5350' if close < open else '#26a69a' for close, open in zip(df_agg['close'], df_agg['open'])]\n",
    "    volume_col = 'volume_usd' if 'volume_usd' in df_agg.columns else 'volume'\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_agg[\"timestamp\"],\n",
    "            y=df_agg[volume_col],\n",
    "            name='Volume',\n",
    "            marker_color=colors,\n",
    "            opacity=0.5,\n",
    "            showlegend=False,\n",
    "            hovertemplate='Volume: $%{y:,.0f}<extra></extra>'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title={'text': title,'x': 0.5,'xanchor': 'center','font': {'size': 20}},\n",
    "        xaxis_rangeslider_visible=False,\n",
    "        height=700,\n",
    "        hovermode='x unified',\n",
    "        plot_bgcolor='#ffffff',\n",
    "        paper_bgcolor='#f8f9fa'\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"Date\", gridcolor='#e5e7eb', row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Price (USD)\", tickformat='$,.0f', gridcolor='#e5e7eb', row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Volume (USD)\", tickformat='$,.0f', gridcolor='#e5e7eb', row=2, col=1)\n",
    "    if not df_agg.empty:\n",
    "        latest_price = df_agg['close'].iloc[-1]\n",
    "        total_volume = df_agg[volume_col].sum()\n",
    "        avg_volume = df_agg[volume_col].mean()\n",
    "        annotation_text = (f\"Latest: ${latest_price:,.2f} | Avg Daily Vol: ${avg_volume:,.0f} | Total Vol: ${total_volume/1e9:.2f}B\")\n",
    "        fig.add_annotation(text=annotation_text, xref=\"paper\", yref=\"paper\", x=0.5, y=1.05, showarrow=False, font=dict(size=12, color='#666'), xanchor='center')\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_individual_pair_charts(data: Dict[str, pd.DataFrame], charts_dir: str = \"exchange_charts\") -> Dict[str, go.Figure]:\n",
    "    os.makedirs(charts_dir, exist_ok=True)\n",
    "    charts = {}\n",
    "    print(\"\\n=== Creating Individual Pair Charts ===\")\n",
    "    for key, df in data.items():\n",
    "        if df is None or df.empty:\n",
    "            continue\n",
    "        exchange, symbol = key.split(\":\", 1)\n",
    "        fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.03, row_heights=[0.7, 0.3], subplot_titles=(f'{exchange} {symbol} Price', 'Volume'))\n",
    "        fig.add_trace(\n",
    "            go.Candlestick(\n",
    "                x=df['timestamp'], open=df['open'], high=df['high'], low=df['low'], close=df['close'],\n",
    "                name='Price', increasing_line_color='#26a69a', decreasing_line_color='#ef5350', showlegend=False\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        colors = ['#ef5350' if close < open else '#26a69a' for close, open in zip(df['close'], df['open'])]\n",
    "        fig.add_trace(go.Bar(x=df['timestamp'], y=df['volume'], name='Volume', marker_color=colors, opacity=0.5, showlegend=False), row=2, col=1)\n",
    "        latest_price = df['close'].iloc[-1]\n",
    "        price_change = ((df['close'].iloc[-1] / df['close'].iloc[0]) - 1) * 100 if len(df) > 1 else 0\n",
    "        avg_volume = df['volume'].mean()\n",
    "        max_price = df['high'].max()\n",
    "        min_price = df['low'].min()\n",
    "        title_text = f'{exchange} - {symbol} (1D)'\n",
    "        fig.update_layout(title={'text': title_text,'x': 0.5,'xanchor': 'center','font': {'size': 18}},\n",
    "                          xaxis_rangeslider_visible=False, height=600, hovermode='x unified', plot_bgcolor='#ffffff', paper_bgcolor='#f8f9fa')\n",
    "        fig.update_xaxes(title_text=\"Date\", gridcolor='#e5e7eb', row=2, col=1)\n",
    "        currency = \"KRW\" if \"KRW\" in symbol else \"USD\"\n",
    "        currency_symbol = \"\u20a9\" if currency == \"KRW\" else \"$\"\n",
    "        fig.update_yaxes(title_text=f\"Price ({currency})\", tickformat=f'{currency_symbol},.0f', gridcolor='#e5e7eb', row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Volume (BTC)\" if currency != \"KRW\" else \"Volume\", tickformat=',.2f', gridcolor='#e5e7eb', row=2, col=1)\n",
    "        stats_text = (f\"Latest: {currency_symbol}{latest_price:,.2f} | Change: {price_change:+.1f}% | \"\n",
    "                      f\"Range: {currency_symbol}{min_price:,.0f}-{currency_symbol}{max_price:,.0f} | Avg Vol: {avg_volume:.2f}\")\n",
    "        fig.add_annotation(text=stats_text, xref=\"paper\", yref=\"paper\", x=0.5, y=1.08, showarrow=False, font=dict(size=11, color='#666'), xanchor='center')\n",
    "        data_info = f\"Data points: {len(df)} | Period: {df['timestamp'].min().date()} to {df['timestamp'].max().date()}\"\n",
    "        fig.add_annotation(text=data_info, xref=\"paper\", yref=\"paper\", x=0.5, y=-0.15, showarrow=False, font=dict(size=10, color='#999'), xanchor='center')\n",
    "        charts[key] = fig\n",
    "        filename = f\"{exchange.lower()}_{symbol.replace('/', '_').lower()}_chart.html\"\n",
    "        filepath = os.path.join(charts_dir, filename)\n",
    "        fig.write_html(filepath)\n",
    "        print(f\"  \u2713 Created chart for {exchange} {symbol} \u2192 {filename}\")\n",
    "    print(f\"Total individual charts created: {len(charts)}\")\n",
    "    return charts\n",
    "\n",
    "\n",
    "def create_comparison_grid(data: Dict[str, pd.DataFrame], pairs_to_compare: list = None) -> go.Figure:\n",
    "    if pairs_to_compare is None:\n",
    "        pairs_to_compare = [\"BTC/USDT\", \"BTC/USDC\", \"BTC/USD\", \"BTC/KRW\"]\n",
    "    fig_dict = {}\n",
    "    for target_symbol in pairs_to_compare:\n",
    "        exchanges_with_pair = []\n",
    "        for key, df in data.items():\n",
    "            if df is None or df.empty:\n",
    "                continue\n",
    "            exchange, symbol = key.split(\":\", 1)\n",
    "            if symbol.replace(\"-\", \"/\").upper() == target_symbol.upper():\n",
    "                exchanges_with_pair.append((exchange, df))\n",
    "        if len(exchanges_with_pair) > 1:\n",
    "            n_ex = len(exchanges_with_pair)\n",
    "            fig = make_subplots(rows=n_ex, cols=1, shared_xaxes=True, vertical_spacing=0.02,\n",
    "                                subplot_titles=[f\"{ex} - {target_symbol}\" for ex, _ in exchanges_with_pair],\n",
    "                                row_heights=[1/n_ex] * n_ex)\n",
    "            for idx, (exchange, df) in enumerate(exchanges_with_pair, 1):\n",
    "                fig.add_trace(\n",
    "                    go.Candlestick(x=df['timestamp'], open=df['open'], high=df['high'], low=df['low'], close=df['close'],\n",
    "                                   name=exchange, increasing_line_color='#26a69a', decreasing_line_color='#ef5350', showlegend=False),\n",
    "                    row=idx, col=1\n",
    "                )\n",
    "            fig.update_layout(title=f'Exchange Comparison: {target_symbol}', xaxis_rangeslider_visible=False,\n",
    "                              height=200 * n_ex, hovermode='x unified')\n",
    "            fig.update_xaxes(title_text=\"Date\", row=n_ex, col=1)\n",
    "            for idx in range(1, n_ex + 1):\n",
    "                currency = \"KRW\" if \"KRW\" in target_symbol else \"USD\"\n",
    "                fig.update_yaxes(title_text=f\"Price ({currency})\", row=idx, col=1)\n",
    "            fig_dict[target_symbol] = fig\n",
    "    return fig_dict\n",
    "\n",
    "\n",
    "def create_all_visualizations(data: Dict[str, pd.DataFrame], df_agg: pd.DataFrame, \n",
    "                              coinbase_prem: pd.DataFrame, kimchi_prem: pd.DataFrame,\n",
    "                              charts_dir: str = \"exchange_charts\") -> Dict:\n",
    "    os.makedirs(charts_dir, exist_ok=True)\n",
    "    results = {'individual_charts': {}, 'aggregated_chart': None, 'premium_charts': None, 'comparison_grids': {}}\n",
    "    print(\"\\n=== Creating Aggregated Chart with Volume ===\")\n",
    "    fig_agg = plot_aggregated_candles_with_volume(df_agg)\n",
    "    agg_path = os.path.join(charts_dir, \"btc_aggregated_with_volume.html\")\n",
    "    fig_agg.write_html(agg_path)\n",
    "    results['aggregated_chart'] = agg_path\n",
    "    print(f\"  \u2713 Saved aggregated chart \u2192 {agg_path}\")\n",
    "    individual_charts = create_individual_pair_charts(data, charts_dir)\n",
    "    results['individual_charts'] = individual_charts\n",
    "    print(\"\\n=== Creating Comparison Grids ===\")\n",
    "    comparison_grids = create_comparison_grid(data)\n",
    "    for symbol, fig in comparison_grids.items():\n",
    "        filename = f\"comparison_{symbol.replace('/', '_').lower()}.html\"\n",
    "        filepath = os.path.join(charts_dir, filename)\n",
    "        fig.write_html(filepath)\n",
    "        results['comparison_grids'][symbol] = filepath\n",
    "        print(f\"  \u2713 Created comparison for {symbol} \u2192 {filename}\")\n",
    "    fig_premiums = plot_premiums(coinbase_prem, kimchi_prem)\n",
    "    premium_path = os.path.join(charts_dir, \"premiums_cb_kimchi.html\")\n",
    "    fig_premiums.write_html(premium_path)\n",
    "    results['premium_charts'] = premium_path\n",
    "    print(f\"\\n=== Visualization Summary ===\")\n",
    "    print(f\"Total charts created: {len(individual_charts) + len(comparison_grids) + 2}\")\n",
    "    print(f\"Location: {os.path.abspath(charts_dir)}\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95c0f9de-dc49-4d67-89e5-939fcd8fbef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# Tablas con estilo (rich + HTML)\n",
    "# ======================================================================================\n",
    "\n",
    "def _style_html_table(df: pd.DataFrame, title: str) -> str:\n",
    "    if df.empty:\n",
    "        return f\"<h3>{title}</h3><p>No data available</p>\"\n",
    "    def format_value(val, col):\n",
    "        if pd.isna(val):\n",
    "            return \"\"\n",
    "        if col in [\"share_change_pp\", \"slope_pp_per_30d\"]:\n",
    "            return f\"{val:+.2f}\"\n",
    "        elif col == \"avg_weight\":\n",
    "            return f\"{val*100:.2f}%\"\n",
    "        else:\n",
    "            return str(val)\n",
    "    def get_color_for_change(val):\n",
    "        if pd.isna(val):\n",
    "            return \"\"\n",
    "        if val > 0:\n",
    "            intensity = min(abs(val) / 10, 1) * 0.3\n",
    "            return f\"background-color: rgba(34, 197, 94, {intensity});\"\n",
    "        else:\n",
    "            intensity = min(abs(val) / 10, 1) * 0.3\n",
    "            return f\"background-color: rgba(239, 68, 68, {intensity});\"\n",
    "    def get_color_for_slope(val):\n",
    "        if pd.isna(val) or val <= 0:\n",
    "            return \"\"\n",
    "        intensity = min(val / 10, 1) * 0.2\n",
    "        return f\"background-color: rgba(59, 130, 246, {intensity});\"\n",
    "    def get_bar_width(val, max_val):\n",
    "        if pd.isna(val) or max_val == 0:\n",
    "            return 0\n",
    "        return min((val / max_val) * 100, 100)\n",
    "    html_rows = []\n",
    "    max_weight = df[\"avg_weight\"].max() if \"avg_weight\" in df.columns else 1\n",
    "    for _, row in df.iterrows():\n",
    "        cells = []\n",
    "        for col in df.columns:\n",
    "            val = row[col]\n",
    "            formatted = format_value(val, col)\n",
    "            style = \"\"\n",
    "            if col == \"share_change_pp\":\n",
    "                style = get_color_for_change(val)\n",
    "            elif col == \"slope_pp_per_30d\":\n",
    "                style = get_color_for_slope(val)\n",
    "            elif col == \"avg_weight\" and not pd.isna(val):\n",
    "                bar_width = get_bar_width(val, max_weight)\n",
    "                style = f\"background: linear-gradient(90deg, rgba(99, 102, 241, 0.2) {bar_width}%, transparent {bar_width}%);\"\n",
    "            cells.append(f'<td style=\"padding:6px 8px;{style}\">{formatted}</td>')\n",
    "        html_rows.append(f'<tr>{\"\".join(cells)}</tr>')\n",
    "    headers = \"\".join([f'<th style=\"text-align:center;padding:6px 8px;background:#f3f4f6;\">{col}</th>' for col in df.columns])\n",
    "    html = f\"\"\"\n",
    "    <table style=\"border-collapse:collapse;width:100%;margin:10px 0;border:1px solid #e5e7eb;\">\n",
    "        <caption style=\"caption-side:top;font-size:16px;font-weight:600;padding:8px 0;text-align:left;\">\n",
    "            {title}\n",
    "        </caption>\n",
    "        <thead><tr>{headers}</tr></thead>\n",
    "        <tbody>{\"\".join(html_rows)}</tbody>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    return html\n",
    "\n",
    "def _export_winners_losers_html(w7: pd.DataFrame, w30: pd.DataFrame, w90: pd.DataFrame, out_path: str):\n",
    "    sections = []\n",
    "    for label, df in [(\"7D\", w7), (\"30D\", w30), (\"90D\", w90)]:\n",
    "        if df is None or df.empty:\n",
    "            continue\n",
    "        winners = df.sort_values(\"share_change_pp\", ascending=False).head(3)[[\"name\",\"share_change_pp\",\"slope_pp_per_30d\",\"avg_weight\"]]\n",
    "        losers  = df.sort_values(\"share_change_pp\", ascending=True).head(3)[[\"name\",\"share_change_pp\",\"slope_pp_per_30d\",\"avg_weight\"]]\n",
    "        sections.append(f\"<h2 style='font-family:system-ui;margin:16px 0 8px'>{label}</h2>\")\n",
    "        sections.append(_style_html_table(winners.reset_index(drop=True), f\"Ganadores {label} (pp de cuota)\"))\n",
    "        sections.append(_style_html_table(losers.reset_index(drop=True),  f\"Perdedores {label} (pp de cuota)\"))\n",
    "    html = f\"\"\"\n",
    "    <html><head><meta charset=\"utf-8\"><title>Ganadores/Perdedores BTC</title></head>\n",
    "    <body style=\"font-family:system-ui; margin:24px\">\n",
    "      <h1 style=\"margin:0 0 8px\">Ganadores / Perdedores por cuota</h1>\n",
    "      <div style=\"color:#666; margin-bottom:16px\">Generado autom\u00e1ticamente</div>\n",
    "      {''.join(sections)}\n",
    "    </body></html>\n",
    "    \"\"\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "\n",
    "def _print_top(df: pd.DataFrame, label: str):\n",
    "    if df is None or df.empty:\n",
    "        print(f\"{label}: sin datos\"); return\n",
    "    top_g = df.sort_values(\"share_change_pp\", ascending=False).head(3)[[\"name\",\"share_change_pp\",\"slope_pp_per_30d\",\"avg_weight\"]]\n",
    "    top_l = df.sort_values(\"share_change_pp\", ascending=True).head(3)[[\"name\",\"share_change_pp\",\"slope_pp_per_30d\",\"avg_weight\"]]\n",
    "    try:\n",
    "        from rich.console import Console\n",
    "        from rich.table import Table\n",
    "        from rich import box\n",
    "        console = Console()\n",
    "        def _as_table(title: str, data: pd.DataFrame, color: str):\n",
    "            t = Table(title=title, box=box.SIMPLE_HEAVY)\n",
    "            for col in [\"name\",\"share_change_pp\",\"slope_pp_per_30d\",\"avg_weight\"]:\n",
    "                t.add_column(col, justify=\"right\" if col != \"name\" else \"left\", style=\"bold\" if col==\"name\" else \"\")\n",
    "            for _, r in data.iterrows():\n",
    "                t.add_row(str(r[\"name\"]), f\"{r['share_change_pp']:+.2f}\", f\"{r['slope_pp_per_30d']:+.2f}\", f\"{r['avg_weight']:.2%}\", style=color)\n",
    "            return t\n",
    "        console.print(_as_table(f\"Ganadores {label} (pp de cuota)\", top_g, \"green\"))\n",
    "        console.print(_as_table(f\"Perdedores {label} (pp de cuota)\", top_l, \"red\"))\n",
    "    except Exception:\n",
    "        print(f\"Ganadores {label} (pp de cuota):\")\n",
    "        print(top_g.to_string(index=False, float_format=lambda x: f\"{x:.2f}\"))\n",
    "        print(f\"Perdedores {label} (pp de cuota):\")\n",
    "        print(top_l.to_string(index=False, float_format=lambda x: f\"{x:.2f}\"))\n",
    "    return top_g, top_l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Configuration\n",
    "Importing necessary libraries and setting up global variables."
   ],
   "id": "md_imports"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cd8ed12-8de0-4d58-92f8-749011497f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# Capa incremental, pesos y utilidades de resumen\n",
    "# ======================================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from typing import Optional, List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "def ensure_dataset(exchange: str, symbol: str, fetch_fn, *fetch_args, **fetch_kwargs) -> pd.DataFrame:\n",
    "    \"\"\"Carga desde CSV si existe. Si ya contiene HOY \u2192 skip. Si falta hoy \u2192 baja ayer+y hoy y hace append. Con logs.\"\"\"\n",
    "    fname = expected_csv_name(exchange, symbol)\n",
    "    fpath = os.path.join(DATA_DIR, fname)\n",
    "    today = today_utc_date()\n",
    "    existing = read_csv_if_exists(fpath)\n",
    "    if existing is not None and not existing.empty:\n",
    "        last_dates = existing[\"timestamp\"].dt.date\n",
    "        print(f\"[{exchange}] {symbol} \u2192 CSV: {len(existing)} filas; \u00faltima={existing['timestamp'].max().date()}\")\n",
    "        if (last_dates == today).any():\n",
    "            print(f\"[{exchange}] {symbol} \u2192 ya incluye HOY \u2192 skip\")\n",
    "            return existing\n",
    "        print(f\"[{exchange}] {symbol} \u2192 falta HOY \u2192 descargar AYER+HOY\")\n",
    "        start_ms = two_day_start_ms_utc() if ONLY_LAST_2_DAYS_IF_MISSING else None\n",
    "        new_df = fetch_fn(*fetch_args, **{**fetch_kwargs, \"start_ms\": start_ms})\n",
    "        if new_df is None or new_df.empty:\n",
    "            print(f\"[{exchange}] {symbol} \u2192 sin datos nuevos; mantengo CSV\")\n",
    "            return existing\n",
    "        start_dt = pd.to_datetime(start_ms, unit='ms', utc=True) if start_ms else existing['timestamp'].max().floor('D')\n",
    "        new_df = new_df[new_df['timestamp'] >= start_dt]\n",
    "        merged = pd.concat([existing, new_df], ignore_index=True).drop_duplicates(subset=['timestamp']).sort_values('timestamp')\n",
    "        added = len(merged) - len(existing)\n",
    "        print(f\"[{exchange}] {symbol} \u2192 a\u00f1adidas {added} filas; nueva \u00faltima={merged['timestamp'].max().date()}\")\n",
    "        save_csv(merged, fname)\n",
    "        return merged\n",
    "    print(f\"[{exchange}] {symbol} \u2192 no hay CSV \u2192 {'hist\u00f3rico' if FULL_HIST_ON_FIRST_RUN else 'AYER+HOY'}\")\n",
    "    start_ms = None if FULL_HIST_ON_FIRST_RUN else two_day_start_ms_utc()\n",
    "    df = fetch_fn(*fetch_args, **{**fetch_kwargs, \"start_ms\": start_ms})\n",
    "    print(f\"[{exchange}] {symbol} \u2192 descargadas {0 if df is None else len(df)} filas\")\n",
    "    save_csv(df, fname)\n",
    "    return df\n",
    "\n",
    "def compute_daily_weights(named_dfs: List[tuple]) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for name, df in named_dfs:\n",
    "        if df is None or df.empty:\n",
    "            continue\n",
    "        tmp = ensure_daily(df)[[\"date\",\"close\",\"volume\"]].copy()\n",
    "        tmp[\"name\"] = name\n",
    "        tmp[\"notional_usd\"] = tmp[\"close\"] * tmp[\"volume\"]\n",
    "        if \"hyperliquid\" in name.lower():\n",
    "            tmp[\"notional_usd\"] *= HYPERLIQUID_WEIGHT_PENALTY\n",
    "        frames.append(tmp[[\"date\",\"name\",\"notional_usd\"]])\n",
    "    if not frames:\n",
    "        return pd.DataFrame()\n",
    "    allv = pd.concat(frames, ignore_index=True)\n",
    "    allv = allv.groupby([\"date\",\"name\"], as_index=False)[\"notional_usd\"].sum()\n",
    "    sums = allv.groupby(\"date\")[\"notional_usd\"].sum().rename(\"tot_notional_usd\")\n",
    "    m = allv.merge(sums, on=\"date\", how=\"left\")\n",
    "    m[\"weight\"] = np.where(m[\"tot_notional_usd\"] > 0, m[\"notional_usd\"] / m[\"tot_notional_usd\"], 0.0)\n",
    "    return m[[\"date\",\"name\",\"notional_usd\",\"tot_notional_usd\",\"weight\"]].sort_values([\"date\",\"weight\"], ascending=[True, False])\n",
    "\n",
    "def summarize_period_weights(weights: pd.DataFrame, days: int) -> pd.DataFrame:\n",
    "    \"\"\"Resumen de pesos para los \u00faltimos N d\u00edas, con cambio vs. periodo previo y tendencia (pendiente mensualizada).\"\"\"\n",
    "    if weights is None or weights.empty:\n",
    "        return pd.DataFrame()\n",
    "    last_day = weights[\"date\"].max()\n",
    "    if pd.isna(last_day):\n",
    "        return pd.DataFrame()\n",
    "    period_start = last_day - pd.Timedelta(days=days-1)\n",
    "    cur = weights[(weights[\"date\"] >= period_start) & (weights[\"date\"] <= last_day)].copy()\n",
    "    if cur.empty:\n",
    "        return pd.DataFrame()\n",
    "    prev_start = period_start - pd.Timedelta(days=days)\n",
    "    prev_end = period_start - pd.Timedelta(days=1)\n",
    "    prev = weights[(weights[\"date\"] >= prev_start) & (weights[\"date\"] <= prev_end)].copy()\n",
    "\n",
    "    cur_agg = cur.groupby(\"name\").agg(\n",
    "        notional_usd=(\"notional_usd\",\"sum\"),\n",
    "        avg_weight=(\"weight\",\"mean\"),\n",
    "        days_observed=(\"date\",\"count\")\n",
    "    ).reset_index()\n",
    "\n",
    "    prev_w = prev.groupby(\"name\")[\"weight\"].mean().rename(\"avg_weight_prev\").reset_index()\n",
    "    prev_n = prev.groupby(\"name\")[\"notional_usd\"].sum().rename(\"notional_usd_prev\").reset_index()\n",
    "\n",
    "    out = cur_agg.merge(prev_w, on=\"name\", how=\"left\").merge(prev_n, on=\"name\", how=\"left\")\n",
    "    out[\"avg_weight_prev\"] = out[\"avg_weight_prev\"].fillna(0.0)\n",
    "    out[\"notional_usd_prev\"] = out[\"notional_usd_prev\"].fillna(0.0)\n",
    "    out[\"share_change_pp\"] = (out[\"avg_weight\"] - out[\"avg_weight_prev\"]) * 100.0\n",
    "    out[\"notional_change_pct\"] = np.where(out[\"notional_usd_prev\"] > 0.0,\n",
    "                                           (out[\"notional_usd\"] - out[\"notional_usd_prev\"]) / out[\"notional_usd_prev\"] * 100.0,\n",
    "                                           np.nan)\n",
    "    slopes = []\n",
    "    for name, g in cur.groupby(\"name\"):\n",
    "        if len(g) >= 2:\n",
    "            x = (g[\"date\"] - g[\"date\"].min()).dt.days.astype(float)\n",
    "            y = g[\"weight\"].astype(float).values\n",
    "            slope = float(np.polyfit(x, y, 1)[0]) * 100.0 * 30.0\n",
    "        else:\n",
    "            slope = np.nan\n",
    "        slopes.append({\"name\": name, \"slope_pp_per_30d\": slope})\n",
    "    out = out.merge(pd.DataFrame(slopes), on=\"name\", how=\"left\")\n",
    "    return out.sort_values([\"avg_weight\"], ascending=[False]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f580b956-bc6b-449e-8d28-160297ca1296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# M\u00e9todos de chequeo/diagn\u00f3stico de pares por exchange (como OKX), en celda independiente\n",
    "# ======================================================================================\n",
    "\n",
    "# ---------- OKX ----------\n",
    "def check_okx_pair_availability(symbol: str = \"BTC/USDC\") -> Dict:\n",
    "    instruments_url = \"https://www.okx.com/api/v5/public/instruments\"\n",
    "    okx_symbol = symbol.replace(\"/\", \"-\")\n",
    "    params = {\"instType\": \"SPOT\", \"instId\": okx_symbol}\n",
    "    try:\n",
    "        r = requests.get(instruments_url, params=params, headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            js = r.json()\n",
    "            if js.get(\"code\") == \"0\":\n",
    "                data = js.get(\"data\", [])\n",
    "                if data:\n",
    "                    inst = data[0]\n",
    "                    return {\n",
    "                        \"available\": True,\n",
    "                        \"symbol\": inst.get(\"instId\"),\n",
    "                        \"baseCcy\": inst.get(\"baseCcy\"),\n",
    "                        \"quoteCcy\": inst.get(\"quoteCcy\"),\n",
    "                        \"state\": inst.get(\"state\"),\n",
    "                        \"minSize\": inst.get(\"minSz\"),\n",
    "                        \"listTime\": inst.get(\"listTime\"),\n",
    "                        \"alias\": inst.get(\"alias\")\n",
    "                    }\n",
    "    except Exception as e:\n",
    "        print(f\"[OKX] Error checking pair availability: {e}\")\n",
    "    return {\"available\": False, \"symbol\": okx_symbol}\n",
    "\n",
    "def diagnose_okx_pair(symbol: str = \"BTC/USDC\"):\n",
    "    print(f\"\\n[OKX DIAGNOSTIC] Checking {symbol}...\")\n",
    "    info = check_okx_pair_availability(symbol)\n",
    "    if not info.get(\"available\"):\n",
    "        print(f\"[OKX DIAGNOSTIC] \u274c Pair {symbol} NOT FOUND on OKX Spot\")\n",
    "        print(f\"[OKX DIAGNOSTIC] Suggestion: Use BTC/USDT instead, which has better liquidity\")\n",
    "        return\n",
    "    print(f\"[OKX DIAGNOSTIC] \u2705 Pair found: {info.get('symbol')}\")\n",
    "    print(f\"[OKX DIAGNOSTIC] State: {info.get('state')}\")\n",
    "    if info.get('listTime'):\n",
    "        list_date = pd.to_datetime(int(info['listTime']), unit='ms')\n",
    "        days_since_listing = (datetime.now() - list_date).days\n",
    "        print(f\"[OKX DIAGNOSTIC] Listed: {list_date.date()} ({days_since_listing} days ago)\")\n",
    "        if days_since_listing < 30:\n",
    "            print(f\"[OKX DIAGNOSTIC] \u26a0\ufe0f This is a NEWLY LISTED pair (< 30 days)\")\n",
    "    ticker_url = \"https://www.okx.com/api/v5/market/ticker\"\n",
    "    params = {\"instId\": info.get('symbol')}\n",
    "    try:\n",
    "        r = requests.get(ticker_url, params=params, headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            js = r.json()\n",
    "            if js.get(\"code\") == \"0\" and js.get(\"data\"):\n",
    "                ticker = js[\"data\"][0]\n",
    "                vol24h = float(ticker.get(\"vol24h\", 0))\n",
    "                volCcy24h = float(ticker.get(\"volCcy24h\", 0))\n",
    "                print(f\"[OKX DIAGNOSTIC] 24h Volume: {vol24h:,.2f} BTC (${volCcy24h:,.0f})\")\n",
    "                if volCcy24h < 100000:\n",
    "                    print(f\"[OKX DIAGNOSTIC] \u26a0\ufe0f LOW LIQUIDITY: 24h volume < $100k\")\n",
    "    except Exception as e:\n",
    "        print(f\"[OKX DIAGNOSTIC] Could not fetch volume data: {e}\")\n",
    "    print(\"[OKX DIAGNOSTIC] Complete\\n\")\n",
    "\n",
    "# ---------- Binance ----------\n",
    "def check_binance_pair_availability(symbol: str = \"BTC/USDT\") -> Dict:\n",
    "    url = \"https://api.binance.com/api/v3/exchangeInfo\"\n",
    "    sym = symbol.replace(\"/\", \"\")\n",
    "    try:\n",
    "        r = requests.get(url, params={\"symbol\": sym}, headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            js = r.json()\n",
    "            if js.get(\"symbols\"):\n",
    "                s = js[\"symbols\"][0]\n",
    "                return {\"available\": True, \"symbol\": s.get(\"symbol\"), \"base\": s.get(\"baseAsset\"),\n",
    "                        \"quote\": s.get(\"quoteAsset\"), \"status\": s.get(\"status\")}\n",
    "    except Exception as e:\n",
    "        print(f\"[Binance] Availability error: {e}\")\n",
    "    return {\"available\": False, \"symbol\": sym}\n",
    "\n",
    "def diagnose_binance_pair(symbol: str = \"BTC/USDT\"):\n",
    "    print(f\"\\n[Binance DIAGNOSTIC] Checking {symbol}...\")\n",
    "    info = check_binance_pair_availability(symbol)\n",
    "    if not info.get(\"available\"):\n",
    "        print(f\"[Binance DIAGNOSTIC] \u274c Pair not found\"); return\n",
    "    print(f\"[Binance DIAGNOSTIC] \u2705 {info['symbol']} | status={info.get('status')} base={info.get('base')} quote={info.get('quote')}\")\n",
    "    # 24h ticker\n",
    "    try:\n",
    "        r = requests.get(\"https://api.binance.com/api/v3/ticker/24hr\",\n",
    "                         params={\"symbol\": info['symbol']}, headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            t = r.json()\n",
    "            print(f\"[Binance DIAGNOSTIC] 24h vol: {float(t.get('volume',0)):.4f} {info.get('base')} | quoteVol: {float(t.get('quoteVolume',0)):.0f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Binance DIAGNOSTIC] Ticker error: {e}\")\n",
    "\n",
    "# ---------- Bybit ----------\n",
    "def check_bybit_pair_availability(symbol: str = \"BTC/USDT\") -> Dict:\n",
    "    url = \"https://api.bybit.com/v5/market/instruments-info\"\n",
    "    sym = symbol.replace(\"/\", \"\")\n",
    "    try:\n",
    "        r = requests.get(url, params={\"category\":\"spot\",\"symbol\": sym}, headers=USER_AGENT, timeout=30)\n",
    "        js = r.json()\n",
    "        if js.get(\"retCode\") == 0 and js.get(\"result\", {}).get(\"list\"):\n",
    "            s = js[\"result\"][\"list\"][0]\n",
    "            return {\"available\": True, \"symbol\": s.get(\"symbol\"), \"status\": s.get(\"status\")}\n",
    "    except Exception as e:\n",
    "        print(f\"[Bybit] Availability error: {e}\")\n",
    "    return {\"available\": False, \"symbol\": sym}\n",
    "\n",
    "def diagnose_bybit_pair(symbol: str = \"BTC/USDT\"):\n",
    "    print(f\"\\n[Bybit DIAGNOSTIC] Checking {symbol}...\")\n",
    "    info = check_bybit_pair_availability(symbol)\n",
    "    if not info.get(\"available\"):\n",
    "        print(\"[Bybit DIAGNOSTIC] \u274c Pair not found\"); return\n",
    "    print(f\"[Bybit DIAGNOSTIC] \u2705 {info['symbol']} | status={info.get('status')}\")\n",
    "    try:\n",
    "        r = requests.get(\"https://api.bybit.com/v5/market/tickers\",\n",
    "                         params={\"category\":\"spot\",\"symbol\": info['symbol']}, headers=USER_AGENT, timeout=30)\n",
    "        js = r.json()\n",
    "        if js.get(\"retCode\") == 0 and js.get(\"result\",{}).get(\"list\"):\n",
    "            t = js[\"result\"][\"list\"][0]\n",
    "            print(f\"[Bybit DIAGNOSTIC] 24h vol: {float(t.get('volume24h',0)):.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Bybit DIAGNOSTIC] Ticker error: {e}\")\n",
    "\n",
    "# ---------- Coinbase ----------\n",
    "def check_coinbase_product_availability(symbol: str = \"BTC/USD\") -> Dict:\n",
    "    prod = CoinbaseFetcher._pick_product(symbol)\n",
    "    url = f\"https://api.exchange.coinbase.com/products/{prod}\"\n",
    "    try:\n",
    "        r = requests.get(url, headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            js = r.json()\n",
    "            return {\"available\": True, \"id\": js.get(\"id\"), \"base\": js.get(\"base_currency\"), \"quote\": js.get(\"quote_currency\"),\n",
    "                    \"status\": js.get(\"status\")}\n",
    "    except Exception as e:\n",
    "        print(f\"[Coinbase] Availability error: {e}\")\n",
    "    return {\"available\": False, \"id\": prod}\n",
    "\n",
    "def diagnose_coinbase_product(symbol: str = \"BTC/USD\"):\n",
    "    print(f\"\\n[Coinbase DIAGNOSTIC] Checking {symbol}...\")\n",
    "    info = check_coinbase_product_availability(symbol)\n",
    "    if not info.get(\"available\"):\n",
    "        print(\"[Coinbase DIAGNOSTIC] \u274c Product not found\"); return\n",
    "    print(f\"[Coinbase DIAGNOSTIC] \u2705 {info['id']} | status={info.get('status')} base={info.get('base')} quote={info.get('quote')}\")\n",
    "    try:\n",
    "        r = requests.get(f\"https://api.exchange.coinbase.com/products/{info['id']}/ticker\", headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            t = r.json()\n",
    "            print(f\"[Coinbase DIAGNOSTIC] 24h vol: {float(t.get('volume',0)):.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Coinbase DIAGNOSTIC] Ticker error: {e}\")\n",
    "\n",
    "# ---------- Crypto.com ----------\n",
    "def check_crypto_com_pair_availability(symbol: str = \"BTC/USDT\") -> Dict:\n",
    "    url = \"https://api.crypto.com/exchange/v1/public/get-instruments\"\n",
    "    inst = symbol.replace(\"/\", \"_\")\n",
    "    try:\n",
    "        r = requests.get(url, headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            js = r.json()\n",
    "            data = js.get(\"result\", {}).get(\"instruments\", [])\n",
    "            ok = any(x.get(\"instrument_name\") == inst for x in data)\n",
    "            return {\"available\": ok, \"instrument_name\": inst}\n",
    "    except Exception as e:\n",
    "        print(f\"[Crypto.com] Availability error: {e}\")\n",
    "    return {\"available\": False, \"instrument_name\": inst}\n",
    "\n",
    "def diagnose_crypto_com_pair(symbol: str = \"BTC/USDT\"):\n",
    "    print(f\"\\n[Crypto.com DIAGNOSTIC] Checking {symbol}...\")\n",
    "    info = check_crypto_com_pair_availability(symbol)\n",
    "    if not info.get(\"available\"):\n",
    "        print(\"[Crypto.com DIAGNOSTIC] \u274c Pair not found\"); return\n",
    "    print(f\"[Crypto.com DIAGNOSTIC] \u2705 {info['instrument_name']}\")\n",
    "    try:\n",
    "        r = requests.get(\"https://api.crypto.com/exchange/v1/public/get-ticker\",\n",
    "                         params={\"instrument_name\": info['instrument_name']}, headers=USER_AGENT, timeout=30)\n",
    "        js = r.json()\n",
    "        if js.get(\"code\") == 0 and js.get(\"result\",{}).get(\"data\"):\n",
    "            t = js[\"result\"][\"data\"][0]\n",
    "            print(f\"[Crypto.com DIAGNOSTIC] 24h vol: {float(t.get('v',0)):.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Crypto.com DIAGNOSTIC] Ticker error: {e}\")\n",
    "\n",
    "# ---------- Hyperliquid ----------\n",
    "def check_hyperliquid_pair_availability(symbol: str = \"BTC/USDC\") -> Dict:\n",
    "    coin = symbol.split('/')[0].upper()\n",
    "    payload = {\"type\": \"candleSnapshot\", \"req\": {\"coin\": coin, \"interval\": \"1d\",\n",
    "                                                 \"startTime\": to_ms(datetime.now(timezone.utc)-timedelta(days=2)),\n",
    "                                                 \"endTime\": to_ms(datetime.now(timezone.utc))}}\n",
    "    try:\n",
    "        js = HyperliquidFetcher._post(payload)\n",
    "        ok = isinstance(js, list) and len(js) > 0\n",
    "        return {\"available\": ok, \"coin\": coin}\n",
    "    except Exception as e:\n",
    "        print(f\"[Hyperliquid] Availability error: {e}\")\n",
    "    return {\"available\": False, \"coin\": coin}\n",
    "\n",
    "def diagnose_hyperliquid_pair(symbol: str = \"BTC/USDC\"):\n",
    "    print(f\"\\n[Hyperliquid DIAGNOSTIC] Checking {symbol}...\")\n",
    "    info = check_hyperliquid_pair_availability(symbol)\n",
    "    if not info.get(\"available\"):\n",
    "        print(\"[Hyperliquid DIAGNOSTIC] \u274c Pair not available\"); return\n",
    "    print(f\"[Hyperliquid DIAGNOSTIC] \u2705 {info['coin']}\")\n",
    "    # No simple public 24h ticker USD endpoint; candle snapshot acts as basic check.\n",
    "\n",
    "# ---------- Upbit ----------\n",
    "def check_upbit_pair_availability(symbol: str = \"BTC/KRW\") -> Dict:\n",
    "    market = \"KRW-BTC\" if symbol.upper().endswith(\"KRW\") else (\"USDT-BTC\" if symbol.upper().endswith(\"USDT\") else \"KRW-BTC\")\n",
    "    try:\n",
    "        r = requests.get(\"https://api.upbit.com/v1/market/all\", headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            arr = r.json()\n",
    "            ok = any(x.get(\"market\") == market for x in arr)\n",
    "            return {\"available\": ok, \"market\": market}\n",
    "    except Exception as e:\n",
    "        print(f\"[Upbit] Availability error: {e}\")\n",
    "    return {\"available\": False, \"market\": market}\n",
    "\n",
    "def diagnose_upbit_pair(symbol: str = \"BTC/KRW\"):\n",
    "    print(f\"\\n[Upbit DIAGNOSTIC] Checking {symbol}...\")\n",
    "    info = check_upbit_pair_availability(symbol)\n",
    "    if not info.get(\"available\"):\n",
    "        print(\"[Upbit DIAGNOSTIC] \u274c Market not found\"); return\n",
    "    print(f\"[Upbit DIAGNOSTIC] \u2705 {info['market']}\")\n",
    "    try:\n",
    "        r = requests.get(\"https://api.upbit.com/v1/ticker\", params={\"markets\": info['market']}, headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            t = r.json()[0]\n",
    "            print(f\"[Upbit DIAGNOSTIC] 24h vol: {float(t.get('acc_trade_volume_24h',0)):.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Upbit DIAGNOSTIC] Ticker error: {e}\")\n",
    "\n",
    "# ---------- Bitget ----------\n",
    "def check_bitget_pair_availability(symbol: str = \"BTC/USDT\") -> Dict:\n",
    "    sym = symbol.replace(\"/\", \"\")\n",
    "    try:\n",
    "        r = requests.get(\"https://api.bitget.com/api/v2/spot/public/symbols\", headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            js = r.json()\n",
    "            data = js.get(\"data\", [])\n",
    "            ok = any(x.get(\"symbol\") == sym for x in data)\n",
    "            return {\"available\": ok, \"symbol\": sym}\n",
    "    except Exception as e:\n",
    "        print(f\"[Bitget] Availability error: {e}\")\n",
    "    return {\"available\": False, \"symbol\": sym}\n",
    "\n",
    "def diagnose_bitget_pair(symbol: str = \"BTC/USDT\"):\n",
    "    print(f\"\\n[Bitget DIAGNOSTIC] Checking {symbol}...\")\n",
    "    info = check_bitget_pair_availability(symbol)\n",
    "    if not info.get(\"available\"):\n",
    "        print(\"[Bitget DIAGNOSTIC] \u274c Pair not found\"); return\n",
    "    print(f\"[Bitget DIAGNOSTIC] \u2705 {info['symbol']}\")\n",
    "    try:\n",
    "        r = requests.get(\"https://api.bitget.com/api/v2/spot/market/tickers\", params={\"symbol\": info['symbol']}, headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            js = r.json()\n",
    "            data = js.get(\"data\", [])\n",
    "            if data:\n",
    "                t = data[0]\n",
    "                print(f\"[Bitget DIAGNOSTIC] 24h vol: {float(t.get('baseVol',0)):.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Bitget DIAGNOSTIC] Ticker error: {e}\")\n",
    "\n",
    "# ---------- MEXC ----------\n",
    "def check_mexc_pair_availability(symbol: str = \"BTC/USDT\") -> Dict:\n",
    "    sym = symbol.replace(\"/\", \"\")\n",
    "    try:\n",
    "        r = requests.get(\"https://api.mexc.com/api/v3/exchangeInfo\", params={\"symbol\": sym}, headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            js = r.json()\n",
    "            s = js.get(\"symbols\", [])\n",
    "            if s:\n",
    "                rec = s[0]\n",
    "                return {\"available\": True, \"symbol\": rec.get(\"symbol\"), \"status\": rec.get(\"status\")}\n",
    "    except Exception as e:\n",
    "        print(f\"[MEXC] Availability error: {e}\")\n",
    "    return {\"available\": False, \"symbol\": sym}\n",
    "\n",
    "def diagnose_mexc_pair(symbol: str = \"BTC/USDT\"):\n",
    "    print(f\"\\n[MEXC DIAGNOSTIC] Checking {symbol}...\")\n",
    "    info = check_mexc_pair_availability(symbol)\n",
    "    if not info.get(\"available\"):\n",
    "        print(\"[MEXC DIAGNOSTIC] \u274c Pair not found\"); return\n",
    "    print(f\"[MEXC DIAGNOSTIC] \u2705 {info['symbol']} | status={info.get('status')}\")\n",
    "    try:\n",
    "        r = requests.get(\"https://api.mexc.com/api/v3/ticker/24hr\", params={\"symbol\": info['symbol']}, headers=USER_AGENT, timeout=30)\n",
    "        if r.status_code == 200:\n",
    "            t = r.json()\n",
    "            print(f\"[MEXC DIAGNOSTIC] 24h vol: {float(t.get('volume',0)):.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[MEXC DIAGNOSTIC] Ticker error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "Entry point to run the data collection and analysis pipeline."
   ],
   "id": "md_main"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc138163-4111-42dc-b671-6f87a16bd6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OKX DIAGNOSTIC] Checking BTC/USDC...\n",
      "[OKX DIAGNOSTIC] \u2705 Pair found: BTC-USDC\n",
      "[OKX DIAGNOSTIC] State: live\n",
      "[OKX DIAGNOSTIC] Listed: 2025-08-21 (78 days ago)\n",
      "[OKX DIAGNOSTIC] 24h Volume: 6.68 BTC ($680,873)\n",
      "[OKX DIAGNOSTIC] Complete\n",
      "\n",
      "=== Descargando/Actualizando velas 1D ===\n",
      "[Binance] BTC/USDT \u2192 no hay CSV \u2192 hist\u00f3rico\n",
      "[Binance] BTC/USDT \u2192 descargadas 2260 filas\n",
      "[Binance] BTC/USDC \u2192 no hay CSV \u2192 hist\u00f3rico\n",
      "[Binance] BTC/USDC \u2192 descargadas 2097 filas\n",
      "[Bybit] BTC/USDT \u2192 no hay CSV \u2192 hist\u00f3rico\n",
      "[Bybit] BTC/USDT \u2192 descargadas 1587 filas\n",
      "[Bybit] BTC/USDC \u2192 no hay CSV \u2192 hist\u00f3rico\n",
      "[Bybit] BTC/USDC \u2192 descargadas 1430 filas\n",
      "[OKX] BTC/USDT \u2192 no hay CSV \u2192 hist\u00f3rico\n",
      "[OKX] Fetching BTC/USDT interval=1D\n",
      "[OKX] Step 1: Fetching recent data for BTC/USDT...\n",
      "[OKX] Got 100 recent candles\n",
      "[OKX] Recent data range: 2025-07-30 \u2192 2025-11-06\n",
      "[OKX] Step 2: Fetching historical data for BTC/USDT...\n",
      "[OKX] Historical: 5 pages, 601 total candles, oldest so far: 2024-03-16\n",
      "[OKX] Historical: 10 pages, 1101 total candles, oldest so far: 2022-11-02\n",
      "[OKX] Historical: 15 pages, 1601 total candles, oldest so far: 2021-06-20\n",
      "[OKX] Historical: 20 pages, 2101 total candles, oldest so far: 2020-02-06\n",
      "[OKX] Historical: 25 pages, 2601 total candles, oldest so far: 2018-09-24\n",
      "[OKX] Historical fetch complete: 30 pages\n",
      "[OKX] BTC/USDT final dataset: 2950 rows\n",
      "[OKX] Date range: 2017-10-10 \u2192 2025-11-06\n",
      "[OKX] BTC/USDT \u2192 descargadas 2950 filas\n",
      "[OKX] BTC/USDC \u2192 no hay CSV \u2192 hist\u00f3rico\n",
      "[OKX] Fetching BTC/USDC interval=1D\n",
      "[OKX] Step 1: Fetching recent data for BTC/USDC...\n",
      "[OKX] Got 79 recent candles\n",
      "[OKX] Recent data range: 2025-08-20 \u2192 2025-11-06\n",
      "[OKX] Step 2: Fetching historical data for BTC/USDC...\n",
      "[OKX] No new historical data, stopping at page 0\n",
      "[OKX] BTC/USDC final dataset: 79 rows\n",
      "[OKX] Date range: 2025-08-20 \u2192 2025-11-06\n",
      "[OKX] \u26a0\ufe0f LIMITED DATA for BTC/USDC: Only 79 candles available\n",
      "[OKX] This suggests BTC/USDC is:\n",
      "[OKX]   - A newly listed pair on OKX\n",
      "[OKX]   - A low liquidity/volume pair\n",
      "[OKX]   - Or has limited historical data on this exchange\n",
      "[OKX] Consider using a different exchange for this pair if more history is needed\n",
      "[OKX] BTC/USDC \u2192 descargadas 79 filas\n",
      "[Coinbase] BTC/USD \u2192 no hay CSV \u2192 hist\u00f3rico\n",
      "[Coinbase] BTC/USD \u2192 descargadas 2260 filas\n",
      "[Crypto.com] BTC/USDT \u2192 no hay CSV \u2192 hist\u00f3rico\n",
      "[Crypto.com] BTC/USDT \u2192 descargadas 1873 filas\n",
      "[Crypto.com] BTC/USDC \u2192 no hay CSV \u2192 hist\u00f3rico\n",
      "[Crypto.com] BTC/USDC \u2192 descargadas 1133 filas\n",
      "[Hyperliquid] BTC/USDC \u2192 no hay CSV \u2192 hist\u00f3rico\n",
      "[Hyperliquid] Fetch BTC/USDC interval=1d since=2019-09-01 \u2192 2025-11-07\n",
      "[Hyperliquid] Fetched rows: 1907; range: 2020-08-19 \u2192 2025-11-07\n",
      "[Hyperliquid] BTC/USDC \u2192 descargadas 1907 filas\n",
      "[Upbit] BTC/KRW \u2192 no hay CSV \u2192 hist\u00f3rico\n",
      "[Upbit] Fetching BTC/KRW (market: KRW-BTC)...\n",
      "[Upbit] Page 1: 200 candles, oldest: 2025-04-22\n",
      "[Upbit] Page 2: 200 candles, oldest: 2024-10-04\n",
      "[Upbit] Page 3: 200 candles, oldest: 2024-03-18\n",
      "[Upbit] Page 4: 200 candles, oldest: 2023-08-31\n",
      "[Upbit] Page 5: 200 candles, oldest: 2023-02-12\n",
      "[Upbit] Page 6: 200 candles, oldest: 2022-07-27\n",
      "[Upbit] Page 7: 200 candles, oldest: 2022-01-08\n",
      "[Upbit] Page 8: 200 candles, oldest: 2021-06-22\n",
      "[Upbit] Page 9: 200 candles, oldest: 2020-12-04\n",
      "[Upbit] Page 10: 200 candles, oldest: 2020-05-18\n",
      "[Upbit] Page 11: 200 candles, oldest: 2019-10-31\n",
      "[Upbit] Page 12: 200 candles, oldest: 2019-04-14\n",
      "[Upbit] Page 13: 200 candles, oldest: 2018-09-26\n",
      "[Upbit] Page 14: 200 candles, oldest: 2018-03-10\n",
      "[Upbit] Page 15: 166 candles, oldest: 2017-09-25\n",
      "[Upbit] Got 166 < 200 candles, reached end of data\n",
      "[Upbit] Final dataset: 2966 rows, range: 2017-09-25 \u2192 2025-11-07\n",
      "[Upbit] BTC/KRW \u2192 descargadas 2966 filas\n",
      "[Bitget] BTC/USDT \u2192 no hay CSV \u2192 hist\u00f3rico\n",
      "[Bitget] Fetched rows: 2650; range: 2018-07-24 \u2192 2025-11-06\n",
      "[Bitget] BTC/USDT \u2192 descargadas 2650 filas\n",
      "[Bitget] BTC/USDC \u2192 no hay CSV \u2192 hist\u00f3rico\n",
      "[Bitget] Fetched rows: 1256; range: 2022-05-25 \u2192 2025-11-06\n",
      "[Bitget] BTC/USDC \u2192 descargadas 1256 filas\n",
      "[MEXC] BTC/USDT \u2192 no hay CSV \u2192 hist\u00f3rico\n",
      "[MEXC] BTC/USDT \u2192 descargadas 1000 filas\n",
      "[MEXC] BTC/USDC \u2192 no hay CSV \u2192 hist\u00f3rico\n",
      "[MEXC] BTC/USDC \u2192 descargadas 1000 filas\n",
      "=== Descargando USDKRW ===\n",
      "[FX] USDKRW fuente: Frankfurter https://api.frankfurter.app; rango: 2017-09-25 \u2192 2025-11-03 (mediana=1204.91)\n",
      "=== Calculando Coinbase Premium ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pedro\\AppData\\Local\\Temp\\ipykernel_378520\\3912009310.py:66: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref = ref.groupby(\"date\").apply(lambda g: pd.Series({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calculando Kimchi Premium ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pedro\\AppData\\Local\\Temp\\ipykernel_378520\\970366390.py:88: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ref_usd = ref_concat.groupby(\"date\").apply(lambda g: pd.Series({\n",
      "C:\\Users\\Pedro\\AppData\\Local\\Temp\\ipykernel_378520\\970366390.py:92: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  ref_usd[\"timestamp\"] = (ref_usd[\"date\"].dt.tz_convert(\"UTC\") if pd.api.types.is_datetime64tz_dtype(ref_usd[\"date\"]) else pd.to_datetime(ref_usd[\"date\"], utc=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Construyendo vela agregada (USD) ===\n",
      "[AGG] outliers filtrados: 7 filas\n",
      "[AGG]   - Binance: 3\n",
      "[AGG]   - Bitget: 1\n",
      "[AGG]   - Bybit: 1\n",
      "[AGG]   - Coinbase: 1\n",
      "[AGG]   - OKX: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pedro\\AppData\\Local\\Temp\\ipykernel_378520\\2142712328.py:44: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  open_vw  = grp.apply(lambda g: np.average(g[\"open\"],  weights=g[\"w_usd\"]))\n",
      "C:\\Users\\Pedro\\AppData\\Local\\Temp\\ipykernel_378520\\2142712328.py:45: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  close_vw = grp.apply(lambda g: np.average(g[\"close\"], weights=g[\"w_usd\"]))\n",
      "C:\\Users\\Pedro\\AppData\\Local\\Temp\\ipykernel_378520\\2142712328.py:52: DeprecationWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  out[\"timestamp\"] = (out[\"date\"].dt.tz_convert(\"UTC\") if pd.api.types.is_datetime64tz_dtype(out[\"date\"]) else pd.to_datetime(out[\"date\"], utc=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AGG] d\u00edas agregados: 2949; rango: 2017-10-11 \u2192 2025-11-07 (vol_usd sum ok)\n",
      "=== Pesos por exchange (diario) ===\n",
      "Pesos \u00faltima fecha:\n",
      "               name  weight\n",
      "      MEXC BTC/USDT  25.58%\n",
      "   Binance BTC/USDT  25.04%\n",
      "Hyperliquid BTC/USD  20.17%\n",
      "     Bybit BTC/USDT  11.19%\n",
      "   Coinbase BTC/USD   6.50%\n",
      "Crypto.com BTC/USDT   5.42%\n",
      "   Binance BTC/USDC   4.72%\n",
      "     Bybit BTC/USDC   1.06%\n",
      "      MEXC BTC/USDC   0.32%\n",
      "Crypto.com BTC/USDC   0.00%\n",
      "=== Pesos 7D/30D/90D y tendencias ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Ganadores 7D (pp de cuota)                      </span>\n",
       "                                                                      \n",
       " <span style=\"font-weight: bold\"> name             </span> <span style=\"font-weight: bold\"> share_change_pp </span> <span style=\"font-weight: bold\"> slope_pp_per_30d </span> <span style=\"font-weight: bold\"> avg_weight </span> \n",
       " \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Binance BTC/USDT </span> <span style=\"color: #008000; text-decoration-color: #008000\">           +2.93 </span> <span style=\"color: #008000; text-decoration-color: #008000\">           +20.55 </span> <span style=\"color: #008000; text-decoration-color: #008000\">     21.46% </span> \n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Bitget BTC/USDT  </span> <span style=\"color: #008000; text-decoration-color: #008000\">           +1.58 </span> <span style=\"color: #008000; text-decoration-color: #008000\">           -11.51 </span> <span style=\"color: #008000; text-decoration-color: #008000\">      4.41% </span> \n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Coinbase BTC/USD </span> <span style=\"color: #008000; text-decoration-color: #008000\">           +1.10 </span> <span style=\"color: #008000; text-decoration-color: #008000\">           +10.14 </span> <span style=\"color: #008000; text-decoration-color: #008000\">      6.51% </span> \n",
       "                                                                      \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Ganadores 7D (pp de cuota)                      \u001b[0m\n",
       "                                                                      \n",
       " \u001b[1m \u001b[0m\u001b[1mname            \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mshare_change_pp\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mslope_pp_per_30d\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mavg_weight\u001b[0m\u001b[1m \u001b[0m \n",
       " \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \n",
       " \u001b[1;32m \u001b[0m\u001b[1;32mBinance BTC/USDT\u001b[0m\u001b[1;32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          +2.93\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          +20.55\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m    21.46%\u001b[0m\u001b[32m \u001b[0m \n",
       " \u001b[1;32m \u001b[0m\u001b[1;32mBitget BTC/USDT \u001b[0m\u001b[1;32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          +1.58\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          -11.51\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m     4.41%\u001b[0m\u001b[32m \u001b[0m \n",
       " \u001b[1;32m \u001b[0m\u001b[1;32mCoinbase BTC/USD\u001b[0m\u001b[1;32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          +1.10\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          +10.14\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m     6.51%\u001b[0m\u001b[32m \u001b[0m \n",
       "                                                                      \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                       Perdedores 7D (pp de cuota)                       </span>\n",
       "                                                                         \n",
       " <span style=\"font-weight: bold\"> name                </span> <span style=\"font-weight: bold\"> share_change_pp </span> <span style=\"font-weight: bold\"> slope_pp_per_30d </span> <span style=\"font-weight: bold\"> avg_weight </span> \n",
       " \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Hyperliquid BTC/USD </span> <span style=\"color: #800000; text-decoration-color: #800000\">           -2.75 </span> <span style=\"color: #800000; text-decoration-color: #800000\">           +21.72 </span> <span style=\"color: #800000; text-decoration-color: #800000\">     20.94% </span> \n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> MEXC BTC/USDT       </span> <span style=\"color: #800000; text-decoration-color: #800000\">           -0.67 </span> <span style=\"color: #800000; text-decoration-color: #800000\">            +9.77 </span> <span style=\"color: #800000; text-decoration-color: #800000\">     20.66% </span> \n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Bybit BTC/USDT      </span> <span style=\"color: #800000; text-decoration-color: #800000\">           -0.48 </span> <span style=\"color: #800000; text-decoration-color: #800000\">            +7.75 </span> <span style=\"color: #800000; text-decoration-color: #800000\">      8.10% </span> \n",
       "                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                       Perdedores 7D (pp de cuota)                       \u001b[0m\n",
       "                                                                         \n",
       " \u001b[1m \u001b[0m\u001b[1mname               \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mshare_change_pp\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mslope_pp_per_30d\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mavg_weight\u001b[0m\u001b[1m \u001b[0m \n",
       " \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \n",
       " \u001b[1;31m \u001b[0m\u001b[1;31mHyperliquid BTC/USD\u001b[0m\u001b[1;31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m          -2.75\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m          +21.72\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m    20.94%\u001b[0m\u001b[31m \u001b[0m \n",
       " \u001b[1;31m \u001b[0m\u001b[1;31mMEXC BTC/USDT      \u001b[0m\u001b[1;31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m          -0.67\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m           +9.77\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m    20.66%\u001b[0m\u001b[31m \u001b[0m \n",
       " \u001b[1;31m \u001b[0m\u001b[1;31mBybit BTC/USDT     \u001b[0m\u001b[1;31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m          -0.48\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m           +7.75\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m     8.10%\u001b[0m\u001b[31m \u001b[0m \n",
       "                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                     Ganadores 30D (pp de cuota)                      </span>\n",
       "                                                                      \n",
       " <span style=\"font-weight: bold\"> name             </span> <span style=\"font-weight: bold\"> share_change_pp </span> <span style=\"font-weight: bold\"> slope_pp_per_30d </span> <span style=\"font-weight: bold\"> avg_weight </span> \n",
       " \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Binance BTC/USDT </span> <span style=\"color: #008000; text-decoration-color: #008000\">           +3.05 </span> <span style=\"color: #008000; text-decoration-color: #008000\">            +3.08 </span> <span style=\"color: #008000; text-decoration-color: #008000\">     19.27% </span> \n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> OKX BTC/USDT     </span> <span style=\"color: #008000; text-decoration-color: #008000\">           +1.41 </span> <span style=\"color: #008000; text-decoration-color: #008000\">            +1.61 </span> <span style=\"color: #008000; text-decoration-color: #008000\">      7.82% </span> \n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Binance BTC/USDC </span> <span style=\"color: #008000; text-decoration-color: #008000\">           +0.65 </span> <span style=\"color: #008000; text-decoration-color: #008000\">            +0.77 </span> <span style=\"color: #008000; text-decoration-color: #008000\">      4.71% </span> \n",
       "                                                                      \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                     Ganadores 30D (pp de cuota)                      \u001b[0m\n",
       "                                                                      \n",
       " \u001b[1m \u001b[0m\u001b[1mname            \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mshare_change_pp\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mslope_pp_per_30d\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mavg_weight\u001b[0m\u001b[1m \u001b[0m \n",
       " \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \n",
       " \u001b[1;32m \u001b[0m\u001b[1;32mBinance BTC/USDT\u001b[0m\u001b[1;32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          +3.05\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m           +3.08\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m    19.27%\u001b[0m\u001b[32m \u001b[0m \n",
       " \u001b[1;32m \u001b[0m\u001b[1;32mOKX BTC/USDT    \u001b[0m\u001b[1;32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          +1.41\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m           +1.61\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m     7.82%\u001b[0m\u001b[32m \u001b[0m \n",
       " \u001b[1;32m \u001b[0m\u001b[1;32mBinance BTC/USDC\u001b[0m\u001b[1;32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          +0.65\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m           +0.77\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m     4.71%\u001b[0m\u001b[32m \u001b[0m \n",
       "                                                                      \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                     Perdedores 30D (pp de cuota)                     </span>\n",
       "                                                                      \n",
       " <span style=\"font-weight: bold\"> name             </span> <span style=\"font-weight: bold\"> share_change_pp </span> <span style=\"font-weight: bold\"> slope_pp_per_30d </span> <span style=\"font-weight: bold\"> avg_weight </span> \n",
       " \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Bitget BTC/USDT  </span> <span style=\"color: #800000; text-decoration-color: #800000\">           -3.61 </span> <span style=\"color: #800000; text-decoration-color: #800000\">            -7.39 </span> <span style=\"color: #800000; text-decoration-color: #800000\">      5.37% </span> \n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Bybit BTC/USDT   </span> <span style=\"color: #800000; text-decoration-color: #800000\">           -1.69 </span> <span style=\"color: #800000; text-decoration-color: #800000\">            -0.65 </span> <span style=\"color: #800000; text-decoration-color: #800000\">      8.48% </span> \n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Coinbase BTC/USD </span> <span style=\"color: #800000; text-decoration-color: #800000\">           -0.55 </span> <span style=\"color: #800000; text-decoration-color: #800000\">            -0.04 </span> <span style=\"color: #800000; text-decoration-color: #800000\">      6.24% </span> \n",
       "                                                                      \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                     Perdedores 30D (pp de cuota)                     \u001b[0m\n",
       "                                                                      \n",
       " \u001b[1m \u001b[0m\u001b[1mname            \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mshare_change_pp\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mslope_pp_per_30d\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mavg_weight\u001b[0m\u001b[1m \u001b[0m \n",
       " \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \n",
       " \u001b[1;31m \u001b[0m\u001b[1;31mBitget BTC/USDT \u001b[0m\u001b[1;31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m          -3.61\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m           -7.39\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m     5.37%\u001b[0m\u001b[31m \u001b[0m \n",
       " \u001b[1;31m \u001b[0m\u001b[1;31mBybit BTC/USDT  \u001b[0m\u001b[1;31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m          -1.69\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m           -0.65\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m     8.48%\u001b[0m\u001b[31m \u001b[0m \n",
       " \u001b[1;31m \u001b[0m\u001b[1;31mCoinbase BTC/USD\u001b[0m\u001b[1;31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m          -0.55\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m           -0.04\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m     6.24%\u001b[0m\u001b[31m \u001b[0m \n",
       "                                                                      \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                       Ganadores 90D (pp de cuota)                       </span>\n",
       "                                                                         \n",
       " <span style=\"font-weight: bold\"> name                </span> <span style=\"font-weight: bold\"> share_change_pp </span> <span style=\"font-weight: bold\"> slope_pp_per_30d </span> <span style=\"font-weight: bold\"> avg_weight </span> \n",
       " \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> MEXC BTC/USDT       </span> <span style=\"color: #008000; text-decoration-color: #008000\">           +7.89 </span> <span style=\"color: #008000; text-decoration-color: #008000\">            +3.61 </span> <span style=\"color: #008000; text-decoration-color: #008000\">     18.00% </span> \n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> Crypto.com BTC/USDT </span> <span style=\"color: #008000; text-decoration-color: #008000\">           +0.70 </span> <span style=\"color: #008000; text-decoration-color: #008000\">            -0.14 </span> <span style=\"color: #008000; text-decoration-color: #008000\">      5.35% </span> \n",
       " <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> OKX BTC/USDT        </span> <span style=\"color: #008000; text-decoration-color: #008000\">           +0.12 </span> <span style=\"color: #008000; text-decoration-color: #008000\">            +0.45 </span> <span style=\"color: #008000; text-decoration-color: #008000\">      7.07% </span> \n",
       "                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                       Ganadores 90D (pp de cuota)                       \u001b[0m\n",
       "                                                                         \n",
       " \u001b[1m \u001b[0m\u001b[1mname               \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mshare_change_pp\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mslope_pp_per_30d\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mavg_weight\u001b[0m\u001b[1m \u001b[0m \n",
       " \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \n",
       " \u001b[1;32m \u001b[0m\u001b[1;32mMEXC BTC/USDT      \u001b[0m\u001b[1;32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          +7.89\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m           +3.61\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m    18.00%\u001b[0m\u001b[32m \u001b[0m \n",
       " \u001b[1;32m \u001b[0m\u001b[1;32mCrypto.com BTC/USDT\u001b[0m\u001b[1;32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          +0.70\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m           -0.14\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m     5.35%\u001b[0m\u001b[32m \u001b[0m \n",
       " \u001b[1;32m \u001b[0m\u001b[1;32mOKX BTC/USDT       \u001b[0m\u001b[1;32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m          +0.12\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m           +0.45\u001b[0m\u001b[32m \u001b[0m \u001b[32m \u001b[0m\u001b[32m     7.07%\u001b[0m\u001b[32m \u001b[0m \n",
       "                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Perdedores 90D (pp de cuota)                       </span>\n",
       "                                                                         \n",
       " <span style=\"font-weight: bold\"> name                </span> <span style=\"font-weight: bold\"> share_change_pp </span> <span style=\"font-weight: bold\"> slope_pp_per_30d </span> <span style=\"font-weight: bold\"> avg_weight </span> \n",
       " \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Hyperliquid BTC/USD </span> <span style=\"color: #800000; text-decoration-color: #800000\">           -3.76 </span> <span style=\"color: #800000; text-decoration-color: #800000\">            -1.78 </span> <span style=\"color: #800000; text-decoration-color: #800000\">     22.52% </span> \n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Bybit BTC/USDT      </span> <span style=\"color: #800000; text-decoration-color: #800000\">           -1.32 </span> <span style=\"color: #800000; text-decoration-color: #800000\">            +0.02 </span> <span style=\"color: #800000; text-decoration-color: #800000\">      9.04% </span> \n",
       " <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Bitget BTC/USDT     </span> <span style=\"color: #800000; text-decoration-color: #800000\">           -0.75 </span> <span style=\"color: #800000; text-decoration-color: #800000\">            -2.07 </span> <span style=\"color: #800000; text-decoration-color: #800000\">      7.94% </span> \n",
       "                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Perdedores 90D (pp de cuota)                       \u001b[0m\n",
       "                                                                         \n",
       " \u001b[1m \u001b[0m\u001b[1mname               \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mshare_change_pp\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mslope_pp_per_30d\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mavg_weight\u001b[0m\u001b[1m \u001b[0m \n",
       " \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \n",
       " \u001b[1;31m \u001b[0m\u001b[1;31mHyperliquid BTC/USD\u001b[0m\u001b[1;31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m          -3.76\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m           -1.78\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m    22.52%\u001b[0m\u001b[31m \u001b[0m \n",
       " \u001b[1;31m \u001b[0m\u001b[1;31mBybit BTC/USDT     \u001b[0m\u001b[1;31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m          -1.32\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m           +0.02\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m     9.04%\u001b[0m\u001b[31m \u001b[0m \n",
       " \u001b[1;31m \u001b[0m\u001b[1;31mBitget BTC/USDT    \u001b[0m\u001b[1;31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m          -0.75\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m           -2.07\u001b[0m\u001b[31m \u001b[0m \u001b[31m \u001b[0m\u001b[31m     7.94%\u001b[0m\u001b[31m \u001b[0m \n",
       "                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Tablas (HTML): exchange_charts\\winners_losers.html\n",
      "=== Creando todas las visualizaciones ===\n",
      "\n",
      "=== Creating Aggregated Chart with Volume ===\n",
      "  \u2713 Saved aggregated chart \u2192 exchange_charts\\btc_aggregated_with_volume.html\n",
      "\n",
      "=== Creating Individual Pair Charts ===\n",
      "  \u2713 Created chart for Binance BTC/USDT \u2192 binance_btc_usdt_chart.html\n",
      "  \u2713 Created chart for Binance BTC/USDC \u2192 binance_btc_usdc_chart.html\n",
      "  \u2713 Created chart for Bybit BTC/USDT \u2192 bybit_btc_usdt_chart.html\n",
      "  \u2713 Created chart for Bybit BTC/USDC \u2192 bybit_btc_usdc_chart.html\n",
      "  \u2713 Created chart for OKX BTC/USDT \u2192 okx_btc_usdt_chart.html\n",
      "  \u2713 Created chart for OKX BTC/USDC \u2192 okx_btc_usdc_chart.html\n",
      "  \u2713 Created chart for Coinbase BTC/USD \u2192 coinbase_btc_usd_chart.html\n",
      "  \u2713 Created chart for Crypto.com BTC/USDT \u2192 crypto.com_btc_usdt_chart.html\n",
      "  \u2713 Created chart for Crypto.com BTC/USDC \u2192 crypto.com_btc_usdc_chart.html\n",
      "  \u2713 Created chart for Hyperliquid BTC/USDC \u2192 hyperliquid_btc_usdc_chart.html\n",
      "  \u2713 Created chart for Upbit BTC/KRW \u2192 upbit_btc_krw_chart.html\n",
      "  \u2713 Created chart for Bitget BTC/USDT \u2192 bitget_btc_usdt_chart.html\n",
      "  \u2713 Created chart for Bitget BTC/USDC \u2192 bitget_btc_usdc_chart.html\n",
      "  \u2713 Created chart for MEXC BTC/USDT \u2192 mexc_btc_usdt_chart.html\n",
      "  \u2713 Created chart for MEXC BTC/USDC \u2192 mexc_btc_usdc_chart.html\n",
      "Total individual charts created: 15\n",
      "\n",
      "=== Creating Comparison Grids ===\n",
      "  \u2713 Created comparison for BTC/USDT \u2192 comparison_btc_usdt.html\n",
      "  \u2713 Created comparison for BTC/USDC \u2192 comparison_btc_usdc.html\n",
      "\n",
      "=== Visualization Summary ===\n",
      "Total charts created: 19\n",
      "Location: C:\\Users\\Pedro\\Documents\\GitHub\\BTC-TestGraph\\exchange_charts\n",
      "\n",
      "=== Resumen de Visualizaciones ===\n",
      "\u2713 Gr\u00e1fico agregado con volumen: exchange_charts\\btc_aggregated_with_volume.html\n",
      "\u2713 Gr\u00e1ficos individuales creados: 15\n",
      "\u2713 Grids de comparaci\u00f3n: 2\n",
      "\u2713 Ubicaci\u00f3n: C:\\Users\\Pedro\\Documents\\GitHub\\BTC-TestGraph\\exchange_charts\n"
     ]
    }
   ],
   "source": [
    "# ================================ MAIN =====================================\n",
    "\n",
    "def main():\n",
    "    diagnose_okx_pair(\"BTC/USDC\")\n",
    "    start_hist_ms = to_ms(datetime(2019,9,1,tzinfo=timezone.utc))\n",
    "    print(\"=== Downloading/Updating 1D candles ===\")\n",
    "    tasks = [\n",
    "        (\"Binance\", \"BTC/USDT\", BinanceFetcher.fetch, {\"interval\":\"1d\", \"start_ms\": start_hist_ms}),\n",
    "        (\"Binance\", \"BTC/USDC\", BinanceFetcher.fetch, {\"interval\":\"1d\", \"start_ms\": start_hist_ms}),\n",
    "        (\"Bybit\",   \"BTC/USDT\", BybitFetcher.fetch,   {\"interval\":\"D\",   \"start_ms\": start_hist_ms}),\n",
    "        (\"Bybit\",   \"BTC/USDC\", BybitFetcher.fetch,   {\"interval\":\"D\",   \"start_ms\": start_hist_ms}),\n",
    "        (\"OKX\", \"BTC/USDT\", OKXFetcher.fetch, {\"interval\":\"1D\", \"start_ms\": start_hist_ms}),\n",
    "        (\"OKX\", \"BTC/USDC\", OKXFetcher.fetch, {\"interval\":\"1D\", \"start_ms\": start_hist_ms}),\n",
    "        (\"Coinbase\",\"BTC/USD\",  CoinbaseFetcher.fetch,{\"granularity_sec\":86400, \"start_ms\": start_hist_ms, \"include_partial_today\": True}),\n",
    "        (\"Crypto.com\",\"BTC/USDT\", CryptoComFetcher.fetch, {\"interval\":\"1D\", \"start_ms\": start_hist_ms}),\n",
    "        (\"Crypto.com\",\"BTC/USDC\", CryptoComFetcher.fetch, {\"interval\":\"1D\", \"start_ms\": start_hist_ms}),\n",
    "        (\"Hyperliquid\",\"BTC/USDC\", HyperliquidFetcher.fetch, {\"interval\":\"1d\", \"start_ms\": start_hist_ms}),\n",
    "        #(\"Hyperliquid\",\"BTC/USDT\", HyperliquidFetcher.fetch, {\"interval\":\"1d\", \"start_ms\": start_hist_ms}),\n",
    "        (\"Upbit\",   \"BTC/KRW\",  UpbitFetcher.fetch_days, {\"start_ms\": start_hist_ms}),\n",
    "        # Nuevos exchanges\n",
    "        (\"Bitget\",  \"BTC/USDT\", BitgetFetcher.fetch, {\"interval\": \"1D\", \"start_ms\": start_hist_ms}),\n",
    "        (\"Bitget\",  \"BTC/USDC\", BitgetFetcher.fetch, {\"interval\": \"1D\", \"start_ms\": start_hist_ms}),\n",
    "        (\"MEXC\",    \"BTC/USDT\", MEXCFetcher.fetch,   {\"interval\": \"1d\", \"start_ms\": start_hist_ms}),\n",
    "        (\"MEXC\",    \"BTC/USDC\", MEXCFetcher.fetch,   {\"interval\": \"1d\", \"start_ms\": start_hist_ms}),\n",
    "    ]\n",
    "\n",
    "    data: Dict[str, pd.DataFrame] = {}\n",
    "    named_usd_datasets: List[tuple] = []\n",
    "\n",
    "    for exch, sym, fn, kwargs in tasks:\n",
    "        df = ensure_dataset(exch, sym, fn, sym, **kwargs)\n",
    "        key = f\"{exch}:{sym}\"\n",
    "        data[key] = df\n",
    "\n",
    "    # Referencia USD (excluye Coinbase para premium CB). USD-like = USD/USDT/USDC, excluye Upbit.\n",
    "    def is_usd_like(exch: str, sym: str) -> bool:\n",
    "        return (sym.endswith(\"/USD\") or sym.endswith(\"/USDT\") or sym.endswith(\"/USDC\")) and exch != \"Upbit\"\n",
    "\n",
    "    # Canonicalizaci\u00f3n para evitar duplicados de la misma fuente\n",
    "    # - Hyperliquid publica un \u00fanico mercado (USD-margined), as\u00ed que unificamos USDT/USDC \u2192 \"BTC/USD\".\n",
    "    # - Para cualquier exchange: si por error llegaran dos datasets con el mismo nombre can\u00f3nico,\n",
    "    #   se fusionan por timestamp para evitar duplicados en los pesos.\n",
    "    def canonicalize_pair(exchange: str, symbol: str) -> str:\n",
    "        s = symbol.upper().replace(\"-\", \"/\").strip()\n",
    "        if exchange == \"Hyperliquid\":\n",
    "            return \"BTC/USD\"\n",
    "        return s\n",
    "\n",
    "    named_usd_map: Dict[str, pd.DataFrame] = {}\n",
    "    for key, df in data.items():\n",
    "        if df is None or df.empty:\n",
    "            continue\n",
    "        exch, sym = key.split(\":\", 1)\n",
    "        if (sym.endswith(\"/USD\") or sym.endswith(\"/USDT\") or sym.endswith(\"/USDC\")) and exch != \"Upbit\":\n",
    "            cname = f\"{exch} {canonicalize_pair(exch, sym)}\"  # p.ej. \"MEXC BTC/USDT\"\n",
    "            if cname in named_usd_map:\n",
    "                merged = pd.concat([named_usd_map[cname], df], ignore_index=True)\n",
    "                merged = merged.drop_duplicates(subset=[\"timestamp\"]).sort_values(\"timestamp\").reset_index(drop=True)\n",
    "                named_usd_map[cname] = merged\n",
    "            else:\n",
    "                named_usd_map[cname] = df\n",
    "\n",
    "    named_usd_datasets = list(named_usd_map.items())\n",
    "\n",
    "\n",
    "    print(\"=== Descargando USDKRW ===\")\n",
    "    all_dfs_for_range = [df for df in data.values() if df is not None and not df.empty]\n",
    "    min_date = min(d[\"timestamp\"].min().date() for d in all_dfs_for_range)\n",
    "    max_date = max(d[\"timestamp\"].max().date() for d in all_dfs_for_range)\n",
    "    fx_df = fetch_usdkrw_timeseries(min_date.isoformat(), max_date.isoformat())\n",
    "\n",
    "    print(\"=== Calculando Coinbase Premium ===\")\n",
    "    coin_df = data.get(\"Coinbase:BTC/USD\")\n",
    "    refs_ex_coin = [df for name, df in named_usd_datasets if not name.startswith(\"Coinbase \")]\n",
    "    cb_prem = compute_coinbase_premium(coin_df, refs_ex_coin) if coin_df is not None and not coin_df.empty and refs_ex_coin else pd.DataFrame()\n",
    "    save_csv(cb_prem, \"coinbase_premium_daily.csv\")\n",
    "\n",
    "    print(\"=== Calculando Kimchi Premium ===\")\n",
    "    if refs_ex_coin:\n",
    "        # Construir referencia USD con pesos en USD y penalizaci\u00f3n a Hyperliquid\n",
    "        parts = []\n",
    "        for df_part in refs_ex_coin:\n",
    "            dd = ensure_daily(df_part)[[\"date\",\"close\",\"volume\",\"exchange\"]].copy()\n",
    "            dd[\"w_usd\"] = dd[\"close\"] * dd[\"volume\"]\n",
    "            dd.loc[dd[\"exchange\"].str.contains(\"Hyperliquid\", case=False, na=False), \"w_usd\"] *= HYPERLIQUID_WEIGHT_PENALTY\n",
    "            parts.append(dd[[\"date\",\"close\",\"w_usd\"]])\n",
    "        ref_concat = pd.concat(parts, ignore_index=True)\n",
    "        ref_usd = ref_concat.groupby(\"date\").apply(lambda g: pd.Series({\n",
    "            \"close\": (g[\"close\"]*g[\"w_usd\"]).sum()/max(g[\"w_usd\"].sum(),1e-12),\n",
    "            \"volume_usd\": g[\"w_usd\"].sum()\n",
    "        })).reset_index()\n",
    "        ref_usd[\"timestamp\"] = (ref_usd[\"date\"].dt.tz_convert(\"UTC\") if pd.api.types.is_datetime64tz_dtype(ref_usd[\"date\"]) else pd.to_datetime(ref_usd[\"date\"], utc=True))\n",
    "        ref_usd_df = pd.DataFrame({\n",
    "            \"timestamp\": ref_usd[\"timestamp\"],\n",
    "            \"open\": ref_usd[\"close\"],\n",
    "            \"high\": ref_usd[\"close\"],\n",
    "            \"low\": ref_usd[\"close\"],\n",
    "            \"close\": ref_usd[\"close\"],\n",
    "            \"volume\": ref_usd[\"volume_usd\"],\n",
    "            \"symbol\": \"REF/USD\", \"exchange\":\"REF\"\n",
    "        })\n",
    "    else:\n",
    "        ref_usd_df = pd.DataFrame()\n",
    "\n",
    "    upbit_df = data.get(\"Upbit:BTC/KRW\", pd.DataFrame())\n",
    "    kimchi = compute_kimchi_premium(upbit_df, ref_usd_df, fx_df) if not upbit_df.empty and not ref_usd_df.empty and not fx_df.empty else pd.DataFrame()\n",
    "    save_csv(kimchi, \"kimchi_premium_daily.csv\")\n",
    "\n",
    "    print(\"=== Construyendo vela agregada (USD) ===\")\n",
    "    usd_dfs = [df for _, df in named_usd_datasets]\n",
    "    agg = aggregate_usd_candles(usd_dfs)\n",
    "    save_csv(agg, \"aggregated_btc_usd_1d.csv\")\n",
    "\n",
    "    print(\"=== Pesos por exchange (diario) ===\")\n",
    "    weights_input = [(name, df) for name, df in named_usd_datasets]\n",
    "    weights = compute_daily_weights(weights_input)\n",
    "    save_csv(weights, \"aggregate_weights_daily.csv\")\n",
    "    if not weights.empty:\n",
    "        last_day = weights[\"date\"].max()\n",
    "        w_last = weights[weights[\"date\"] == last_day][[\"name\",\"weight\"]].sort_values(\"weight\", ascending=False)\n",
    "        print(\"Pesos \u00faltima fecha:\")\n",
    "        print(w_last.to_string(index=False, float_format=lambda x: f\"{x*100:.2f}%\"))\n",
    "\n",
    "    # === Pesos 7D/30D/90D y an\u00e1lisis de tendencia ===\n",
    "    print(\"=== Pesos 7D/30D/90D y tendencias ===\")\n",
    "    w7  = summarize_period_weights(weights, 7)\n",
    "    w30 = summarize_period_weights(weights, 30)\n",
    "    w90 = summarize_period_weights(weights, 90)\n",
    "    save_csv(w7,  \"aggregate_weights_last7d.csv\")\n",
    "    save_csv(w30, \"aggregate_weights_last30d.csv\")\n",
    "    save_csv(w90, \"aggregate_weights_last90d.csv\")\n",
    "\n",
    "    _print_top(w7,  \"7D\")\n",
    "    _print_top(w30, \"30D\")\n",
    "    _print_top(w90, \"90D\")\n",
    "\n",
    "    # Exportar HTML con estilo de ganadores/perdedores\n",
    "    winners_html = os.path.join(CHARTS_DIR, \"winners_losers.html\")\n",
    "    _export_winners_losers_html(w7, w30, w90, winners_html)\n",
    "    print(\" - Tablas (HTML):\", winners_html)\n",
    "\n",
    "    # === Visualizaciones mejoradas ===\n",
    "    print(\"=== Creando todas las visualizaciones ===\")\n",
    "    \n",
    "    # Usa la nueva funci\u00f3n integrada\n",
    "    viz_results = create_all_visualizations(\n",
    "        data=data,\n",
    "        df_agg=agg,\n",
    "        coinbase_prem=cb_prem,\n",
    "        kimchi_prem=kimchi,\n",
    "        charts_dir=CHARTS_DIR\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Resumen de Visualizaciones ===\")\n",
    "    print(f\"\u2713 Gr\u00e1fico agregado con volumen: {viz_results['aggregated_chart']}\")\n",
    "    print(f\"\u2713 Gr\u00e1ficos individuales creados: {len(viz_results['individual_charts'])}\")\n",
    "    print(f\"\u2713 Grids de comparaci\u00f3n: {len(viz_results['comparison_grids'])}\")\n",
    "    print(f\"\u2713 Ubicaci\u00f3n: {os.path.abspath(CHARTS_DIR)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa667a9a-4db8-46d2-87dd-bb8319c1398d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618dbb53-6d4d-4afd-895a-b8aeb054fbfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf1b431-0e34-45ea-8625-6d35229c7f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c45e08-3bb1-4d7a-87c4-98c977419b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b444b-8aa2-4fb7-bef2-ef14f52e44db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bface8b2-6c0d-44eb-adec-c7d0c41392ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68851011-019f-41a0-927f-bd598a9426bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}